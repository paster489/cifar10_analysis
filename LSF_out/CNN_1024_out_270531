loading ...
loaded conda.sh
sh shell detected
main => start
 
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Train/Validation random split => start
 
DataLoader => start
 
To_device => start
 
Train => start
 
Epoch [0], train_loss: 2.3193, train_acc: 0.1052, val_loss: 2.2984, val_acc: 0.1035
Epoch [1], train_loss: 2.1920, train_acc: 0.1745, val_loss: 2.0479, val_acc: 0.2338
Epoch [2], train_loss: 1.8688, train_acc: 0.3182, val_loss: 1.7485, val_acc: 0.3596
Epoch [3], train_loss: 1.6022, train_acc: 0.4093, val_loss: 1.5276, val_acc: 0.4317
Epoch [4], train_loss: 1.4622, train_acc: 0.4645, val_loss: 1.4162, val_acc: 0.4820
Epoch [5], train_loss: 1.3215, train_acc: 0.5195, val_loss: 1.2631, val_acc: 0.5429
Epoch [6], train_loss: 1.2191, train_acc: 0.5576, val_loss: 1.1930, val_acc: 0.5650
Epoch [7], train_loss: 1.1103, train_acc: 0.6003, val_loss: 1.1200, val_acc: 0.6012
Epoch [8], train_loss: 1.0217, train_acc: 0.6324, val_loss: 1.0140, val_acc: 0.6369
Epoch [9], train_loss: 0.9434, train_acc: 0.6642, val_loss: 0.9906, val_acc: 0.6486
Epoch [10], train_loss: 0.8630, train_acc: 0.6932, val_loss: 0.9060, val_acc: 0.6796
Epoch [11], train_loss: 0.7970, train_acc: 0.7164, val_loss: 0.8928, val_acc: 0.6789
Epoch [12], train_loss: 0.7395, train_acc: 0.7380, val_loss: 0.8532, val_acc: 0.6977
Epoch [13], train_loss: 0.6741, train_acc: 0.7623, val_loss: 0.8059, val_acc: 0.7213
Epoch [14], train_loss: 0.6118, train_acc: 0.7837, val_loss: 0.8101, val_acc: 0.7141
Epoch [15], train_loss: 0.5542, train_acc: 0.8054, val_loss: 0.7800, val_acc: 0.7352
Epoch [16], train_loss: 0.5167, train_acc: 0.8176, val_loss: 0.7765, val_acc: 0.7378
Epoch [17], train_loss: 0.4663, train_acc: 0.8339, val_loss: 0.7834, val_acc: 0.7398
Epoch [18], train_loss: 0.4076, train_acc: 0.8571, val_loss: 0.7934, val_acc: 0.7413
Epoch [19], train_loss: 0.3336, train_acc: 0.8834, val_loss: 0.8145, val_acc: 0.7432
Epoch [20], train_loss: 0.2822, train_acc: 0.9013, val_loss: 0.8636, val_acc: 0.7468
Epoch [21], train_loss: 0.2375, train_acc: 0.9171, val_loss: 0.9403, val_acc: 0.7426
Epoch [22], train_loss: 0.2019, train_acc: 0.9285, val_loss: 0.9853, val_acc: 0.7325
Epoch [23], train_loss: 0.1459, train_acc: 0.9502, val_loss: 1.0646, val_acc: 0.7412
Epoch [24], train_loss: 0.1068, train_acc: 0.9637, val_loss: 1.2129, val_acc: 0.7424
Epoch [25], train_loss: 0.0809, train_acc: 0.9734, val_loss: 1.3132, val_acc: 0.7421
Epoch [26], train_loss: 0.0762, train_acc: 0.9736, val_loss: 1.3297, val_acc: 0.7412
Epoch [27], train_loss: 0.0539, train_acc: 0.9827, val_loss: 1.4507, val_acc: 0.7400
Epoch [28], train_loss: 0.0407, train_acc: 0.9867, val_loss: 1.5709, val_acc: 0.7391
Epoch [29], train_loss: 0.0304, train_acc: 0.9908, val_loss: 1.5952, val_acc: 0.7435
Epoch [30], train_loss: 0.0267, train_acc: 0.9917, val_loss: 1.7087, val_acc: 0.7440
Epoch [31], train_loss: 0.0356, train_acc: 0.9877, val_loss: 1.6265, val_acc: 0.7391
Epoch [32], train_loss: 0.0474, train_acc: 0.9843, val_loss: 1.6161, val_acc: 0.7526
Epoch [33], train_loss: 0.0378, train_acc: 0.9871, val_loss: 1.7308, val_acc: 0.7358
Epoch [34], train_loss: 0.0468, train_acc: 0.9842, val_loss: 1.6450, val_acc: 0.7424
Epoch [35], train_loss: 0.0281, train_acc: 0.9907, val_loss: 1.7065, val_acc: 0.7434
Epoch [36], train_loss: 0.0167, train_acc: 0.9949, val_loss: 1.8181, val_acc: 0.7467
Epoch [37], train_loss: 0.0200, train_acc: 0.9937, val_loss: 1.8468, val_acc: 0.7474
Epoch [38], train_loss: 0.0162, train_acc: 0.9950, val_loss: 1.9125, val_acc: 0.7358
Epoch [39], train_loss: 0.0340, train_acc: 0.9880, val_loss: 1.8227, val_acc: 0.7329
Epoch [40], train_loss: 0.0541, train_acc: 0.9814, val_loss: 1.6324, val_acc: 0.7463
Epoch [41], train_loss: 0.0188, train_acc: 0.9934, val_loss: 1.8647, val_acc: 0.7420
Epoch [42], train_loss: 0.0151, train_acc: 0.9947, val_loss: 1.9093, val_acc: 0.7491
Epoch [43], train_loss: 0.0154, train_acc: 0.9951, val_loss: 1.9118, val_acc: 0.7484
Epoch [44], train_loss: 0.0149, train_acc: 0.9951, val_loss: 1.9643, val_acc: 0.7423
Epoch [45], train_loss: 0.0446, train_acc: 0.9848, val_loss: 1.6854, val_acc: 0.7416
Epoch [46], train_loss: 0.0219, train_acc: 0.9929, val_loss: 1.7663, val_acc: 0.7466
Epoch [47], train_loss: 0.0182, train_acc: 0.9938, val_loss: 1.9302, val_acc: 0.7457
Epoch [48], train_loss: 0.0172, train_acc: 0.9947, val_loss: 1.9630, val_acc: 0.7434
Epoch [49], train_loss: 0.0210, train_acc: 0.9929, val_loss: 1.8261, val_acc: 0.7513
Epoch [50], train_loss: 0.0340, train_acc: 0.9882, val_loss: 1.7750, val_acc: 0.7426
Epoch [51], train_loss: 0.0167, train_acc: 0.9942, val_loss: 1.9467, val_acc: 0.7461
Epoch [52], train_loss: 0.0165, train_acc: 0.9947, val_loss: 1.9546, val_acc: 0.7527
Epoch [53], train_loss: 0.0154, train_acc: 0.9946, val_loss: 1.9556, val_acc: 0.7489
Epoch [54], train_loss: 0.0140, train_acc: 0.9953, val_loss: 1.9272, val_acc: 0.7529
Epoch [55], train_loss: 0.0121, train_acc: 0.9959, val_loss: 2.1143, val_acc: 0.7535
Epoch [56], train_loss: 0.0195, train_acc: 0.9934, val_loss: 1.9575, val_acc: 0.7452
Epoch [57], train_loss: 0.0238, train_acc: 0.9918, val_loss: 2.0034, val_acc: 0.7406
Epoch [58], train_loss: 0.0245, train_acc: 0.9915, val_loss: 1.8725, val_acc: 0.7479
Epoch [59], train_loss: 0.0164, train_acc: 0.9944, val_loss: 1.9657, val_acc: 0.7444
Epoch [60], train_loss: 0.0255, train_acc: 0.9913, val_loss: 1.9197, val_acc: 0.7483
Epoch [61], train_loss: 0.0155, train_acc: 0.9952, val_loss: 1.8842, val_acc: 0.7541
Epoch [62], train_loss: 0.0086, train_acc: 0.9971, val_loss: 2.0656, val_acc: 0.7526
Epoch [63], train_loss: 0.0161, train_acc: 0.9945, val_loss: 2.0991, val_acc: 0.7533
Epoch [64], train_loss: 0.0209, train_acc: 0.9934, val_loss: 1.9591, val_acc: 0.7506
Epoch [65], train_loss: 0.0149, train_acc: 0.9950, val_loss: 2.0676, val_acc: 0.7563
Epoch [66], train_loss: 0.0139, train_acc: 0.9956, val_loss: 2.1544, val_acc: 0.7441
Epoch [67], train_loss: 0.0302, train_acc: 0.9895, val_loss: 1.8238, val_acc: 0.7445
Epoch [68], train_loss: 0.0229, train_acc: 0.9921, val_loss: 1.9697, val_acc: 0.7546
Epoch [69], train_loss: 0.0120, train_acc: 0.9960, val_loss: 2.1159, val_acc: 0.7514
Epoch [70], train_loss: 0.0176, train_acc: 0.9939, val_loss: 2.0434, val_acc: 0.7516
Epoch [71], train_loss: 0.0255, train_acc: 0.9912, val_loss: 1.9245, val_acc: 0.7454
Epoch [72], train_loss: 0.0229, train_acc: 0.9920, val_loss: 1.9046, val_acc: 0.7561
Epoch [73], train_loss: 0.0163, train_acc: 0.9942, val_loss: 1.9869, val_acc: 0.7572
Epoch [74], train_loss: 0.0117, train_acc: 0.9962, val_loss: 1.9973, val_acc: 0.7507
Epoch [75], train_loss: 0.0151, train_acc: 0.9952, val_loss: 1.9623, val_acc: 0.7498
Epoch [76], train_loss: 0.0105, train_acc: 0.9968, val_loss: 2.0804, val_acc: 0.7535
Epoch [77], train_loss: 0.0092, train_acc: 0.9966, val_loss: 2.2056, val_acc: 0.7448
Epoch [78], train_loss: 0.0129, train_acc: 0.9958, val_loss: 2.1629, val_acc: 0.7540
Epoch [79], train_loss: 0.0143, train_acc: 0.9951, val_loss: 1.9653, val_acc: 0.7555
Epoch [80], train_loss: 0.0186, train_acc: 0.9936, val_loss: 2.0221, val_acc: 0.7475
Epoch [81], train_loss: 0.0255, train_acc: 0.9912, val_loss: 2.0163, val_acc: 0.7526
Epoch [82], train_loss: 0.0210, train_acc: 0.9928, val_loss: 2.0470, val_acc: 0.7508
Epoch [83], train_loss: 0.0195, train_acc: 0.9937, val_loss: 2.0357, val_acc: 0.7468
Epoch [84], train_loss: 0.0139, train_acc: 0.9955, val_loss: 2.1479, val_acc: 0.7584
Epoch [85], train_loss: 0.0102, train_acc: 0.9965, val_loss: 2.0959, val_acc: 0.7542
Epoch [86], train_loss: 0.0090, train_acc: 0.9968, val_loss: 2.3000, val_acc: 0.7527
Epoch [87], train_loss: 0.0083, train_acc: 0.9970, val_loss: 2.1947, val_acc: 0.7510
Epoch [88], train_loss: 0.0086, train_acc: 0.9972, val_loss: 2.2105, val_acc: 0.7489
Epoch [89], train_loss: 0.0173, train_acc: 0.9938, val_loss: 2.1700, val_acc: 0.7479
Epoch [90], train_loss: 0.0243, train_acc: 0.9917, val_loss: 1.9799, val_acc: 0.7422
Epoch [91], train_loss: 0.0223, train_acc: 0.9929, val_loss: 2.0541, val_acc: 0.7523
Epoch [92], train_loss: 0.0152, train_acc: 0.9946, val_loss: 1.9899, val_acc: 0.7503
Epoch [93], train_loss: 0.0114, train_acc: 0.9960, val_loss: 2.0309, val_acc: 0.7510
Epoch [94], train_loss: 0.0132, train_acc: 0.9958, val_loss: 2.0684, val_acc: 0.7511
Epoch [95], train_loss: 0.0135, train_acc: 0.9957, val_loss: 2.0464, val_acc: 0.7626
Epoch [96], train_loss: 0.0252, train_acc: 0.9916, val_loss: 1.9711, val_acc: 0.7437
Epoch [97], train_loss: 0.0245, train_acc: 0.9915, val_loss: 1.9191, val_acc: 0.7451
Epoch [98], train_loss: 0.0124, train_acc: 0.9958, val_loss: 2.0584, val_acc: 0.7517
Epoch [99], train_loss: 0.0077, train_acc: 0.9975, val_loss: 2.2744, val_acc: 0.7498
 
Visualize trining => save images
 
Load the model => start
 
Check best/last models => start
 
Summary result of test set => best model  {'val_loss': 0.8103553056716919, 'val_acc': 0.7290202975273132}
Summary result of test set => last model {'val_loss': 2.3785901069641113, 'val_acc': 0.7446678280830383}
Test set evaluation => save results for postprocessing
 
** accuracy: 0.729
--
confusion matrix
[[829  19  54  11   7   3  14   9  33  21]
 [ 18 907   5   2   0   3  11   3   8  43]
 [ 70   2 657  47  58  56  70  24   8   8]
 [ 38  11  95 426  58 189 132  28  11  12]
 [ 25   7 114  32 633  33  88  59   7   2]
 [ 17   9  61 103  38 672  58  33   5   4]
 [  9   5  37  25  21  10 884   5   2   2]
 [ 17   9  54  28  67  70  11 738   1   5]
 [ 92  44  16  14   5   5  11   1 791  21]
 [ 49 132  11  10   2   7  13  10  10 756]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.71      0.83      0.77      1000
  automobile       0.79      0.91      0.85      1000
        bird       0.60      0.66      0.62      1000
         cat       0.61      0.43      0.50      1000
        deer       0.71      0.63      0.67      1000
         dog       0.64      0.67      0.66      1000
        frog       0.68      0.88      0.77      1000
       horse       0.81      0.74      0.77      1000
        ship       0.90      0.79      0.84      1000
       truck       0.86      0.76      0.81      1000

    accuracy                           0.73     10000
   macro avg       0.73      0.73      0.73     10000
weighted avg       0.73      0.73      0.73     10000

END OF CODE
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 270531: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <access4> by user <ingap> in cluster <wexac> at Tue Feb 27 11:13:48 2024
Job was executed on host(s) <hgn41>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 11:18:44 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 11:18:44 2024
Terminated at Tue Feb 27 11:25:45 2024
Results reported at Tue Feb 27 11:25:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_1024_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_1024_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1439.00 sec.
    Max Memory :                                 3384 MB
    Average Memory :                             3257.12 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6856.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   422 sec.
    Turnaround time :                            717 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_1024_err_270531> for stderr output of this job.

