loading ...
loaded conda.sh
sh shell detected
main => start
 
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Train/Validation random split => start
 
DataLoader => start
 
To_device => start
 
Train => start
 
Epoch [0], train_loss: 1.9221, train_acc: 0.2742, val_loss: 1.5362, val_acc: 0.4263
Epoch [1], train_loss: 1.3891, train_acc: 0.4838, val_loss: 1.2484, val_acc: 0.5400
Epoch [2], train_loss: 1.1274, train_acc: 0.5915, val_loss: 1.0746, val_acc: 0.6109
Epoch [3], train_loss: 0.9389, train_acc: 0.6622, val_loss: 0.9396, val_acc: 0.6609
Epoch [4], train_loss: 0.7842, train_acc: 0.7217, val_loss: 0.8338, val_acc: 0.6999
Epoch [5], train_loss: 0.6882, train_acc: 0.7564, val_loss: 0.7735, val_acc: 0.7256
Epoch [6], train_loss: 0.5746, train_acc: 0.7957, val_loss: 0.6861, val_acc: 0.7601
Epoch [7], train_loss: 0.4881, train_acc: 0.8275, val_loss: 0.7009, val_acc: 0.7566
Epoch [8], train_loss: 0.4049, train_acc: 0.8582, val_loss: 0.7066, val_acc: 0.7634
Epoch [9], train_loss: 0.3376, train_acc: 0.8791, val_loss: 0.7097, val_acc: 0.7713
Epoch [10], train_loss: 0.2553, train_acc: 0.9095, val_loss: 0.7566, val_acc: 0.7721
Epoch [11], train_loss: 0.1958, train_acc: 0.9313, val_loss: 0.8273, val_acc: 0.7787
Epoch [12], train_loss: 0.1440, train_acc: 0.9496, val_loss: 0.9746, val_acc: 0.7786
Epoch [13], train_loss: 0.1247, train_acc: 0.9549, val_loss: 1.0273, val_acc: 0.7771
Epoch [14], train_loss: 0.1056, train_acc: 0.9634, val_loss: 1.0866, val_acc: 0.7728
Epoch [15], train_loss: 0.0822, train_acc: 0.9715, val_loss: 1.1678, val_acc: 0.7698
Epoch [16], train_loss: 0.0656, train_acc: 0.9771, val_loss: 1.1479, val_acc: 0.7653
Epoch [17], train_loss: 0.0631, train_acc: 0.9785, val_loss: 1.2612, val_acc: 0.7748
Epoch [18], train_loss: 0.0634, train_acc: 0.9771, val_loss: 1.2989, val_acc: 0.7644
Epoch [19], train_loss: 0.0650, train_acc: 0.9773, val_loss: 1.2761, val_acc: 0.7694
Epoch [20], train_loss: 0.0487, train_acc: 0.9843, val_loss: 1.3451, val_acc: 0.7650
Epoch [21], train_loss: 0.0597, train_acc: 0.9800, val_loss: 1.3307, val_acc: 0.7731
Epoch [22], train_loss: 0.0387, train_acc: 0.9866, val_loss: 1.4471, val_acc: 0.7734
Epoch [23], train_loss: 0.0466, train_acc: 0.9842, val_loss: 1.4102, val_acc: 0.7752
Epoch [24], train_loss: 0.0518, train_acc: 0.9824, val_loss: 1.3262, val_acc: 0.7861
Epoch [25], train_loss: 0.0410, train_acc: 0.9867, val_loss: 1.4811, val_acc: 0.7700
Epoch [26], train_loss: 0.0586, train_acc: 0.9805, val_loss: 1.2164, val_acc: 0.7751
Epoch [27], train_loss: 0.0340, train_acc: 0.9887, val_loss: 1.5126, val_acc: 0.7674
Epoch [28], train_loss: 0.0449, train_acc: 0.9845, val_loss: 1.3660, val_acc: 0.7821
Epoch [29], train_loss: 0.0365, train_acc: 0.9879, val_loss: 1.5131, val_acc: 0.7617
Epoch [30], train_loss: 0.0384, train_acc: 0.9868, val_loss: 1.4116, val_acc: 0.7738
Epoch [31], train_loss: 0.0297, train_acc: 0.9900, val_loss: 1.5814, val_acc: 0.7776
Epoch [32], train_loss: 0.0493, train_acc: 0.9841, val_loss: 1.5206, val_acc: 0.7650
Epoch [33], train_loss: 0.0385, train_acc: 0.9873, val_loss: 1.3709, val_acc: 0.7748
Epoch [34], train_loss: 0.0252, train_acc: 0.9920, val_loss: 1.6167, val_acc: 0.7830
Epoch [35], train_loss: 0.0329, train_acc: 0.9896, val_loss: 1.6279, val_acc: 0.7728
Epoch [36], train_loss: 0.0369, train_acc: 0.9879, val_loss: 1.5337, val_acc: 0.7698
Epoch [37], train_loss: 0.0308, train_acc: 0.9902, val_loss: 1.6024, val_acc: 0.7690
Epoch [38], train_loss: 0.0396, train_acc: 0.9869, val_loss: 1.4643, val_acc: 0.7722
Epoch [39], train_loss: 0.0329, train_acc: 0.9890, val_loss: 1.4567, val_acc: 0.7743
Epoch [40], train_loss: 0.0392, train_acc: 0.9875, val_loss: 1.5234, val_acc: 0.7722
Epoch [41], train_loss: 0.0359, train_acc: 0.9879, val_loss: 1.4761, val_acc: 0.7762
Epoch [42], train_loss: 0.0320, train_acc: 0.9900, val_loss: 1.4223, val_acc: 0.7686
Epoch [43], train_loss: 0.0234, train_acc: 0.9924, val_loss: 1.6395, val_acc: 0.7818
Epoch [44], train_loss: 0.0407, train_acc: 0.9870, val_loss: 1.5157, val_acc: 0.7781
Epoch [45], train_loss: 0.0219, train_acc: 0.9925, val_loss: 1.8158, val_acc: 0.7648
Epoch [46], train_loss: 0.0279, train_acc: 0.9914, val_loss: 1.7790, val_acc: 0.7651
Epoch [47], train_loss: 0.0337, train_acc: 0.9890, val_loss: 1.6239, val_acc: 0.7717
Epoch [48], train_loss: 0.0269, train_acc: 0.9912, val_loss: 1.6492, val_acc: 0.7682
Epoch [49], train_loss: 0.0360, train_acc: 0.9881, val_loss: 1.5637, val_acc: 0.7717
Epoch [50], train_loss: 0.0275, train_acc: 0.9913, val_loss: 1.5918, val_acc: 0.7617
Epoch [51], train_loss: 0.0273, train_acc: 0.9910, val_loss: 1.5674, val_acc: 0.7839
Epoch [52], train_loss: 0.0306, train_acc: 0.9899, val_loss: 1.5334, val_acc: 0.7758
Epoch [53], train_loss: 0.0255, train_acc: 0.9922, val_loss: 1.6289, val_acc: 0.7721
Epoch [54], train_loss: 0.0299, train_acc: 0.9905, val_loss: 1.6681, val_acc: 0.7755
Epoch [55], train_loss: 0.0266, train_acc: 0.9917, val_loss: 1.6076, val_acc: 0.7715
Epoch [56], train_loss: 0.0259, train_acc: 0.9913, val_loss: 1.6589, val_acc: 0.7725
Epoch [57], train_loss: 0.0309, train_acc: 0.9906, val_loss: 1.5250, val_acc: 0.7780
Epoch [58], train_loss: 0.0207, train_acc: 0.9931, val_loss: 1.9184, val_acc: 0.7789
Epoch [59], train_loss: 0.0274, train_acc: 0.9912, val_loss: 1.6102, val_acc: 0.7727
Epoch [60], train_loss: 0.0278, train_acc: 0.9915, val_loss: 1.7591, val_acc: 0.7734
Epoch [61], train_loss: 0.0202, train_acc: 0.9935, val_loss: 1.8247, val_acc: 0.7690
Epoch [62], train_loss: 0.0318, train_acc: 0.9903, val_loss: 1.6575, val_acc: 0.7673
Epoch [63], train_loss: 0.0232, train_acc: 0.9928, val_loss: 1.8006, val_acc: 0.7780
Epoch [64], train_loss: 0.0323, train_acc: 0.9899, val_loss: 1.7919, val_acc: 0.7639
Epoch [65], train_loss: 0.0299, train_acc: 0.9910, val_loss: 1.6780, val_acc: 0.7826
Epoch [66], train_loss: 0.0165, train_acc: 0.9945, val_loss: 1.9865, val_acc: 0.7714
Epoch [67], train_loss: 0.0288, train_acc: 0.9915, val_loss: 1.6905, val_acc: 0.7600
Epoch [68], train_loss: 0.0276, train_acc: 0.9911, val_loss: 1.7660, val_acc: 0.7728
Epoch [69], train_loss: 0.0161, train_acc: 0.9950, val_loss: 1.8570, val_acc: 0.7736
Epoch [70], train_loss: 0.0275, train_acc: 0.9913, val_loss: 1.8432, val_acc: 0.7732
Epoch [71], train_loss: 0.0273, train_acc: 0.9912, val_loss: 1.8484, val_acc: 0.7722
Epoch [72], train_loss: 0.0230, train_acc: 0.9926, val_loss: 1.8924, val_acc: 0.7743
Epoch [73], train_loss: 0.0308, train_acc: 0.9908, val_loss: 1.7921, val_acc: 0.7785
Epoch [74], train_loss: 0.0223, train_acc: 0.9926, val_loss: 1.8527, val_acc: 0.7763
Epoch [75], train_loss: 0.0169, train_acc: 0.9946, val_loss: 1.8049, val_acc: 0.7737
Epoch [76], train_loss: 0.0215, train_acc: 0.9931, val_loss: 1.9226, val_acc: 0.7834
Epoch [77], train_loss: 0.0269, train_acc: 0.9912, val_loss: 1.8946, val_acc: 0.7757
Epoch [78], train_loss: 0.0286, train_acc: 0.9911, val_loss: 1.8849, val_acc: 0.7677
Epoch [79], train_loss: 0.0246, train_acc: 0.9923, val_loss: 1.7462, val_acc: 0.7694
Epoch [80], train_loss: 0.0157, train_acc: 0.9954, val_loss: 1.8598, val_acc: 0.7789
Epoch [81], train_loss: 0.0325, train_acc: 0.9900, val_loss: 1.6031, val_acc: 0.7739
Epoch [82], train_loss: 0.0251, train_acc: 0.9925, val_loss: 1.8012, val_acc: 0.7705
Epoch [83], train_loss: 0.0197, train_acc: 0.9939, val_loss: 1.8563, val_acc: 0.7730
Epoch [84], train_loss: 0.0140, train_acc: 0.9957, val_loss: 1.9437, val_acc: 0.7665
Epoch [85], train_loss: 0.0330, train_acc: 0.9902, val_loss: 1.7247, val_acc: 0.7677
Epoch [86], train_loss: 0.0281, train_acc: 0.9913, val_loss: 1.8154, val_acc: 0.7673
Epoch [87], train_loss: 0.0235, train_acc: 0.9925, val_loss: 1.8791, val_acc: 0.7701
Epoch [88], train_loss: 0.0159, train_acc: 0.9951, val_loss: 1.9295, val_acc: 0.7785
Epoch [89], train_loss: 0.0174, train_acc: 0.9949, val_loss: 1.8694, val_acc: 0.7599
Epoch [90], train_loss: 0.0306, train_acc: 0.9909, val_loss: 1.7121, val_acc: 0.7786
Epoch [91], train_loss: 0.0154, train_acc: 0.9949, val_loss: 1.9107, val_acc: 0.7746
Epoch [92], train_loss: 0.0328, train_acc: 0.9900, val_loss: 1.8688, val_acc: 0.7591
Epoch [93], train_loss: 0.0281, train_acc: 0.9917, val_loss: 1.8150, val_acc: 0.7718
Epoch [94], train_loss: 0.0136, train_acc: 0.9958, val_loss: 2.0098, val_acc: 0.7792
Epoch [95], train_loss: 0.0206, train_acc: 0.9939, val_loss: 1.9018, val_acc: 0.7779
Epoch [96], train_loss: 0.0243, train_acc: 0.9926, val_loss: 1.9285, val_acc: 0.7697
Epoch [97], train_loss: 0.0283, train_acc: 0.9920, val_loss: 2.0170, val_acc: 0.7548
Epoch [98], train_loss: 0.0225, train_acc: 0.9931, val_loss: 1.9238, val_acc: 0.7684
Epoch [99], train_loss: 0.0159, train_acc: 0.9950, val_loss: 2.0181, val_acc: 0.7653
 
Visualize trining => save images
 
Load the model => start
 
Check best/last models => start
 
Summary result of test set => best model  {'val_loss': 0.7050888538360596, 'val_acc': 0.758593738079071}
Summary result of test set => last model {'val_loss': 2.077684164047241, 'val_acc': 0.7617474794387817}
Test set evaluation => save results for postprocessing
 
** accuracy: 0.759
--
confusion matrix
[[756  21  41   8  14   3   4   8 100  45]
 [  9 891   0   1   2   3   2   1  23  68]
 [ 58   4 612  50 109  73  40  13  26  15]
 [ 25  10  60 503  87 195  50  22  27  21]
 [ 14   5  51  31 805  21  23  34  11   5]
 [  4   4  26 110  66 712  12  36  17  13]
 [  8   6  42  52  60  23 781   5  17   6]
 [ 12   7  25  31  93  59   2 747   8  16]
 [ 20  29   2  12   6   4   2   1 901  23]
 [ 16  56   3   8   4   1   1   5  26 880]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.82      0.76      0.79      1000
  automobile       0.86      0.89      0.88      1000
        bird       0.71      0.61      0.66      1000
         cat       0.62      0.50      0.56      1000
        deer       0.65      0.81      0.72      1000
         dog       0.65      0.71      0.68      1000
        frog       0.85      0.78      0.81      1000
       horse       0.86      0.75      0.80      1000
        ship       0.78      0.90      0.84      1000
       truck       0.81      0.88      0.84      1000

    accuracy                           0.76     10000
   macro avg       0.76      0.76      0.76     10000
weighted avg       0.76      0.76      0.76     10000

END OF CODE
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 270520: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <access4> by user <ingap> in cluster <wexac> at Tue Feb 27 11:13:12 2024
Job was executed on host(s) <hgn55>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 11:18:13 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 11:18:13 2024
Terminated at Tue Feb 27 11:25:31 2024
Results reported at Tue Feb 27 11:25:31 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_256_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_256_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1433.00 sec.
    Max Memory :                                 3350 MB
    Average Memory :                             3237.72 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6890.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   442 sec.
    Turnaround time :                            739 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_256_err_270520> for stderr output of this job.

