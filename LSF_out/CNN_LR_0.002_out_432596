loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 2.0187, train_acc: 0.2300, val_loss: 1.6684, val_acc: 0.3801, val_precision: 0.3997, val_recall: 0.3801, val_f1: 0.3530
Epoch [1], train_loss: 1.4566, train_acc: 0.4628, val_loss: 1.3255, val_acc: 0.5149, val_precision: 0.5278, val_recall: 0.5149, val_f1: 0.5096
Epoch [2], train_loss: 1.1974, train_acc: 0.5680, val_loss: 1.1451, val_acc: 0.5841, val_precision: 0.6049, val_recall: 0.5841, val_f1: 0.5769
Epoch [3], train_loss: 1.0360, train_acc: 0.6306, val_loss: 0.9830, val_acc: 0.6441, val_precision: 0.6571, val_recall: 0.6441, val_f1: 0.6422
Epoch [4], train_loss: 0.9146, train_acc: 0.6732, val_loss: 0.9525, val_acc: 0.6612, val_precision: 0.6809, val_recall: 0.6612, val_f1: 0.6645
Epoch [5], train_loss: 0.8260, train_acc: 0.7086, val_loss: 0.9177, val_acc: 0.6733, val_precision: 0.6837, val_recall: 0.6733, val_f1: 0.6679
Epoch [6], train_loss: 0.7419, train_acc: 0.7347, val_loss: 0.9193, val_acc: 0.6745, val_precision: 0.6923, val_recall: 0.6745, val_f1: 0.6747
Epoch [7], train_loss: 0.6652, train_acc: 0.7644, val_loss: 0.9198, val_acc: 0.6810, val_precision: 0.7060, val_recall: 0.6810, val_f1: 0.6840
Epoch [8], train_loss: 0.6113, train_acc: 0.7830, val_loss: 0.9372, val_acc: 0.6887, val_precision: 0.6983, val_recall: 0.6887, val_f1: 0.6873
Epoch [9], train_loss: 0.5418, train_acc: 0.8048, val_loss: 1.0322, val_acc: 0.6784, val_precision: 0.6921, val_recall: 0.6784, val_f1: 0.6769
Epoch [10], train_loss: 0.4895, train_acc: 0.8230, val_loss: 0.9887, val_acc: 0.6913, val_precision: 0.6986, val_recall: 0.6913, val_f1: 0.6893
Epoch [11], train_loss: 0.4262, train_acc: 0.8488, val_loss: 1.0733, val_acc: 0.6838, val_precision: 0.6919, val_recall: 0.6838, val_f1: 0.6808
Epoch [12], train_loss: 0.3871, train_acc: 0.8604, val_loss: 1.0960, val_acc: 0.6990, val_precision: 0.7026, val_recall: 0.6990, val_f1: 0.6940
Epoch [13], train_loss: 0.3435, train_acc: 0.8760, val_loss: 1.2433, val_acc: 0.6836, val_precision: 0.6901, val_recall: 0.6836, val_f1: 0.6793
Epoch [14], train_loss: 0.3090, train_acc: 0.8884, val_loss: 1.2946, val_acc: 0.6866, val_precision: 0.7001, val_recall: 0.6866, val_f1: 0.6869
Epoch [15], train_loss: 0.2845, train_acc: 0.8975, val_loss: 1.3535, val_acc: 0.6818, val_precision: 0.6935, val_recall: 0.6818, val_f1: 0.6825
Epoch [16], train_loss: 0.2679, train_acc: 0.9045, val_loss: 1.3890, val_acc: 0.6813, val_precision: 0.6951, val_recall: 0.6813, val_f1: 0.6824
Epoch [17], train_loss: 0.2435, train_acc: 0.9126, val_loss: 1.4375, val_acc: 0.6872, val_precision: 0.7043, val_recall: 0.6872, val_f1: 0.6897
Epoch [18], train_loss: 0.2230, train_acc: 0.9224, val_loss: 1.4603, val_acc: 0.6792, val_precision: 0.7086, val_recall: 0.6792, val_f1: 0.6867
Epoch [19], train_loss: 0.2020, train_acc: 0.9284, val_loss: 1.5548, val_acc: 0.6935, val_precision: 0.7060, val_recall: 0.6935, val_f1: 0.6933
Epoch [20], train_loss: 0.2005, train_acc: 0.9305, val_loss: 1.6455, val_acc: 0.6770, val_precision: 0.6871, val_recall: 0.6770, val_f1: 0.6779
Epoch [21], train_loss: 0.1866, train_acc: 0.9355, val_loss: 1.6453, val_acc: 0.6795, val_precision: 0.6911, val_recall: 0.6795, val_f1: 0.6806
Epoch [22], train_loss: 0.1647, train_acc: 0.9428, val_loss: 1.9303, val_acc: 0.6745, val_precision: 0.6829, val_recall: 0.6745, val_f1: 0.6718
Epoch [23], train_loss: 0.1830, train_acc: 0.9381, val_loss: 1.7319, val_acc: 0.6800, val_precision: 0.6850, val_recall: 0.6800, val_f1: 0.6771
Epoch [24], train_loss: 0.1670, train_acc: 0.9440, val_loss: 1.8102, val_acc: 0.6845, val_precision: 0.7010, val_recall: 0.6845, val_f1: 0.6868
Epoch [25], train_loss: 0.1671, train_acc: 0.9434, val_loss: 1.8251, val_acc: 0.6724, val_precision: 0.6846, val_recall: 0.6724, val_f1: 0.6736
Epoch [26], train_loss: 0.1632, train_acc: 0.9446, val_loss: 1.9846, val_acc: 0.6668, val_precision: 0.6733, val_recall: 0.6668, val_f1: 0.6641
Epoch [27], train_loss: 0.1492, train_acc: 0.9488, val_loss: 1.9506, val_acc: 0.6806, val_precision: 0.6941, val_recall: 0.6806, val_f1: 0.6823
Epoch [28], train_loss: 0.1524, train_acc: 0.9496, val_loss: 1.9811, val_acc: 0.6738, val_precision: 0.6837, val_recall: 0.6738, val_f1: 0.6742
Epoch [29], train_loss: 0.1585, train_acc: 0.9472, val_loss: 2.0917, val_acc: 0.6779, val_precision: 0.6980, val_recall: 0.6779, val_f1: 0.6817
Epoch [30], train_loss: 0.1479, train_acc: 0.9522, val_loss: 1.9562, val_acc: 0.6796, val_precision: 0.6917, val_recall: 0.6796, val_f1: 0.6809
Epoch [31], train_loss: 0.1533, train_acc: 0.9501, val_loss: 1.9605, val_acc: 0.6784, val_precision: 0.6901, val_recall: 0.6784, val_f1: 0.6796
Epoch [32], train_loss: 0.1348, train_acc: 0.9548, val_loss: 2.0341, val_acc: 0.6748, val_precision: 0.6915, val_recall: 0.6748, val_f1: 0.6771
Epoch [33], train_loss: 0.1356, train_acc: 0.9541, val_loss: 2.1428, val_acc: 0.6744, val_precision: 0.6800, val_recall: 0.6744, val_f1: 0.6719
Epoch [34], train_loss: 0.1491, train_acc: 0.9517, val_loss: 2.1345, val_acc: 0.6825, val_precision: 0.6919, val_recall: 0.6825, val_f1: 0.6821
Epoch [35], train_loss: 0.1252, train_acc: 0.9585, val_loss: 2.1609, val_acc: 0.6648, val_precision: 0.6813, val_recall: 0.6648, val_f1: 0.6652
Epoch [36], train_loss: 0.1374, train_acc: 0.9549, val_loss: 2.1051, val_acc: 0.6805, val_precision: 0.6888, val_recall: 0.6805, val_f1: 0.6786
Epoch [37], train_loss: 0.1529, train_acc: 0.9506, val_loss: 2.0276, val_acc: 0.6784, val_precision: 0.6894, val_recall: 0.6784, val_f1: 0.6774
Epoch [38], train_loss: 0.1232, train_acc: 0.9601, val_loss: 2.1672, val_acc: 0.6709, val_precision: 0.6816, val_recall: 0.6709, val_f1: 0.6719
Epoch [39], train_loss: 0.1265, train_acc: 0.9583, val_loss: 2.0851, val_acc: 0.6810, val_precision: 0.6931, val_recall: 0.6810, val_f1: 0.6814
Epoch [40], train_loss: 0.1321, train_acc: 0.9573, val_loss: 2.2165, val_acc: 0.6785, val_precision: 0.6985, val_recall: 0.6785, val_f1: 0.6821
Epoch [41], train_loss: 0.1259, train_acc: 0.9591, val_loss: 2.3021, val_acc: 0.6783, val_precision: 0.6943, val_recall: 0.6783, val_f1: 0.6811
Epoch [42], train_loss: 0.1307, train_acc: 0.9591, val_loss: 2.1778, val_acc: 0.6694, val_precision: 0.6863, val_recall: 0.6694, val_f1: 0.6732
Epoch [43], train_loss: 0.1311, train_acc: 0.9587, val_loss: 2.1280, val_acc: 0.6675, val_precision: 0.6801, val_recall: 0.6675, val_f1: 0.6692
Epoch [44], train_loss: 0.1223, train_acc: 0.9617, val_loss: 2.1715, val_acc: 0.6859, val_precision: 0.6963, val_recall: 0.6859, val_f1: 0.6864
Epoch [45], train_loss: 0.1283, train_acc: 0.9589, val_loss: 2.1701, val_acc: 0.6845, val_precision: 0.7042, val_recall: 0.6845, val_f1: 0.6880
Epoch [46], train_loss: 0.1229, train_acc: 0.9617, val_loss: 2.2255, val_acc: 0.6812, val_precision: 0.6918, val_recall: 0.6812, val_f1: 0.6812
Epoch [47], train_loss: 0.1245, train_acc: 0.9605, val_loss: 2.0892, val_acc: 0.6830, val_precision: 0.7013, val_recall: 0.6830, val_f1: 0.6872
Epoch [48], train_loss: 0.1130, train_acc: 0.9641, val_loss: 2.4253, val_acc: 0.6671, val_precision: 0.6865, val_recall: 0.6671, val_f1: 0.6708
Epoch [49], train_loss: 0.1200, train_acc: 0.9623, val_loss: 2.3545, val_acc: 0.6692, val_precision: 0.6898, val_recall: 0.6692, val_f1: 0.6742
Epoch [50], train_loss: 0.1174, train_acc: 0.9646, val_loss: 2.5247, val_acc: 0.6695, val_precision: 0.6867, val_recall: 0.6695, val_f1: 0.6714
Epoch [51], train_loss: 0.1061, train_acc: 0.9664, val_loss: 2.3131, val_acc: 0.6833, val_precision: 0.7002, val_recall: 0.6833, val_f1: 0.6861
Epoch [52], train_loss: 0.1438, train_acc: 0.9555, val_loss: 2.3560, val_acc: 0.6631, val_precision: 0.6759, val_recall: 0.6631, val_f1: 0.6636
Epoch [53], train_loss: 0.1194, train_acc: 0.9631, val_loss: 2.3798, val_acc: 0.6755, val_precision: 0.6915, val_recall: 0.6755, val_f1: 0.6780
Epoch [54], train_loss: 0.1271, train_acc: 0.9612, val_loss: 2.3621, val_acc: 0.6723, val_precision: 0.6908, val_recall: 0.6723, val_f1: 0.6749
Epoch [55], train_loss: 0.1185, train_acc: 0.9633, val_loss: 2.3077, val_acc: 0.6704, val_precision: 0.6825, val_recall: 0.6704, val_f1: 0.6708
Epoch [56], train_loss: 0.1215, train_acc: 0.9624, val_loss: 2.2591, val_acc: 0.6793, val_precision: 0.6948, val_recall: 0.6793, val_f1: 0.6821
Epoch [57], train_loss: 0.1176, train_acc: 0.9637, val_loss: 2.2637, val_acc: 0.6821, val_precision: 0.6885, val_recall: 0.6821, val_f1: 0.6810
Epoch [58], train_loss: 0.1254, train_acc: 0.9613, val_loss: 2.3681, val_acc: 0.6765, val_precision: 0.6931, val_recall: 0.6765, val_f1: 0.6797
Epoch [59], train_loss: 0.1179, train_acc: 0.9633, val_loss: 2.3279, val_acc: 0.6849, val_precision: 0.6932, val_recall: 0.6849, val_f1: 0.6851
Epoch [60], train_loss: 0.1160, train_acc: 0.9642, val_loss: 2.5070, val_acc: 0.6698, val_precision: 0.6839, val_recall: 0.6698, val_f1: 0.6701
Epoch [61], train_loss: 0.1024, train_acc: 0.9683, val_loss: 2.6072, val_acc: 0.6794, val_precision: 0.6905, val_recall: 0.6794, val_f1: 0.6806
Epoch [62], train_loss: 0.1380, train_acc: 0.9592, val_loss: 2.3858, val_acc: 0.6650, val_precision: 0.6810, val_recall: 0.6650, val_f1: 0.6668
Epoch [63], train_loss: 0.1156, train_acc: 0.9650, val_loss: 2.4256, val_acc: 0.6611, val_precision: 0.6827, val_recall: 0.6611, val_f1: 0.6654
Epoch [64], train_loss: 0.1110, train_acc: 0.9667, val_loss: 2.2080, val_acc: 0.6798, val_precision: 0.6982, val_recall: 0.6798, val_f1: 0.6837
Epoch [65], train_loss: 0.1050, train_acc: 0.9674, val_loss: 2.3676, val_acc: 0.6644, val_precision: 0.6775, val_recall: 0.6644, val_f1: 0.6622
Epoch [66], train_loss: 0.0963, train_acc: 0.9706, val_loss: 2.5299, val_acc: 0.6749, val_precision: 0.6842, val_recall: 0.6749, val_f1: 0.6733
Epoch [67], train_loss: 0.1206, train_acc: 0.9639, val_loss: 2.2409, val_acc: 0.6666, val_precision: 0.6799, val_recall: 0.6666, val_f1: 0.6670
Epoch [68], train_loss: 0.1156, train_acc: 0.9642, val_loss: 2.4870, val_acc: 0.6657, val_precision: 0.6830, val_recall: 0.6657, val_f1: 0.6671
Epoch [69], train_loss: 0.1087, train_acc: 0.9661, val_loss: 2.4848, val_acc: 0.6718, val_precision: 0.6788, val_recall: 0.6718, val_f1: 0.6713
Epoch [70], train_loss: 0.1203, train_acc: 0.9637, val_loss: 2.5737, val_acc: 0.6661, val_precision: 0.6736, val_recall: 0.6661, val_f1: 0.6658
Epoch [71], train_loss: 0.1114, train_acc: 0.9659, val_loss: 2.4908, val_acc: 0.6750, val_precision: 0.6907, val_recall: 0.6750, val_f1: 0.6773
Epoch [72], train_loss: 0.0781, train_acc: 0.9752, val_loss: 2.5797, val_acc: 0.6748, val_precision: 0.6837, val_recall: 0.6748, val_f1: 0.6753
Epoch [73], train_loss: 0.1339, train_acc: 0.9591, val_loss: 2.4932, val_acc: 0.6804, val_precision: 0.6890, val_recall: 0.6804, val_f1: 0.6799
Epoch [74], train_loss: 0.1070, train_acc: 0.9673, val_loss: 2.4804, val_acc: 0.6694, val_precision: 0.6859, val_recall: 0.6694, val_f1: 0.6706
Epoch [75], train_loss: 0.1086, train_acc: 0.9676, val_loss: 2.6542, val_acc: 0.6626, val_precision: 0.6740, val_recall: 0.6626, val_f1: 0.6620
Epoch [76], train_loss: 0.1057, train_acc: 0.9682, val_loss: 2.6023, val_acc: 0.6680, val_precision: 0.6805, val_recall: 0.6680, val_f1: 0.6687
Epoch [77], train_loss: 0.1174, train_acc: 0.9650, val_loss: 2.3567, val_acc: 0.6827, val_precision: 0.6909, val_recall: 0.6827, val_f1: 0.6826
Epoch [78], train_loss: 0.1022, train_acc: 0.9686, val_loss: 2.6088, val_acc: 0.6709, val_precision: 0.6842, val_recall: 0.6709, val_f1: 0.6724
Epoch [79], train_loss: 0.1312, train_acc: 0.9626, val_loss: 2.4686, val_acc: 0.6773, val_precision: 0.6865, val_recall: 0.6773, val_f1: 0.6778
Epoch [80], train_loss: 0.0854, train_acc: 0.9747, val_loss: 2.6317, val_acc: 0.6759, val_precision: 0.6863, val_recall: 0.6759, val_f1: 0.6766
Epoch [81], train_loss: 0.1020, train_acc: 0.9701, val_loss: 2.6311, val_acc: 0.6736, val_precision: 0.6853, val_recall: 0.6736, val_f1: 0.6725
Epoch [82], train_loss: 0.1069, train_acc: 0.9683, val_loss: 2.5398, val_acc: 0.6695, val_precision: 0.6736, val_recall: 0.6695, val_f1: 0.6670
Epoch [83], train_loss: 0.1114, train_acc: 0.9667, val_loss: 2.5075, val_acc: 0.6816, val_precision: 0.6917, val_recall: 0.6816, val_f1: 0.6814
Epoch [84], train_loss: 0.1000, train_acc: 0.9698, val_loss: 2.6400, val_acc: 0.6753, val_precision: 0.6867, val_recall: 0.6753, val_f1: 0.6759
Epoch [85], train_loss: 0.0974, train_acc: 0.9711, val_loss: 2.6987, val_acc: 0.6760, val_precision: 0.6880, val_recall: 0.6760, val_f1: 0.6774
Epoch [86], train_loss: 0.1215, train_acc: 0.9651, val_loss: 2.5012, val_acc: 0.6726, val_precision: 0.6811, val_recall: 0.6726, val_f1: 0.6726
Epoch [87], train_loss: 0.1136, train_acc: 0.9679, val_loss: 2.6034, val_acc: 0.6732, val_precision: 0.6840, val_recall: 0.6732, val_f1: 0.6727
Epoch [88], train_loss: 0.1053, train_acc: 0.9692, val_loss: 2.3938, val_acc: 0.6801, val_precision: 0.6899, val_recall: 0.6801, val_f1: 0.6792
Epoch [89], train_loss: 0.0984, train_acc: 0.9708, val_loss: 2.6112, val_acc: 0.6864, val_precision: 0.6906, val_recall: 0.6864, val_f1: 0.6836
Epoch [90], train_loss: 0.1007, train_acc: 0.9706, val_loss: 2.5473, val_acc: 0.6792, val_precision: 0.6923, val_recall: 0.6792, val_f1: 0.6813
Epoch [91], train_loss: 0.1056, train_acc: 0.9697, val_loss: 2.5427, val_acc: 0.6734, val_precision: 0.6901, val_recall: 0.6734, val_f1: 0.6767
Epoch [92], train_loss: 0.0987, train_acc: 0.9723, val_loss: 2.8062, val_acc: 0.6653, val_precision: 0.6861, val_recall: 0.6653, val_f1: 0.6697
Epoch [93], train_loss: 0.1060, train_acc: 0.9694, val_loss: 2.4780, val_acc: 0.6767, val_precision: 0.6840, val_recall: 0.6767, val_f1: 0.6736
Epoch [94], train_loss: 0.1091, train_acc: 0.9694, val_loss: 2.5224, val_acc: 0.6697, val_precision: 0.6847, val_recall: 0.6697, val_f1: 0.6723
Epoch [95], train_loss: 0.1088, train_acc: 0.9678, val_loss: 2.4913, val_acc: 0.6757, val_precision: 0.6834, val_recall: 0.6757, val_f1: 0.6753
Epoch [96], train_loss: 0.1064, train_acc: 0.9703, val_loss: 2.6108, val_acc: 0.6683, val_precision: 0.6779, val_recall: 0.6683, val_f1: 0.6680
Epoch [97], train_loss: 0.0815, train_acc: 0.9771, val_loss: 2.6669, val_acc: 0.6589, val_precision: 0.6768, val_recall: 0.6589, val_f1: 0.6617
Epoch [98], train_loss: 0.1226, train_acc: 0.9650, val_loss: 2.4891, val_acc: 0.6708, val_precision: 0.6881, val_recall: 0.6708, val_f1: 0.6747
Epoch [99], train_loss: 0.0953, train_acc: 0.9733, val_loss: 2.6364, val_acc: 0.6754, val_precision: 0.6879, val_recall: 0.6754, val_f1: 0.6770
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.9225, val_acc: 0.6722, val_precision: 0.6817, val_recall: 0.6722, val_f1: 0.6656
Summary result of test set => last model => val_loss: 2.6242, val_acc: 0.6756, val_precision: 0.6886, val_recall: 0.6756, val_f1: 0.6768
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.6703
--
confusion matrix
[[730  24  36  19  37   3  11  18  84  38]
 [ 18 854   0   5   9   1   9   6  28  70]
 [ 73   6 433  67 188  59 107  33  24  10]
 [ 29  19  38 443 123 116 144  39  24  25]
 [ 18   7  42  47 676  17  85  95   8   5]
 [  9  11  61 230  93 443  57  68  15  13]
 [  6   6  30  30  64  13 827   8  10   6]
 [  9   5  39  39  64  47  20 753   3  21]
 [ 74  40   8  13  22   4  13   5 800  21]
 [ 33 135   5  15   7   9  16  13  23 744]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.73      0.73      0.73      1000
  automobile       0.77      0.85      0.81      1000
        bird       0.63      0.43      0.51      1000
         cat       0.49      0.44      0.46      1000
        deer       0.53      0.68      0.59      1000
         dog       0.62      0.44      0.52      1000
        frog       0.64      0.83      0.72      1000
       horse       0.73      0.75      0.74      1000
        ship       0.79      0.80      0.79      1000
       truck       0.78      0.74      0.76      1000

    accuracy                           0.67     10000
   macro avg       0.67      0.67      0.66     10000
weighted avg       0.67      0.67      0.66     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.6734
--
confusion matrix
[[353  10  20  10  33   1   3   8  36  14]
 [ 15 450   0   1   5   0   3   1  11  26]
 [ 31   2 252  34  98  15  54  25  18   3]
 [ 11   7  23 199  52  57  70  25  12  15]
 [  7   3  20  16 329   9  32  46   4   5]
 [  4   6  29 111  58 222  43  29   6   6]
 [  4   6   7  12  41   6 415   3   6   7]
 [  6   3  13  22  44  28   6 364   1  13]
 [ 33  20   8   5   9   1   4   2 411  11]
 [  6  68   3   7   6   5  11   7  16 372]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.75      0.72      0.74       488
  automobile       0.78      0.88      0.83       512
        bird       0.67      0.47      0.56       532
         cat       0.48      0.42      0.45       471
        deer       0.49      0.70      0.57       471
         dog       0.65      0.43      0.52       514
        frog       0.65      0.82      0.72       507
       horse       0.71      0.73      0.72       500
        ship       0.79      0.82      0.80       504
       truck       0.79      0.74      0.76       501

    accuracy                           0.67      5000
   macro avg       0.68      0.67      0.67      5000
weighted avg       0.68      0.67      0.67      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 432596: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 10:15:02 2024
Job was executed on host(s) <hgn47>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 10:15:37 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 10:15:37 2024
Terminated at Wed Feb 28 10:24:17 2024
Results reported at Wed Feb 28 10:24:17 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_LR_0.002_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_LR_0.002_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.00001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_00001" --model "CNN"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.002 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0.002" --model "CNN"


# Optimizer

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1563.00 sec.
    Max Memory :                                 3348 MB
    Average Memory :                             3200.79 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6892.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   521 sec.
    Turnaround time :                            555 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_LR_0.002_err_432596> for stderr output of this job.

