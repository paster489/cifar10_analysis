loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 2.1761, train_acc: 0.1927, val_loss: 2.0211, val_acc: 0.2533, val_precision: 0.2291, val_recall: 0.2533, val_f1: 0.2145
Epoch [1], train_loss: 1.9867, train_acc: 0.2677, val_loss: 1.9636, val_acc: 0.2723, val_precision: 0.2471, val_recall: 0.2723, val_f1: 0.2382
Epoch [2], train_loss: 1.9279, train_acc: 0.2908, val_loss: 1.8908, val_acc: 0.3070, val_precision: 0.2978, val_recall: 0.3070, val_f1: 0.2786
Epoch [3], train_loss: 1.8125, train_acc: 0.3395, val_loss: 1.7467, val_acc: 0.3633, val_precision: 0.3665, val_recall: 0.3633, val_f1: 0.3424
Epoch [4], train_loss: 1.6944, train_acc: 0.3805, val_loss: 1.6727, val_acc: 0.3922, val_precision: 0.3967, val_recall: 0.3922, val_f1: 0.3820
Epoch [5], train_loss: 1.6453, train_acc: 0.3976, val_loss: 1.6343, val_acc: 0.4046, val_precision: 0.4088, val_recall: 0.4046, val_f1: 0.3913
Epoch [6], train_loss: 1.6096, train_acc: 0.4104, val_loss: 1.6087, val_acc: 0.4089, val_precision: 0.4129, val_recall: 0.4089, val_f1: 0.3976
Epoch [7], train_loss: 1.5803, train_acc: 0.4220, val_loss: 1.5713, val_acc: 0.4253, val_precision: 0.4339, val_recall: 0.4253, val_f1: 0.4139
Epoch [8], train_loss: 1.5554, train_acc: 0.4311, val_loss: 1.5614, val_acc: 0.4228, val_precision: 0.4535, val_recall: 0.4228, val_f1: 0.4202
Epoch [9], train_loss: 1.5353, train_acc: 0.4401, val_loss: 1.5330, val_acc: 0.4404, val_precision: 0.4491, val_recall: 0.4404, val_f1: 0.4338
Epoch [10], train_loss: 1.5147, train_acc: 0.4492, val_loss: 1.5199, val_acc: 0.4454, val_precision: 0.4567, val_recall: 0.4454, val_f1: 0.4322
Epoch [11], train_loss: 1.5015, train_acc: 0.4546, val_loss: 1.5126, val_acc: 0.4504, val_precision: 0.4638, val_recall: 0.4504, val_f1: 0.4388
Epoch [12], train_loss: 1.4868, train_acc: 0.4599, val_loss: 1.4836, val_acc: 0.4634, val_precision: 0.4713, val_recall: 0.4634, val_f1: 0.4506
Epoch [13], train_loss: 1.4712, train_acc: 0.4662, val_loss: 1.5019, val_acc: 0.4597, val_precision: 0.4727, val_recall: 0.4597, val_f1: 0.4430
Epoch [14], train_loss: 1.4583, train_acc: 0.4722, val_loss: 1.4758, val_acc: 0.4659, val_precision: 0.4806, val_recall: 0.4659, val_f1: 0.4606
Epoch [15], train_loss: 1.4495, train_acc: 0.4748, val_loss: 1.4507, val_acc: 0.4782, val_precision: 0.4912, val_recall: 0.4782, val_f1: 0.4721
Epoch [16], train_loss: 1.4398, train_acc: 0.4791, val_loss: 1.4462, val_acc: 0.4797, val_precision: 0.4820, val_recall: 0.4797, val_f1: 0.4676
Epoch [17], train_loss: 1.4273, train_acc: 0.4860, val_loss: 1.4332, val_acc: 0.4834, val_precision: 0.4981, val_recall: 0.4834, val_f1: 0.4777
Epoch [18], train_loss: 1.4181, train_acc: 0.4881, val_loss: 1.4331, val_acc: 0.4844, val_precision: 0.5005, val_recall: 0.4844, val_f1: 0.4820
Epoch [19], train_loss: 1.4077, train_acc: 0.4910, val_loss: 1.4244, val_acc: 0.4857, val_precision: 0.4991, val_recall: 0.4857, val_f1: 0.4786
Epoch [20], train_loss: 1.3995, train_acc: 0.4970, val_loss: 1.4158, val_acc: 0.4867, val_precision: 0.4984, val_recall: 0.4867, val_f1: 0.4832
Epoch [21], train_loss: 1.3890, train_acc: 0.4997, val_loss: 1.4027, val_acc: 0.4943, val_precision: 0.5035, val_recall: 0.4943, val_f1: 0.4912
Epoch [22], train_loss: 1.3765, train_acc: 0.5039, val_loss: 1.3916, val_acc: 0.4976, val_precision: 0.5062, val_recall: 0.4976, val_f1: 0.4901
Epoch [23], train_loss: 1.3702, train_acc: 0.5073, val_loss: 1.3824, val_acc: 0.5001, val_precision: 0.5072, val_recall: 0.5001, val_f1: 0.4955
Epoch [24], train_loss: 1.3578, train_acc: 0.5123, val_loss: 1.3687, val_acc: 0.5077, val_precision: 0.5212, val_recall: 0.5077, val_f1: 0.5032
Epoch [25], train_loss: 1.3485, train_acc: 0.5147, val_loss: 1.3578, val_acc: 0.5133, val_precision: 0.5195, val_recall: 0.5133, val_f1: 0.5071
Epoch [26], train_loss: 1.3382, train_acc: 0.5196, val_loss: 1.3529, val_acc: 0.5172, val_precision: 0.5229, val_recall: 0.5172, val_f1: 0.5104
Epoch [27], train_loss: 1.3321, train_acc: 0.5235, val_loss: 1.3733, val_acc: 0.5128, val_precision: 0.5227, val_recall: 0.5128, val_f1: 0.5090
Epoch [28], train_loss: 1.3216, train_acc: 0.5256, val_loss: 1.3513, val_acc: 0.5107, val_precision: 0.5285, val_recall: 0.5107, val_f1: 0.5049
Epoch [29], train_loss: 1.3118, train_acc: 0.5301, val_loss: 1.3350, val_acc: 0.5233, val_precision: 0.5321, val_recall: 0.5233, val_f1: 0.5188
Epoch [30], train_loss: 1.3030, train_acc: 0.5333, val_loss: 1.3278, val_acc: 0.5268, val_precision: 0.5354, val_recall: 0.5268, val_f1: 0.5206
Epoch [31], train_loss: 1.2939, train_acc: 0.5371, val_loss: 1.3279, val_acc: 0.5283, val_precision: 0.5417, val_recall: 0.5283, val_f1: 0.5236
Epoch [32], train_loss: 1.2836, train_acc: 0.5415, val_loss: 1.3043, val_acc: 0.5349, val_precision: 0.5432, val_recall: 0.5349, val_f1: 0.5281
Epoch [33], train_loss: 1.2771, train_acc: 0.5445, val_loss: 1.3045, val_acc: 0.5413, val_precision: 0.5504, val_recall: 0.5413, val_f1: 0.5368
Epoch [34], train_loss: 1.2686, train_acc: 0.5499, val_loss: 1.3222, val_acc: 0.5277, val_precision: 0.5318, val_recall: 0.5277, val_f1: 0.5193
Epoch [35], train_loss: 1.2602, train_acc: 0.5501, val_loss: 1.3064, val_acc: 0.5435, val_precision: 0.5583, val_recall: 0.5435, val_f1: 0.5444
Epoch [36], train_loss: 1.2500, train_acc: 0.5544, val_loss: 1.2722, val_acc: 0.5499, val_precision: 0.5574, val_recall: 0.5499, val_f1: 0.5460
Epoch [37], train_loss: 1.2425, train_acc: 0.5568, val_loss: 1.2741, val_acc: 0.5449, val_precision: 0.5513, val_recall: 0.5449, val_f1: 0.5383
Epoch [38], train_loss: 1.2340, train_acc: 0.5595, val_loss: 1.2643, val_acc: 0.5512, val_precision: 0.5618, val_recall: 0.5512, val_f1: 0.5494
Epoch [39], train_loss: 1.2259, train_acc: 0.5651, val_loss: 1.2591, val_acc: 0.5553, val_precision: 0.5635, val_recall: 0.5553, val_f1: 0.5496
Epoch [40], train_loss: 1.2175, train_acc: 0.5677, val_loss: 1.2537, val_acc: 0.5590, val_precision: 0.5683, val_recall: 0.5590, val_f1: 0.5526
Epoch [41], train_loss: 1.2090, train_acc: 0.5688, val_loss: 1.2438, val_acc: 0.5575, val_precision: 0.5635, val_recall: 0.5575, val_f1: 0.5518
Epoch [42], train_loss: 1.2016, train_acc: 0.5737, val_loss: 1.2456, val_acc: 0.5583, val_precision: 0.5691, val_recall: 0.5583, val_f1: 0.5530
Epoch [43], train_loss: 1.1952, train_acc: 0.5764, val_loss: 1.2390, val_acc: 0.5669, val_precision: 0.5767, val_recall: 0.5669, val_f1: 0.5643
Epoch [44], train_loss: 1.1861, train_acc: 0.5799, val_loss: 1.2392, val_acc: 0.5598, val_precision: 0.5607, val_recall: 0.5598, val_f1: 0.5518
Epoch [45], train_loss: 1.1789, train_acc: 0.5830, val_loss: 1.2306, val_acc: 0.5648, val_precision: 0.5807, val_recall: 0.5648, val_f1: 0.5640
Epoch [46], train_loss: 1.1706, train_acc: 0.5876, val_loss: 1.2219, val_acc: 0.5708, val_precision: 0.5789, val_recall: 0.5708, val_f1: 0.5686
Epoch [47], train_loss: 1.1650, train_acc: 0.5876, val_loss: 1.2096, val_acc: 0.5704, val_precision: 0.5743, val_recall: 0.5704, val_f1: 0.5631
Epoch [48], train_loss: 1.1548, train_acc: 0.5922, val_loss: 1.2059, val_acc: 0.5715, val_precision: 0.5748, val_recall: 0.5715, val_f1: 0.5664
Epoch [49], train_loss: 1.1481, train_acc: 0.5958, val_loss: 1.1981, val_acc: 0.5755, val_precision: 0.5875, val_recall: 0.5755, val_f1: 0.5731
Epoch [50], train_loss: 1.1426, train_acc: 0.5941, val_loss: 1.1999, val_acc: 0.5720, val_precision: 0.5878, val_recall: 0.5720, val_f1: 0.5689
Epoch [51], train_loss: 1.1347, train_acc: 0.5994, val_loss: 1.1874, val_acc: 0.5832, val_precision: 0.5981, val_recall: 0.5832, val_f1: 0.5845
Epoch [52], train_loss: 1.1271, train_acc: 0.6020, val_loss: 1.1872, val_acc: 0.5768, val_precision: 0.5769, val_recall: 0.5768, val_f1: 0.5666
Epoch [53], train_loss: 1.1223, train_acc: 0.6054, val_loss: 1.1773, val_acc: 0.5887, val_precision: 0.5948, val_recall: 0.5887, val_f1: 0.5841
Epoch [54], train_loss: 1.1146, train_acc: 0.6066, val_loss: 1.1820, val_acc: 0.5785, val_precision: 0.5873, val_recall: 0.5785, val_f1: 0.5704
Epoch [55], train_loss: 1.1100, train_acc: 0.6072, val_loss: 1.1842, val_acc: 0.5825, val_precision: 0.5871, val_recall: 0.5825, val_f1: 0.5725
Epoch [56], train_loss: 1.0996, train_acc: 0.6118, val_loss: 1.1577, val_acc: 0.5949, val_precision: 0.6079, val_recall: 0.5949, val_f1: 0.5957
Epoch [57], train_loss: 1.0934, train_acc: 0.6148, val_loss: 1.1692, val_acc: 0.5858, val_precision: 0.6025, val_recall: 0.5858, val_f1: 0.5836
Epoch [58], train_loss: 1.0876, train_acc: 0.6156, val_loss: 1.1638, val_acc: 0.5890, val_precision: 0.6080, val_recall: 0.5890, val_f1: 0.5894
Epoch [59], train_loss: 1.0825, train_acc: 0.6196, val_loss: 1.1828, val_acc: 0.5855, val_precision: 0.5868, val_recall: 0.5855, val_f1: 0.5744
Epoch [60], train_loss: 1.0716, train_acc: 0.6229, val_loss: 1.1397, val_acc: 0.5984, val_precision: 0.6105, val_recall: 0.5984, val_f1: 0.5988
Epoch [61], train_loss: 1.0638, train_acc: 0.6254, val_loss: 1.1484, val_acc: 0.5949, val_precision: 0.6062, val_recall: 0.5949, val_f1: 0.5860
Epoch [62], train_loss: 1.0610, train_acc: 0.6262, val_loss: 1.1290, val_acc: 0.6062, val_precision: 0.6114, val_recall: 0.6062, val_f1: 0.6011
Epoch [63], train_loss: 1.0555, train_acc: 0.6278, val_loss: 1.1242, val_acc: 0.6019, val_precision: 0.6146, val_recall: 0.6019, val_f1: 0.6027
Epoch [64], train_loss: 1.0480, train_acc: 0.6319, val_loss: 1.1211, val_acc: 0.6010, val_precision: 0.6127, val_recall: 0.6010, val_f1: 0.5999
Epoch [65], train_loss: 1.0406, train_acc: 0.6346, val_loss: 1.1156, val_acc: 0.6059, val_precision: 0.6182, val_recall: 0.6059, val_f1: 0.6050
Epoch [66], train_loss: 1.0384, train_acc: 0.6331, val_loss: 1.1124, val_acc: 0.6103, val_precision: 0.6183, val_recall: 0.6103, val_f1: 0.6069
Epoch [67], train_loss: 1.0324, train_acc: 0.6361, val_loss: 1.1187, val_acc: 0.6006, val_precision: 0.6160, val_recall: 0.6006, val_f1: 0.6007
Epoch [68], train_loss: 1.0252, train_acc: 0.6384, val_loss: 1.1153, val_acc: 0.6068, val_precision: 0.6137, val_recall: 0.6068, val_f1: 0.6019
Epoch [69], train_loss: 1.0186, train_acc: 0.6428, val_loss: 1.1118, val_acc: 0.6117, val_precision: 0.6187, val_recall: 0.6117, val_f1: 0.6054
Epoch [70], train_loss: 1.0148, train_acc: 0.6447, val_loss: 1.1377, val_acc: 0.5964, val_precision: 0.6103, val_recall: 0.5964, val_f1: 0.5883
Epoch [71], train_loss: 1.0097, train_acc: 0.6451, val_loss: 1.0970, val_acc: 0.6103, val_precision: 0.6291, val_recall: 0.6103, val_f1: 0.6116
Epoch [72], train_loss: 1.0030, train_acc: 0.6470, val_loss: 1.0971, val_acc: 0.6190, val_precision: 0.6267, val_recall: 0.6190, val_f1: 0.6129
Epoch [73], train_loss: 0.9998, train_acc: 0.6490, val_loss: 1.0936, val_acc: 0.6198, val_precision: 0.6272, val_recall: 0.6198, val_f1: 0.6139
Epoch [74], train_loss: 0.9973, train_acc: 0.6492, val_loss: 1.0933, val_acc: 0.6105, val_precision: 0.6371, val_recall: 0.6105, val_f1: 0.6105
Epoch [75], train_loss: 0.9869, train_acc: 0.6535, val_loss: 1.0732, val_acc: 0.6209, val_precision: 0.6273, val_recall: 0.6209, val_f1: 0.6186
Epoch [76], train_loss: 0.9838, train_acc: 0.6537, val_loss: 1.0731, val_acc: 0.6210, val_precision: 0.6292, val_recall: 0.6210, val_f1: 0.6179
Epoch [77], train_loss: 0.9771, train_acc: 0.6559, val_loss: 1.0744, val_acc: 0.6204, val_precision: 0.6328, val_recall: 0.6204, val_f1: 0.6193
Epoch [78], train_loss: 0.9710, train_acc: 0.6581, val_loss: 1.0814, val_acc: 0.6191, val_precision: 0.6421, val_recall: 0.6191, val_f1: 0.6190
Epoch [79], train_loss: 0.9651, train_acc: 0.6621, val_loss: 1.0772, val_acc: 0.6195, val_precision: 0.6319, val_recall: 0.6195, val_f1: 0.6172
Epoch [80], train_loss: 0.9597, train_acc: 0.6649, val_loss: 1.0872, val_acc: 0.6197, val_precision: 0.6472, val_recall: 0.6197, val_f1: 0.6219
Epoch [81], train_loss: 0.9558, train_acc: 0.6643, val_loss: 1.0795, val_acc: 0.6212, val_precision: 0.6262, val_recall: 0.6212, val_f1: 0.6134
Epoch [82], train_loss: 0.9525, train_acc: 0.6655, val_loss: 1.0704, val_acc: 0.6185, val_precision: 0.6278, val_recall: 0.6185, val_f1: 0.6142
Epoch [83], train_loss: 0.9472, train_acc: 0.6677, val_loss: 1.0695, val_acc: 0.6197, val_precision: 0.6405, val_recall: 0.6197, val_f1: 0.6216
Epoch [84], train_loss: 0.9409, train_acc: 0.6687, val_loss: 1.0748, val_acc: 0.6214, val_precision: 0.6428, val_recall: 0.6214, val_f1: 0.6218
Epoch [85], train_loss: 0.9383, train_acc: 0.6712, val_loss: 1.0421, val_acc: 0.6314, val_precision: 0.6359, val_recall: 0.6314, val_f1: 0.6267
Epoch [86], train_loss: 0.9315, train_acc: 0.6738, val_loss: 1.0474, val_acc: 0.6313, val_precision: 0.6412, val_recall: 0.6313, val_f1: 0.6290
Epoch [87], train_loss: 0.9259, train_acc: 0.6738, val_loss: 1.0330, val_acc: 0.6376, val_precision: 0.6439, val_recall: 0.6376, val_f1: 0.6356
Epoch [88], train_loss: 0.9205, train_acc: 0.6778, val_loss: 1.0414, val_acc: 0.6325, val_precision: 0.6389, val_recall: 0.6325, val_f1: 0.6274
Epoch [89], train_loss: 0.9140, train_acc: 0.6791, val_loss: 1.0378, val_acc: 0.6325, val_precision: 0.6397, val_recall: 0.6325, val_f1: 0.6287
Epoch [90], train_loss: 0.9103, train_acc: 0.6814, val_loss: 1.0274, val_acc: 0.6374, val_precision: 0.6429, val_recall: 0.6374, val_f1: 0.6346
Epoch [91], train_loss: 0.9095, train_acc: 0.6800, val_loss: 1.0520, val_acc: 0.6263, val_precision: 0.6367, val_recall: 0.6263, val_f1: 0.6239
Epoch [92], train_loss: 0.9003, train_acc: 0.6850, val_loss: 1.0238, val_acc: 0.6419, val_precision: 0.6454, val_recall: 0.6419, val_f1: 0.6379
Epoch [93], train_loss: 0.8933, train_acc: 0.6869, val_loss: 1.0223, val_acc: 0.6365, val_precision: 0.6430, val_recall: 0.6365, val_f1: 0.6341
Epoch [94], train_loss: 0.8921, train_acc: 0.6884, val_loss: 1.0201, val_acc: 0.6428, val_precision: 0.6460, val_recall: 0.6428, val_f1: 0.6363
Epoch [95], train_loss: 0.8838, train_acc: 0.6901, val_loss: 1.0213, val_acc: 0.6450, val_precision: 0.6560, val_recall: 0.6450, val_f1: 0.6445
Epoch [96], train_loss: 0.8803, train_acc: 0.6917, val_loss: 1.0272, val_acc: 0.6375, val_precision: 0.6488, val_recall: 0.6375, val_f1: 0.6353
Epoch [97], train_loss: 0.8752, train_acc: 0.6943, val_loss: 1.0613, val_acc: 0.6201, val_precision: 0.6507, val_recall: 0.6201, val_f1: 0.6238
Epoch [98], train_loss: 0.8686, train_acc: 0.6947, val_loss: 1.0233, val_acc: 0.6391, val_precision: 0.6526, val_recall: 0.6391, val_f1: 0.6360
Epoch [99], train_loss: 0.8622, train_acc: 0.6985, val_loss: 1.0198, val_acc: 0.6408, val_precision: 0.6537, val_recall: 0.6408, val_f1: 0.6407
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 1.0123, val_acc: 0.6412, val_precision: 0.6566, val_recall: 0.6412, val_f1: 0.6405
Summary result of test set => last model => val_loss: 1.0123, val_acc: 0.6412, val_precision: 0.6566, val_recall: 0.6412, val_f1: 0.6405
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.6401
--
confusion matrix
[[675  40  93  22  12   8   8  12  75  55]
 [ 22 798  14   5   3   6   7   5  24 116]
 [ 59  12 584  86  69  78  35  40  16  21]
 [ 22  13 109 492  31 176  45  56  18  38]
 [ 22   9 174  91 451  52  64 119   9   9]
 [  9   7  88 205  29 545  12  81   9  15]
 [ 12   9  73 112  47  28 665  25  11  18]
 [ 16   7  41  57  46  65  10 723   6  29]
 [ 99  57  25  21   5   9   2   3 744  35]
 [ 38 135  17  27   2   7   7  18  25 724]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.69      0.68      0.68      1000
  automobile       0.73      0.80      0.76      1000
        bird       0.48      0.58      0.53      1000
         cat       0.44      0.49      0.46      1000
        deer       0.65      0.45      0.53      1000
         dog       0.56      0.55      0.55      1000
        frog       0.78      0.67      0.72      1000
       horse       0.67      0.72      0.69      1000
        ship       0.79      0.74      0.77      1000
       truck       0.68      0.72      0.70      1000

    accuracy                           0.64     10000
   macro avg       0.65      0.64      0.64     10000
weighted avg       0.65      0.64      0.64     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.6410
--
confusion matrix
[[339  13  42   9  11   5   0   8  35  26]
 [  5 428   4   3   3   0   1   3  18  47]
 [ 27   6 331  41  34  28  24  20  12   9]
 [ 13   9  53 210  15 108  14  24   8  17]
 [ 12   4  75  40 215  31  24  53   8   9]
 [  4   4  49  95  15 284  15  33   3  12]
 [  7   5  44  63  28   6 321   9   7  17]
 [  7   4  27  35  30  28   3 354   2  10]
 [ 59  27   8   9   1   2   1   2 369  26]
 [ 11  76  11  12   2   3   7  13  12 354]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.70      0.69      0.70       488
  automobile       0.74      0.84      0.79       512
        bird       0.51      0.62      0.56       532
         cat       0.41      0.45      0.43       471
        deer       0.61      0.46      0.52       471
         dog       0.57      0.55      0.56       514
        frog       0.78      0.63      0.70       507
       horse       0.68      0.71      0.69       500
        ship       0.78      0.73      0.75       504
       truck       0.67      0.71      0.69       501

    accuracy                           0.64      5000
   macro avg       0.65      0.64      0.64      5000
weighted avg       0.65      0.64      0.64      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 432580: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 10:14:01 2024
Job was executed on host(s) <hgn43>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 10:14:14 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 10:14:14 2024
Terminated at Wed Feb 28 10:23:07 2024
Results reported at Wed Feb 28 10:23:07 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_LR_00001_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_LR_00001_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.00001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_00001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1559.00 sec.
    Max Memory :                                 3575 MB
    Average Memory :                             3201.83 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6665.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                141
    Run time :                                   535 sec.
    Turnaround time :                            546 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_LR_00001_err_432580> for stderr output of this job.

