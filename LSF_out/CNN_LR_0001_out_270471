loading ...
loaded conda.sh
sh shell detected
main => start
 
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Train/Validation random split => start
 
DataLoader => start
 
To_device => start
 
Train => start
 
Epoch [0], train_loss: 1.9286, train_acc: 0.2847, val_loss: 1.6390, val_acc: 0.3979
Epoch [1], train_loss: 1.5667, train_acc: 0.4191, val_loss: 1.4941, val_acc: 0.4614
Epoch [2], train_loss: 1.4235, train_acc: 0.4779, val_loss: 1.3725, val_acc: 0.5088
Epoch [3], train_loss: 1.3208, train_acc: 0.5183, val_loss: 1.2731, val_acc: 0.5416
Epoch [4], train_loss: 1.2452, train_acc: 0.5495, val_loss: 1.2347, val_acc: 0.5558
Epoch [5], train_loss: 1.1670, train_acc: 0.5792, val_loss: 1.1405, val_acc: 0.5904
Epoch [6], train_loss: 1.0855, train_acc: 0.6106, val_loss: 1.0867, val_acc: 0.6112
Epoch [7], train_loss: 1.0224, train_acc: 0.6364, val_loss: 1.0611, val_acc: 0.6223
Epoch [8], train_loss: 0.9662, train_acc: 0.6573, val_loss: 0.9997, val_acc: 0.6386
Epoch [9], train_loss: 0.9141, train_acc: 0.6759, val_loss: 0.9483, val_acc: 0.6537
Epoch [10], train_loss: 0.8539, train_acc: 0.6988, val_loss: 0.9114, val_acc: 0.6764
Epoch [11], train_loss: 0.8065, train_acc: 0.7164, val_loss: 0.8768, val_acc: 0.6833
Epoch [12], train_loss: 0.7620, train_acc: 0.7341, val_loss: 0.8317, val_acc: 0.7040
Epoch [13], train_loss: 0.7193, train_acc: 0.7466, val_loss: 0.8424, val_acc: 0.7115
Epoch [14], train_loss: 0.6657, train_acc: 0.7666, val_loss: 0.8185, val_acc: 0.7131
Epoch [15], train_loss: 0.6247, train_acc: 0.7825, val_loss: 0.7908, val_acc: 0.7295
Epoch [16], train_loss: 0.5747, train_acc: 0.7999, val_loss: 0.8096, val_acc: 0.7336
Epoch [17], train_loss: 0.5338, train_acc: 0.8145, val_loss: 0.7905, val_acc: 0.7373
Epoch [18], train_loss: 0.4827, train_acc: 0.8334, val_loss: 0.8107, val_acc: 0.7318
Epoch [19], train_loss: 0.4331, train_acc: 0.8500, val_loss: 0.7975, val_acc: 0.7408
Epoch [20], train_loss: 0.3831, train_acc: 0.8671, val_loss: 0.8594, val_acc: 0.7285
Epoch [21], train_loss: 0.3374, train_acc: 0.8835, val_loss: 0.8445, val_acc: 0.7391
Epoch [22], train_loss: 0.2845, train_acc: 0.9015, val_loss: 0.8902, val_acc: 0.7424
Epoch [23], train_loss: 0.2388, train_acc: 0.9172, val_loss: 1.0359, val_acc: 0.7145
Epoch [24], train_loss: 0.1912, train_acc: 0.9349, val_loss: 1.0237, val_acc: 0.7431
Epoch [25], train_loss: 0.1556, train_acc: 0.9479, val_loss: 1.0597, val_acc: 0.7404
Epoch [26], train_loss: 0.1123, train_acc: 0.9626, val_loss: 1.1815, val_acc: 0.7313
Epoch [27], train_loss: 0.1041, train_acc: 0.9646, val_loss: 1.2170, val_acc: 0.7369
Epoch [28], train_loss: 0.0694, train_acc: 0.9777, val_loss: 1.4072, val_acc: 0.7253
Epoch [29], train_loss: 0.0610, train_acc: 0.9799, val_loss: 1.4201, val_acc: 0.7329
Epoch [30], train_loss: 0.0602, train_acc: 0.9799, val_loss: 1.5890, val_acc: 0.7289
Epoch [31], train_loss: 0.0605, train_acc: 0.9794, val_loss: 1.4320, val_acc: 0.7418
Epoch [32], train_loss: 0.0418, train_acc: 0.9864, val_loss: 1.5648, val_acc: 0.7435
Epoch [33], train_loss: 0.0451, train_acc: 0.9848, val_loss: 1.6045, val_acc: 0.7238
Epoch [34], train_loss: 0.0464, train_acc: 0.9848, val_loss: 1.5722, val_acc: 0.7340
Epoch [35], train_loss: 0.0349, train_acc: 0.9882, val_loss: 1.6200, val_acc: 0.7427
Epoch [36], train_loss: 0.0383, train_acc: 0.9868, val_loss: 1.7603, val_acc: 0.7259
Epoch [37], train_loss: 0.0336, train_acc: 0.9889, val_loss: 1.6603, val_acc: 0.7434
Epoch [38], train_loss: 0.0374, train_acc: 0.9870, val_loss: 1.6360, val_acc: 0.7434
Epoch [39], train_loss: 0.0278, train_acc: 0.9913, val_loss: 1.7606, val_acc: 0.7443
Epoch [40], train_loss: 0.0304, train_acc: 0.9901, val_loss: 1.7629, val_acc: 0.7331
Epoch [41], train_loss: 0.0341, train_acc: 0.9882, val_loss: 1.7600, val_acc: 0.7425
Epoch [42], train_loss: 0.0290, train_acc: 0.9900, val_loss: 1.7241, val_acc: 0.7410
Epoch [43], train_loss: 0.0217, train_acc: 0.9930, val_loss: 1.9343, val_acc: 0.7445
Epoch [44], train_loss: 0.0261, train_acc: 0.9913, val_loss: 1.8048, val_acc: 0.7357
Epoch [45], train_loss: 0.0253, train_acc: 0.9914, val_loss: 1.7390, val_acc: 0.7298
Epoch [46], train_loss: 0.0304, train_acc: 0.9897, val_loss: 1.8339, val_acc: 0.7366
Epoch [47], train_loss: 0.0308, train_acc: 0.9890, val_loss: 1.8641, val_acc: 0.7334
Epoch [48], train_loss: 0.0148, train_acc: 0.9951, val_loss: 1.9965, val_acc: 0.7432
Epoch [49], train_loss: 0.0334, train_acc: 0.9887, val_loss: 1.7351, val_acc: 0.7383
Epoch [50], train_loss: 0.0193, train_acc: 0.9938, val_loss: 1.9316, val_acc: 0.7448
Epoch [51], train_loss: 0.0110, train_acc: 0.9966, val_loss: 2.0148, val_acc: 0.7226
Epoch [52], train_loss: 0.0356, train_acc: 0.9884, val_loss: 1.8324, val_acc: 0.7375
Epoch [53], train_loss: 0.0246, train_acc: 0.9917, val_loss: 1.8223, val_acc: 0.7400
Epoch [54], train_loss: 0.0210, train_acc: 0.9931, val_loss: 1.9165, val_acc: 0.7366
Epoch [55], train_loss: 0.0205, train_acc: 0.9933, val_loss: 2.0106, val_acc: 0.7435
Epoch [56], train_loss: 0.0175, train_acc: 0.9942, val_loss: 1.9343, val_acc: 0.7440
Epoch [57], train_loss: 0.0183, train_acc: 0.9938, val_loss: 2.2668, val_acc: 0.7236
Epoch [58], train_loss: 0.0226, train_acc: 0.9923, val_loss: 1.9886, val_acc: 0.7491
Epoch [59], train_loss: 0.0208, train_acc: 0.9930, val_loss: 1.8643, val_acc: 0.7385
Epoch [60], train_loss: 0.0168, train_acc: 0.9946, val_loss: 1.8773, val_acc: 0.7460
Epoch [61], train_loss: 0.0201, train_acc: 0.9933, val_loss: 1.9763, val_acc: 0.7417
Epoch [62], train_loss: 0.0210, train_acc: 0.9931, val_loss: 1.8867, val_acc: 0.7424
Epoch [63], train_loss: 0.0182, train_acc: 0.9935, val_loss: 1.8461, val_acc: 0.7497
Epoch [64], train_loss: 0.0151, train_acc: 0.9951, val_loss: 2.0260, val_acc: 0.7297
Epoch [65], train_loss: 0.0179, train_acc: 0.9945, val_loss: 2.0866, val_acc: 0.7358
Epoch [66], train_loss: 0.0182, train_acc: 0.9938, val_loss: 2.0645, val_acc: 0.7426
Epoch [67], train_loss: 0.0078, train_acc: 0.9977, val_loss: 2.0171, val_acc: 0.7504
Epoch [68], train_loss: 0.0167, train_acc: 0.9948, val_loss: 1.8949, val_acc: 0.7491
Epoch [69], train_loss: 0.0091, train_acc: 0.9968, val_loss: 2.1548, val_acc: 0.7440
Epoch [70], train_loss: 0.0233, train_acc: 0.9922, val_loss: 1.9428, val_acc: 0.7335
Epoch [71], train_loss: 0.0183, train_acc: 0.9935, val_loss: 2.0051, val_acc: 0.7479
Epoch [72], train_loss: 0.0139, train_acc: 0.9952, val_loss: 2.0836, val_acc: 0.7389
Epoch [73], train_loss: 0.0106, train_acc: 0.9964, val_loss: 1.9833, val_acc: 0.7518
Epoch [74], train_loss: 0.0057, train_acc: 0.9982, val_loss: 2.0578, val_acc: 0.7436
Epoch [75], train_loss: 0.0228, train_acc: 0.9922, val_loss: 1.9040, val_acc: 0.7491
Epoch [76], train_loss: 0.0132, train_acc: 0.9952, val_loss: 2.0084, val_acc: 0.7497
Epoch [77], train_loss: 0.0150, train_acc: 0.9953, val_loss: 1.9932, val_acc: 0.7415
Epoch [78], train_loss: 0.0115, train_acc: 0.9963, val_loss: 2.0487, val_acc: 0.7435
Epoch [79], train_loss: 0.0148, train_acc: 0.9949, val_loss: 2.0629, val_acc: 0.7386
Epoch [80], train_loss: 0.0145, train_acc: 0.9948, val_loss: 2.0669, val_acc: 0.7358
Epoch [81], train_loss: 0.0063, train_acc: 0.9982, val_loss: 2.0870, val_acc: 0.7544
Epoch [82], train_loss: 0.0211, train_acc: 0.9932, val_loss: 1.8470, val_acc: 0.7417
Epoch [83], train_loss: 0.0127, train_acc: 0.9957, val_loss: 2.0615, val_acc: 0.7481
Epoch [84], train_loss: 0.0086, train_acc: 0.9971, val_loss: 2.0477, val_acc: 0.7489
Epoch [85], train_loss: 0.0160, train_acc: 0.9946, val_loss: 1.9979, val_acc: 0.7383
Epoch [86], train_loss: 0.0107, train_acc: 0.9962, val_loss: 2.1383, val_acc: 0.7525
Epoch [87], train_loss: 0.0068, train_acc: 0.9980, val_loss: 2.0634, val_acc: 0.7473
Epoch [88], train_loss: 0.0112, train_acc: 0.9961, val_loss: 1.9991, val_acc: 0.7507
Epoch [89], train_loss: 0.0098, train_acc: 0.9970, val_loss: 2.0960, val_acc: 0.7570
Epoch [90], train_loss: 0.0115, train_acc: 0.9964, val_loss: 2.1607, val_acc: 0.7363
Epoch [91], train_loss: 0.0098, train_acc: 0.9968, val_loss: 2.0959, val_acc: 0.7560
Epoch [92], train_loss: 0.0153, train_acc: 0.9944, val_loss: 2.0239, val_acc: 0.7534
Epoch [93], train_loss: 0.0070, train_acc: 0.9977, val_loss: 2.0471, val_acc: 0.7499
Epoch [94], train_loss: 0.0102, train_acc: 0.9967, val_loss: 1.9906, val_acc: 0.7446
Epoch [95], train_loss: 0.0147, train_acc: 0.9950, val_loss: 2.0578, val_acc: 0.7533
Epoch [96], train_loss: 0.0074, train_acc: 0.9977, val_loss: 2.0359, val_acc: 0.7615
Epoch [97], train_loss: 0.0086, train_acc: 0.9970, val_loss: 2.1176, val_acc: 0.7436
Epoch [98], train_loss: 0.0131, train_acc: 0.9952, val_loss: 2.1758, val_acc: 0.7526
Epoch [99], train_loss: 0.0048, train_acc: 0.9984, val_loss: 2.3301, val_acc: 0.7481
 
Visualize trining => save images
 
Load the model => start
 
Check best/last models => start
 
Summary result of test set => best model  {'val_loss': 0.8041881918907166, 'val_acc': 0.7295898199081421}
Summary result of test set => last model {'val_loss': 2.436166763305664, 'val_acc': 0.740917980670929}
Test set evaluation => save results for postprocessing
 
** accuracy: 0.728
--
confusion matrix
[[801  15  41  10  27   1   5  13  59  28]
 [ 25 828   4   8   4   3   7   4  34  83]
 [ 63   7 604  58 132  35  38  41  15   7]
 [ 34   9  79 524 102 119  48  49  20  16]
 [ 11   5  62  36 756  11  30  76  12   1]
 [ 14   6  61 171  83 549  20  80  10   6]
 [ 13   2  32  59  84   9 772   8  17   4]
 [ 16   1  24  35  72  25   4 811   5   7]
 [ 77  26   7  16   5   2   8   4 842  13]
 [ 48  76   8  12  11   3   3  22  28 789]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.73      0.80      0.76      1000
  automobile       0.85      0.83      0.84      1000
        bird       0.66      0.60      0.63      1000
         cat       0.56      0.52      0.54      1000
        deer       0.59      0.76      0.66      1000
         dog       0.73      0.55      0.62      1000
        frog       0.83      0.77      0.80      1000
       horse       0.73      0.81      0.77      1000
        ship       0.81      0.84      0.82      1000
       truck       0.83      0.79      0.81      1000

    accuracy                           0.73     10000
   macro avg       0.73      0.73      0.73     10000
weighted avg       0.73      0.73      0.73     10000

END OF CODE
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 270471: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <access4> by user <ingap> in cluster <wexac> at Tue Feb 27 11:10:31 2024
Job was executed on host(s) <hgn53>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 11:10:45 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 11:10:45 2024
Terminated at Tue Feb 27 11:19:23 2024
Results reported at Tue Feb 27 11:19:23 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_LR_0001_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_LR_0001_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1532.00 sec.
    Max Memory :                                 3513 MB
    Average Memory :                             3238.88 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6727.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   520 sec.
    Turnaround time :                            532 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_LR_0001_err_270471> for stderr output of this job.

