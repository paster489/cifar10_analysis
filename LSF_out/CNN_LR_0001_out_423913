loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.9286, train_acc: 0.2847, val_loss: 1.6390, val_acc: 0.3979, val_precision: 0.4098, val_recall: 0.3979, val_f1: 0.3771
Epoch [1], train_loss: 1.5667, train_acc: 0.4191, val_loss: 1.4941, val_acc: 0.4614, val_precision: 0.4694, val_recall: 0.4614, val_f1: 0.4483
Epoch [2], train_loss: 1.4235, train_acc: 0.4779, val_loss: 1.3725, val_acc: 0.5088, val_precision: 0.5154, val_recall: 0.5088, val_f1: 0.4951
Epoch [3], train_loss: 1.3208, train_acc: 0.5183, val_loss: 1.2731, val_acc: 0.5416, val_precision: 0.5508, val_recall: 0.5416, val_f1: 0.5353
Epoch [4], train_loss: 1.2452, train_acc: 0.5495, val_loss: 1.2347, val_acc: 0.5558, val_precision: 0.5855, val_recall: 0.5558, val_f1: 0.5602
Epoch [5], train_loss: 1.1670, train_acc: 0.5792, val_loss: 1.1405, val_acc: 0.5904, val_precision: 0.6018, val_recall: 0.5904, val_f1: 0.5877
Epoch [6], train_loss: 1.0855, train_acc: 0.6106, val_loss: 1.0867, val_acc: 0.6112, val_precision: 0.6338, val_recall: 0.6112, val_f1: 0.6114
Epoch [7], train_loss: 1.0224, train_acc: 0.6364, val_loss: 1.0611, val_acc: 0.6223, val_precision: 0.6457, val_recall: 0.6223, val_f1: 0.6246
Epoch [8], train_loss: 0.9662, train_acc: 0.6573, val_loss: 0.9997, val_acc: 0.6386, val_precision: 0.6591, val_recall: 0.6386, val_f1: 0.6350
Epoch [9], train_loss: 0.9141, train_acc: 0.6759, val_loss: 0.9483, val_acc: 0.6537, val_precision: 0.6702, val_recall: 0.6537, val_f1: 0.6519
Epoch [10], train_loss: 0.8539, train_acc: 0.6988, val_loss: 0.9114, val_acc: 0.6764, val_precision: 0.6905, val_recall: 0.6764, val_f1: 0.6739
Epoch [11], train_loss: 0.8065, train_acc: 0.7164, val_loss: 0.8768, val_acc: 0.6833, val_precision: 0.6949, val_recall: 0.6833, val_f1: 0.6810
Epoch [12], train_loss: 0.7620, train_acc: 0.7341, val_loss: 0.8317, val_acc: 0.7040, val_precision: 0.7109, val_recall: 0.7040, val_f1: 0.7022
Epoch [13], train_loss: 0.7193, train_acc: 0.7466, val_loss: 0.8424, val_acc: 0.7115, val_precision: 0.7203, val_recall: 0.7115, val_f1: 0.7090
Epoch [14], train_loss: 0.6657, train_acc: 0.7666, val_loss: 0.8185, val_acc: 0.7131, val_precision: 0.7231, val_recall: 0.7131, val_f1: 0.7119
Epoch [15], train_loss: 0.6247, train_acc: 0.7825, val_loss: 0.7908, val_acc: 0.7295, val_precision: 0.7381, val_recall: 0.7295, val_f1: 0.7288
Epoch [16], train_loss: 0.5747, train_acc: 0.7999, val_loss: 0.8096, val_acc: 0.7336, val_precision: 0.7438, val_recall: 0.7336, val_f1: 0.7316
Epoch [17], train_loss: 0.5338, train_acc: 0.8145, val_loss: 0.7905, val_acc: 0.7373, val_precision: 0.7470, val_recall: 0.7373, val_f1: 0.7360
Epoch [18], train_loss: 0.4827, train_acc: 0.8334, val_loss: 0.8107, val_acc: 0.7318, val_precision: 0.7419, val_recall: 0.7318, val_f1: 0.7314
Epoch [19], train_loss: 0.4331, train_acc: 0.8500, val_loss: 0.7975, val_acc: 0.7408, val_precision: 0.7466, val_recall: 0.7408, val_f1: 0.7382
Epoch [20], train_loss: 0.3831, train_acc: 0.8671, val_loss: 0.8594, val_acc: 0.7285, val_precision: 0.7508, val_recall: 0.7285, val_f1: 0.7325
Epoch [21], train_loss: 0.3374, train_acc: 0.8835, val_loss: 0.8445, val_acc: 0.7391, val_precision: 0.7473, val_recall: 0.7391, val_f1: 0.7382
Epoch [22], train_loss: 0.2845, train_acc: 0.9015, val_loss: 0.8902, val_acc: 0.7424, val_precision: 0.7552, val_recall: 0.7424, val_f1: 0.7417
Epoch [23], train_loss: 0.2388, train_acc: 0.9172, val_loss: 1.0359, val_acc: 0.7145, val_precision: 0.7439, val_recall: 0.7145, val_f1: 0.7136
Epoch [24], train_loss: 0.1912, train_acc: 0.9349, val_loss: 1.0237, val_acc: 0.7431, val_precision: 0.7624, val_recall: 0.7431, val_f1: 0.7457
Epoch [25], train_loss: 0.1556, train_acc: 0.9479, val_loss: 1.0597, val_acc: 0.7404, val_precision: 0.7440, val_recall: 0.7404, val_f1: 0.7376
Epoch [26], train_loss: 0.1123, train_acc: 0.9626, val_loss: 1.1815, val_acc: 0.7313, val_precision: 0.7524, val_recall: 0.7313, val_f1: 0.7351
Epoch [27], train_loss: 0.1041, train_acc: 0.9646, val_loss: 1.2170, val_acc: 0.7369, val_precision: 0.7447, val_recall: 0.7369, val_f1: 0.7359
Epoch [28], train_loss: 0.0694, train_acc: 0.9777, val_loss: 1.4072, val_acc: 0.7253, val_precision: 0.7474, val_recall: 0.7253, val_f1: 0.7285
Epoch [29], train_loss: 0.0610, train_acc: 0.9799, val_loss: 1.4201, val_acc: 0.7329, val_precision: 0.7482, val_recall: 0.7329, val_f1: 0.7333
Epoch [30], train_loss: 0.0602, train_acc: 0.9799, val_loss: 1.5890, val_acc: 0.7289, val_precision: 0.7502, val_recall: 0.7289, val_f1: 0.7286
Epoch [31], train_loss: 0.0605, train_acc: 0.9794, val_loss: 1.4320, val_acc: 0.7418, val_precision: 0.7489, val_recall: 0.7418, val_f1: 0.7412
Epoch [32], train_loss: 0.0418, train_acc: 0.9864, val_loss: 1.5648, val_acc: 0.7435, val_precision: 0.7516, val_recall: 0.7435, val_f1: 0.7409
Epoch [33], train_loss: 0.0451, train_acc: 0.9848, val_loss: 1.6045, val_acc: 0.7238, val_precision: 0.7280, val_recall: 0.7238, val_f1: 0.7182
Epoch [34], train_loss: 0.0464, train_acc: 0.9848, val_loss: 1.5722, val_acc: 0.7340, val_precision: 0.7435, val_recall: 0.7340, val_f1: 0.7338
Epoch [35], train_loss: 0.0349, train_acc: 0.9882, val_loss: 1.6200, val_acc: 0.7427, val_precision: 0.7515, val_recall: 0.7427, val_f1: 0.7430
Epoch [36], train_loss: 0.0383, train_acc: 0.9868, val_loss: 1.7603, val_acc: 0.7259, val_precision: 0.7400, val_recall: 0.7259, val_f1: 0.7246
Epoch [37], train_loss: 0.0336, train_acc: 0.9889, val_loss: 1.6603, val_acc: 0.7434, val_precision: 0.7629, val_recall: 0.7434, val_f1: 0.7439
Epoch [38], train_loss: 0.0374, train_acc: 0.9870, val_loss: 1.6360, val_acc: 0.7434, val_precision: 0.7531, val_recall: 0.7434, val_f1: 0.7421
Epoch [39], train_loss: 0.0278, train_acc: 0.9913, val_loss: 1.7606, val_acc: 0.7443, val_precision: 0.7542, val_recall: 0.7443, val_f1: 0.7429
Epoch [40], train_loss: 0.0304, train_acc: 0.9901, val_loss: 1.7629, val_acc: 0.7331, val_precision: 0.7385, val_recall: 0.7331, val_f1: 0.7296
Epoch [41], train_loss: 0.0341, train_acc: 0.9882, val_loss: 1.7600, val_acc: 0.7425, val_precision: 0.7538, val_recall: 0.7425, val_f1: 0.7427
Epoch [42], train_loss: 0.0290, train_acc: 0.9900, val_loss: 1.7241, val_acc: 0.7410, val_precision: 0.7476, val_recall: 0.7410, val_f1: 0.7388
Epoch [43], train_loss: 0.0217, train_acc: 0.9930, val_loss: 1.9343, val_acc: 0.7445, val_precision: 0.7584, val_recall: 0.7445, val_f1: 0.7448
Epoch [44], train_loss: 0.0261, train_acc: 0.9913, val_loss: 1.8048, val_acc: 0.7357, val_precision: 0.7457, val_recall: 0.7357, val_f1: 0.7355
Epoch [45], train_loss: 0.0253, train_acc: 0.9914, val_loss: 1.7390, val_acc: 0.7298, val_precision: 0.7409, val_recall: 0.7298, val_f1: 0.7287
Epoch [46], train_loss: 0.0304, train_acc: 0.9897, val_loss: 1.8339, val_acc: 0.7366, val_precision: 0.7557, val_recall: 0.7366, val_f1: 0.7391
Epoch [47], train_loss: 0.0308, train_acc: 0.9890, val_loss: 1.8641, val_acc: 0.7334, val_precision: 0.7508, val_recall: 0.7334, val_f1: 0.7343
Epoch [48], train_loss: 0.0148, train_acc: 0.9951, val_loss: 1.9965, val_acc: 0.7432, val_precision: 0.7489, val_recall: 0.7432, val_f1: 0.7415
Epoch [49], train_loss: 0.0334, train_acc: 0.9887, val_loss: 1.7351, val_acc: 0.7383, val_precision: 0.7480, val_recall: 0.7383, val_f1: 0.7361
Epoch [50], train_loss: 0.0193, train_acc: 0.9938, val_loss: 1.9316, val_acc: 0.7448, val_precision: 0.7554, val_recall: 0.7448, val_f1: 0.7439
Epoch [51], train_loss: 0.0110, train_acc: 0.9966, val_loss: 2.0148, val_acc: 0.7226, val_precision: 0.7355, val_recall: 0.7226, val_f1: 0.7206
Epoch [52], train_loss: 0.0356, train_acc: 0.9884, val_loss: 1.8324, val_acc: 0.7375, val_precision: 0.7444, val_recall: 0.7375, val_f1: 0.7353
Epoch [53], train_loss: 0.0246, train_acc: 0.9917, val_loss: 1.8223, val_acc: 0.7400, val_precision: 0.7547, val_recall: 0.7400, val_f1: 0.7401
Epoch [54], train_loss: 0.0210, train_acc: 0.9931, val_loss: 1.9165, val_acc: 0.7366, val_precision: 0.7473, val_recall: 0.7366, val_f1: 0.7365
Epoch [55], train_loss: 0.0205, train_acc: 0.9933, val_loss: 2.0106, val_acc: 0.7435, val_precision: 0.7501, val_recall: 0.7435, val_f1: 0.7406
Epoch [56], train_loss: 0.0175, train_acc: 0.9942, val_loss: 1.9343, val_acc: 0.7440, val_precision: 0.7542, val_recall: 0.7440, val_f1: 0.7447
Epoch [57], train_loss: 0.0183, train_acc: 0.9938, val_loss: 2.2668, val_acc: 0.7236, val_precision: 0.7450, val_recall: 0.7236, val_f1: 0.7221
Epoch [58], train_loss: 0.0226, train_acc: 0.9923, val_loss: 1.9886, val_acc: 0.7491, val_precision: 0.7544, val_recall: 0.7491, val_f1: 0.7478
Epoch [59], train_loss: 0.0208, train_acc: 0.9930, val_loss: 1.8643, val_acc: 0.7385, val_precision: 0.7488, val_recall: 0.7385, val_f1: 0.7380
Epoch [60], train_loss: 0.0168, train_acc: 0.9946, val_loss: 1.8773, val_acc: 0.7460, val_precision: 0.7554, val_recall: 0.7460, val_f1: 0.7460
Epoch [61], train_loss: 0.0201, train_acc: 0.9933, val_loss: 1.9763, val_acc: 0.7417, val_precision: 0.7508, val_recall: 0.7417, val_f1: 0.7393
Epoch [62], train_loss: 0.0210, train_acc: 0.9931, val_loss: 1.8867, val_acc: 0.7424, val_precision: 0.7567, val_recall: 0.7424, val_f1: 0.7426
Epoch [63], train_loss: 0.0182, train_acc: 0.9935, val_loss: 1.8461, val_acc: 0.7497, val_precision: 0.7571, val_recall: 0.7497, val_f1: 0.7490
Epoch [64], train_loss: 0.0151, train_acc: 0.9951, val_loss: 2.0260, val_acc: 0.7297, val_precision: 0.7508, val_recall: 0.7297, val_f1: 0.7314
Epoch [65], train_loss: 0.0179, train_acc: 0.9945, val_loss: 2.0866, val_acc: 0.7358, val_precision: 0.7472, val_recall: 0.7358, val_f1: 0.7346
Epoch [66], train_loss: 0.0182, train_acc: 0.9938, val_loss: 2.0645, val_acc: 0.7426, val_precision: 0.7529, val_recall: 0.7426, val_f1: 0.7418
Epoch [67], train_loss: 0.0078, train_acc: 0.9977, val_loss: 2.0171, val_acc: 0.7504, val_precision: 0.7643, val_recall: 0.7504, val_f1: 0.7520
Epoch [68], train_loss: 0.0167, train_acc: 0.9948, val_loss: 1.8949, val_acc: 0.7491, val_precision: 0.7600, val_recall: 0.7491, val_f1: 0.7477
Epoch [69], train_loss: 0.0091, train_acc: 0.9968, val_loss: 2.1548, val_acc: 0.7440, val_precision: 0.7493, val_recall: 0.7440, val_f1: 0.7407
Epoch [70], train_loss: 0.0233, train_acc: 0.9922, val_loss: 1.9428, val_acc: 0.7335, val_precision: 0.7444, val_recall: 0.7335, val_f1: 0.7334
Epoch [71], train_loss: 0.0183, train_acc: 0.9935, val_loss: 2.0051, val_acc: 0.7479, val_precision: 0.7578, val_recall: 0.7479, val_f1: 0.7474
Epoch [72], train_loss: 0.0139, train_acc: 0.9952, val_loss: 2.0836, val_acc: 0.7389, val_precision: 0.7519, val_recall: 0.7389, val_f1: 0.7363
Epoch [73], train_loss: 0.0106, train_acc: 0.9964, val_loss: 1.9833, val_acc: 0.7518, val_precision: 0.7579, val_recall: 0.7518, val_f1: 0.7497
Epoch [74], train_loss: 0.0057, train_acc: 0.9982, val_loss: 2.0578, val_acc: 0.7436, val_precision: 0.7549, val_recall: 0.7436, val_f1: 0.7434
Epoch [75], train_loss: 0.0228, train_acc: 0.9922, val_loss: 1.9040, val_acc: 0.7491, val_precision: 0.7588, val_recall: 0.7491, val_f1: 0.7488
Epoch [76], train_loss: 0.0132, train_acc: 0.9952, val_loss: 2.0084, val_acc: 0.7497, val_precision: 0.7579, val_recall: 0.7497, val_f1: 0.7492
Epoch [77], train_loss: 0.0150, train_acc: 0.9953, val_loss: 1.9932, val_acc: 0.7415, val_precision: 0.7536, val_recall: 0.7415, val_f1: 0.7415
Epoch [78], train_loss: 0.0115, train_acc: 0.9963, val_loss: 2.0487, val_acc: 0.7435, val_precision: 0.7616, val_recall: 0.7435, val_f1: 0.7452
Epoch [79], train_loss: 0.0148, train_acc: 0.9949, val_loss: 2.0629, val_acc: 0.7386, val_precision: 0.7479, val_recall: 0.7386, val_f1: 0.7374
Epoch [80], train_loss: 0.0145, train_acc: 0.9948, val_loss: 2.0669, val_acc: 0.7358, val_precision: 0.7465, val_recall: 0.7358, val_f1: 0.7349
Epoch [81], train_loss: 0.0063, train_acc: 0.9982, val_loss: 2.0870, val_acc: 0.7544, val_precision: 0.7629, val_recall: 0.7544, val_f1: 0.7538
Epoch [82], train_loss: 0.0211, train_acc: 0.9932, val_loss: 1.8470, val_acc: 0.7417, val_precision: 0.7518, val_recall: 0.7417, val_f1: 0.7412
Epoch [83], train_loss: 0.0127, train_acc: 0.9957, val_loss: 2.0615, val_acc: 0.7481, val_precision: 0.7579, val_recall: 0.7481, val_f1: 0.7482
Epoch [84], train_loss: 0.0086, train_acc: 0.9971, val_loss: 2.0477, val_acc: 0.7489, val_precision: 0.7549, val_recall: 0.7489, val_f1: 0.7463
Epoch [85], train_loss: 0.0160, train_acc: 0.9946, val_loss: 1.9979, val_acc: 0.7383, val_precision: 0.7548, val_recall: 0.7383, val_f1: 0.7402
Epoch [86], train_loss: 0.0107, train_acc: 0.9962, val_loss: 2.1383, val_acc: 0.7525, val_precision: 0.7592, val_recall: 0.7525, val_f1: 0.7510
Epoch [87], train_loss: 0.0068, train_acc: 0.9980, val_loss: 2.0634, val_acc: 0.7473, val_precision: 0.7554, val_recall: 0.7473, val_f1: 0.7469
Epoch [88], train_loss: 0.0112, train_acc: 0.9961, val_loss: 1.9991, val_acc: 0.7507, val_precision: 0.7571, val_recall: 0.7507, val_f1: 0.7493
Epoch [89], train_loss: 0.0098, train_acc: 0.9970, val_loss: 2.0960, val_acc: 0.7570, val_precision: 0.7603, val_recall: 0.7570, val_f1: 0.7549
Epoch [90], train_loss: 0.0115, train_acc: 0.9964, val_loss: 2.1607, val_acc: 0.7363, val_precision: 0.7484, val_recall: 0.7363, val_f1: 0.7370
Epoch [91], train_loss: 0.0098, train_acc: 0.9968, val_loss: 2.0959, val_acc: 0.7560, val_precision: 0.7638, val_recall: 0.7560, val_f1: 0.7554
Epoch [92], train_loss: 0.0153, train_acc: 0.9944, val_loss: 2.0239, val_acc: 0.7534, val_precision: 0.7589, val_recall: 0.7534, val_f1: 0.7512
Epoch [93], train_loss: 0.0070, train_acc: 0.9977, val_loss: 2.0471, val_acc: 0.7499, val_precision: 0.7590, val_recall: 0.7499, val_f1: 0.7483
Epoch [94], train_loss: 0.0102, train_acc: 0.9967, val_loss: 1.9906, val_acc: 0.7446, val_precision: 0.7585, val_recall: 0.7446, val_f1: 0.7446
Epoch [95], train_loss: 0.0147, train_acc: 0.9950, val_loss: 2.0578, val_acc: 0.7533, val_precision: 0.7658, val_recall: 0.7533, val_f1: 0.7542
Epoch [96], train_loss: 0.0074, train_acc: 0.9977, val_loss: 2.0359, val_acc: 0.7615, val_precision: 0.7707, val_recall: 0.7615, val_f1: 0.7615
Epoch [97], train_loss: 0.0086, train_acc: 0.9970, val_loss: 2.1176, val_acc: 0.7436, val_precision: 0.7554, val_recall: 0.7436, val_f1: 0.7432
Epoch [98], train_loss: 0.0131, train_acc: 0.9952, val_loss: 2.1758, val_acc: 0.7526, val_precision: 0.7618, val_recall: 0.7526, val_f1: 0.7526
Epoch [99], train_loss: 0.0048, train_acc: 0.9984, val_loss: 2.3301, val_acc: 0.7481, val_precision: 0.7576, val_recall: 0.7481, val_f1: 0.7466
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.8042, val_acc: 0.7296, val_precision: 0.7406, val_recall: 0.7296, val_f1: 0.7290
Summary result of test set => last model => val_loss: 2.4362, val_acc: 0.7409, val_precision: 0.7516, val_recall: 0.7409, val_f1: 0.7380
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7276
--
confusion matrix
[[801  15  41  10  27   1   5  13  59  28]
 [ 25 828   4   8   4   3   7   4  34  83]
 [ 63   7 604  58 132  35  38  41  15   7]
 [ 34   9  79 524 102 119  48  49  20  16]
 [ 11   5  62  36 756  11  30  76  12   1]
 [ 14   6  61 171  83 549  20  80  10   6]
 [ 13   2  32  59  84   9 772   8  17   4]
 [ 16   1  24  35  72  25   4 811   5   7]
 [ 77  26   7  16   5   2   8   4 842  13]
 [ 48  76   8  12  11   3   3  22  28 789]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.73      0.80      0.76      1000
  automobile       0.85      0.83      0.84      1000
        bird       0.66      0.60      0.63      1000
         cat       0.56      0.52      0.54      1000
        deer       0.59      0.76      0.66      1000
         dog       0.73      0.55      0.62      1000
        frog       0.83      0.77      0.80      1000
       horse       0.73      0.81      0.77      1000
        ship       0.81      0.84      0.82      1000
       truck       0.83      0.79      0.81      1000

    accuracy                           0.73     10000
   macro avg       0.73      0.73      0.73     10000
weighted avg       0.73      0.73      0.73     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7366
--
confusion matrix
[[402   5  17   7  11   3   3   7  18  15]
 [ 17 426   1   4   8   0   2   2  16  36]
 [ 33   2 339  24  51  16  31  16  13   7]
 [ 15   0  36 250  34  66  18  24  20   8]
 [ 13   3  24  20 353   3  13  35   6   1]
 [  8   3  29  88  36 284  13  41   9   3]
 [  3   1  20  27  49   2 387   6   9   3]
 [  8   4  21  16  36  13   2 394   2   4]
 [ 44   3   4   5   4   0   2   1 435   6]
 [ 11  31   1   7   6   2   7  10  13 413]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.73      0.82      0.77       488
  automobile       0.89      0.83      0.86       512
        bird       0.69      0.64      0.66       532
         cat       0.56      0.53      0.54       471
        deer       0.60      0.75      0.67       471
         dog       0.73      0.55      0.63       514
        frog       0.81      0.76      0.79       507
       horse       0.74      0.79      0.76       500
        ship       0.80      0.86      0.83       504
       truck       0.83      0.82      0.83       501

    accuracy                           0.74      5000
   macro avg       0.74      0.74      0.73      5000
weighted avg       0.74      0.74      0.74      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423913: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:24:39 2024
Job was executed on host(s) <hgn53>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:26:37 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:26:37 2024
Terminated at Wed Feb 28 08:34:54 2024
Results reported at Wed Feb 28 08:34:54 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_LR_0001_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_LR_0001_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1482.00 sec.
    Max Memory :                                 3363 MB
    Average Memory :                             3264.17 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6877.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   503 sec.
    Turnaround time :                            615 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_LR_0001_err_423913> for stderr output of this job.

