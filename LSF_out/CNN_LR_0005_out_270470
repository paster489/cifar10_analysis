loading ...
loaded conda.sh
sh shell detected
main => start
 
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Train/Validation random split => start
 
DataLoader => start
 
To_device => start
 
Train => start
 
Epoch [0], train_loss: 1.8820, train_acc: 0.2948, val_loss: 1.5777, val_acc: 0.4035
Epoch [1], train_loss: 1.3611, train_acc: 0.4965, val_loss: 1.2082, val_acc: 0.5560
Epoch [2], train_loss: 1.0833, train_acc: 0.6085, val_loss: 0.9848, val_acc: 0.6403
Epoch [3], train_loss: 0.8899, train_acc: 0.6841, val_loss: 0.8479, val_acc: 0.6885
Epoch [4], train_loss: 0.7539, train_acc: 0.7346, val_loss: 0.8467, val_acc: 0.6974
Epoch [5], train_loss: 0.6545, train_acc: 0.7700, val_loss: 0.7306, val_acc: 0.7384
Epoch [6], train_loss: 0.5493, train_acc: 0.8065, val_loss: 0.7346, val_acc: 0.7387
Epoch [7], train_loss: 0.4581, train_acc: 0.8393, val_loss: 0.7136, val_acc: 0.7553
Epoch [8], train_loss: 0.3736, train_acc: 0.8696, val_loss: 0.6977, val_acc: 0.7714
Epoch [9], train_loss: 0.2915, train_acc: 0.8984, val_loss: 0.7490, val_acc: 0.7724
Epoch [10], train_loss: 0.2236, train_acc: 0.9218, val_loss: 0.7698, val_acc: 0.7784
Epoch [11], train_loss: 0.1603, train_acc: 0.9432, val_loss: 0.9245, val_acc: 0.7688
Epoch [12], train_loss: 0.1309, train_acc: 0.9544, val_loss: 0.9468, val_acc: 0.7743
Epoch [13], train_loss: 0.0877, train_acc: 0.9705, val_loss: 1.1120, val_acc: 0.7737
Epoch [14], train_loss: 0.0832, train_acc: 0.9704, val_loss: 1.0646, val_acc: 0.7713
Epoch [15], train_loss: 0.0668, train_acc: 0.9768, val_loss: 1.2547, val_acc: 0.7567
Epoch [16], train_loss: 0.0571, train_acc: 0.9807, val_loss: 1.1787, val_acc: 0.7710
Epoch [17], train_loss: 0.0601, train_acc: 0.9797, val_loss: 1.2415, val_acc: 0.7753
Epoch [18], train_loss: 0.0514, train_acc: 0.9823, val_loss: 1.3399, val_acc: 0.7754
Epoch [19], train_loss: 0.0527, train_acc: 0.9817, val_loss: 1.2622, val_acc: 0.7775
Epoch [20], train_loss: 0.0466, train_acc: 0.9843, val_loss: 1.3504, val_acc: 0.7687
Epoch [21], train_loss: 0.0463, train_acc: 0.9841, val_loss: 1.4644, val_acc: 0.7575
Epoch [22], train_loss: 0.0478, train_acc: 0.9837, val_loss: 1.3390, val_acc: 0.7810
Epoch [23], train_loss: 0.0414, train_acc: 0.9861, val_loss: 1.4642, val_acc: 0.7669
Epoch [24], train_loss: 0.0362, train_acc: 0.9879, val_loss: 1.3900, val_acc: 0.7783
Epoch [25], train_loss: 0.0463, train_acc: 0.9846, val_loss: 1.3800, val_acc: 0.7762
Epoch [26], train_loss: 0.0365, train_acc: 0.9877, val_loss: 1.3334, val_acc: 0.7778
Epoch [27], train_loss: 0.0331, train_acc: 0.9886, val_loss: 1.4300, val_acc: 0.7702
Epoch [28], train_loss: 0.0427, train_acc: 0.9860, val_loss: 1.4330, val_acc: 0.7672
Epoch [29], train_loss: 0.0338, train_acc: 0.9890, val_loss: 1.4515, val_acc: 0.7804
Epoch [30], train_loss: 0.0276, train_acc: 0.9914, val_loss: 1.5698, val_acc: 0.7649
Epoch [31], train_loss: 0.0337, train_acc: 0.9895, val_loss: 1.3812, val_acc: 0.7705
Epoch [32], train_loss: 0.0342, train_acc: 0.9885, val_loss: 1.4443, val_acc: 0.7749
Epoch [33], train_loss: 0.0333, train_acc: 0.9890, val_loss: 1.5490, val_acc: 0.7598
Epoch [34], train_loss: 0.0339, train_acc: 0.9892, val_loss: 1.4017, val_acc: 0.7748
Epoch [35], train_loss: 0.0275, train_acc: 0.9911, val_loss: 1.5642, val_acc: 0.7755
Epoch [36], train_loss: 0.0311, train_acc: 0.9898, val_loss: 1.5039, val_acc: 0.7784
Epoch [37], train_loss: 0.0276, train_acc: 0.9908, val_loss: 1.5688, val_acc: 0.7826
Epoch [38], train_loss: 0.0305, train_acc: 0.9902, val_loss: 1.4233, val_acc: 0.7915
Epoch [39], train_loss: 0.0269, train_acc: 0.9912, val_loss: 1.4646, val_acc: 0.7756
Epoch [40], train_loss: 0.0285, train_acc: 0.9911, val_loss: 1.5110, val_acc: 0.7808
Epoch [41], train_loss: 0.0236, train_acc: 0.9921, val_loss: 1.4750, val_acc: 0.7821
Epoch [42], train_loss: 0.0272, train_acc: 0.9912, val_loss: 1.6249, val_acc: 0.7726
Epoch [43], train_loss: 0.0295, train_acc: 0.9902, val_loss: 1.7270, val_acc: 0.7697
Epoch [44], train_loss: 0.0217, train_acc: 0.9928, val_loss: 1.6312, val_acc: 0.7761
Epoch [45], train_loss: 0.0277, train_acc: 0.9914, val_loss: 1.4432, val_acc: 0.7735
Epoch [46], train_loss: 0.0225, train_acc: 0.9927, val_loss: 1.5322, val_acc: 0.7923
Epoch [47], train_loss: 0.0213, train_acc: 0.9932, val_loss: 1.5331, val_acc: 0.7837
Epoch [48], train_loss: 0.0260, train_acc: 0.9913, val_loss: 1.5798, val_acc: 0.7825
Epoch [49], train_loss: 0.0253, train_acc: 0.9919, val_loss: 1.5087, val_acc: 0.7842
Epoch [50], train_loss: 0.0228, train_acc: 0.9928, val_loss: 1.5691, val_acc: 0.7820
Epoch [51], train_loss: 0.0221, train_acc: 0.9929, val_loss: 1.6111, val_acc: 0.7828
Epoch [52], train_loss: 0.0193, train_acc: 0.9938, val_loss: 1.6065, val_acc: 0.7733
Epoch [53], train_loss: 0.0242, train_acc: 0.9925, val_loss: 1.5050, val_acc: 0.7804
Epoch [54], train_loss: 0.0241, train_acc: 0.9928, val_loss: 1.6011, val_acc: 0.7854
Epoch [55], train_loss: 0.0169, train_acc: 0.9951, val_loss: 1.7897, val_acc: 0.7736
Epoch [56], train_loss: 0.0251, train_acc: 0.9921, val_loss: 1.6788, val_acc: 0.7874
Epoch [57], train_loss: 0.0245, train_acc: 0.9925, val_loss: 1.5752, val_acc: 0.7822
Epoch [58], train_loss: 0.0189, train_acc: 0.9940, val_loss: 1.5073, val_acc: 0.7726
Epoch [59], train_loss: 0.0184, train_acc: 0.9942, val_loss: 1.6466, val_acc: 0.7781
Epoch [60], train_loss: 0.0201, train_acc: 0.9934, val_loss: 1.7333, val_acc: 0.7658
Epoch [61], train_loss: 0.0270, train_acc: 0.9917, val_loss: 1.6004, val_acc: 0.7867
Epoch [62], train_loss: 0.0168, train_acc: 0.9948, val_loss: 1.7157, val_acc: 0.7790
Epoch [63], train_loss: 0.0272, train_acc: 0.9915, val_loss: 1.5695, val_acc: 0.7780
Epoch [64], train_loss: 0.0117, train_acc: 0.9961, val_loss: 1.7438, val_acc: 0.7862
Epoch [65], train_loss: 0.0189, train_acc: 0.9945, val_loss: 1.7175, val_acc: 0.7858
Epoch [66], train_loss: 0.0228, train_acc: 0.9928, val_loss: 1.6823, val_acc: 0.7818
Epoch [67], train_loss: 0.0215, train_acc: 0.9931, val_loss: 1.8618, val_acc: 0.7724
Epoch [68], train_loss: 0.0209, train_acc: 0.9931, val_loss: 1.6655, val_acc: 0.7768
Epoch [69], train_loss: 0.0199, train_acc: 0.9939, val_loss: 1.6115, val_acc: 0.7899
Epoch [70], train_loss: 0.0105, train_acc: 0.9965, val_loss: 1.8750, val_acc: 0.7738
Epoch [71], train_loss: 0.0197, train_acc: 0.9941, val_loss: 1.8116, val_acc: 0.7733
Epoch [72], train_loss: 0.0146, train_acc: 0.9959, val_loss: 1.6468, val_acc: 0.7887
Epoch [73], train_loss: 0.0217, train_acc: 0.9928, val_loss: 1.7226, val_acc: 0.7849
Epoch [74], train_loss: 0.0145, train_acc: 0.9953, val_loss: 1.8245, val_acc: 0.7705
Epoch [75], train_loss: 0.0200, train_acc: 0.9942, val_loss: 1.7125, val_acc: 0.7851
Epoch [76], train_loss: 0.0201, train_acc: 0.9934, val_loss: 1.8007, val_acc: 0.7827
Epoch [77], train_loss: 0.0141, train_acc: 0.9956, val_loss: 1.8410, val_acc: 0.7874
Epoch [78], train_loss: 0.0246, train_acc: 0.9921, val_loss: 1.6752, val_acc: 0.7854
Epoch [79], train_loss: 0.0140, train_acc: 0.9956, val_loss: 1.8646, val_acc: 0.7842
Epoch [80], train_loss: 0.0158, train_acc: 0.9950, val_loss: 1.8125, val_acc: 0.7740
Epoch [81], train_loss: 0.0160, train_acc: 0.9947, val_loss: 1.7553, val_acc: 0.7841
Epoch [82], train_loss: 0.0122, train_acc: 0.9963, val_loss: 1.7956, val_acc: 0.7792
Epoch [83], train_loss: 0.0235, train_acc: 0.9930, val_loss: 1.6871, val_acc: 0.7812
Epoch [84], train_loss: 0.0108, train_acc: 0.9965, val_loss: 1.8303, val_acc: 0.7884
Epoch [85], train_loss: 0.0218, train_acc: 0.9935, val_loss: 1.9423, val_acc: 0.7772
Epoch [86], train_loss: 0.0179, train_acc: 0.9941, val_loss: 1.7881, val_acc: 0.7779
Epoch [87], train_loss: 0.0125, train_acc: 0.9959, val_loss: 1.8313, val_acc: 0.7832
Epoch [88], train_loss: 0.0182, train_acc: 0.9944, val_loss: 1.8470, val_acc: 0.7875
Epoch [89], train_loss: 0.0142, train_acc: 0.9955, val_loss: 1.9695, val_acc: 0.7851
Epoch [90], train_loss: 0.0129, train_acc: 0.9960, val_loss: 1.8739, val_acc: 0.7880
Epoch [91], train_loss: 0.0195, train_acc: 0.9945, val_loss: 1.7127, val_acc: 0.7828
Epoch [92], train_loss: 0.0146, train_acc: 0.9959, val_loss: 1.8098, val_acc: 0.7841
Epoch [93], train_loss: 0.0120, train_acc: 0.9964, val_loss: 1.8706, val_acc: 0.7819
Epoch [94], train_loss: 0.0190, train_acc: 0.9945, val_loss: 1.7018, val_acc: 0.7769
Epoch [95], train_loss: 0.0122, train_acc: 0.9964, val_loss: 1.8144, val_acc: 0.7903
Epoch [96], train_loss: 0.0092, train_acc: 0.9972, val_loss: 1.9687, val_acc: 0.7841
Epoch [97], train_loss: 0.0217, train_acc: 0.9935, val_loss: 1.7816, val_acc: 0.7764
Epoch [98], train_loss: 0.0147, train_acc: 0.9956, val_loss: 1.7870, val_acc: 0.7808
Epoch [99], train_loss: 0.0099, train_acc: 0.9971, val_loss: 1.8610, val_acc: 0.7819
 
Visualize trining => save images
 
Load the model => start
 
Check best/last models => start
 
Summary result of test set => best model  {'val_loss': 0.7310553789138794, 'val_acc': 0.7627929449081421}
Summary result of test set => last model {'val_loss': 1.977461814880371, 'val_acc': 0.7762695550918579}
Test set evaluation => save results for postprocessing
 
** accuracy: 0.766
--
confusion matrix
[[815  18  47  16  14   4   8  11  45  22]
 [ 21 891   2   2   1   1   5   1  20  56]
 [ 54   8 675  54  77  44  48  20   8  12]
 [ 27  22  57 580  58 132  63  30  13  18]
 [ 13   6 108  44 691  28  35  65   5   5]
 [ 16   2  39 176  35 663  21  30   9   9]
 [  7   9  33  42  34  17 832   5  12   9]
 [ 15   4  31  40  41  45   7 803   2  12]
 [ 52  32  16  12   4   2   4   1 857  20]
 [ 35  69   3  10   3   3   3   8  12 854]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.77      0.81      0.79      1000
  automobile       0.84      0.89      0.86      1000
        bird       0.67      0.68      0.67      1000
         cat       0.59      0.58      0.59      1000
        deer       0.72      0.69      0.71      1000
         dog       0.71      0.66      0.68      1000
        frog       0.81      0.83      0.82      1000
       horse       0.82      0.80      0.81      1000
        ship       0.87      0.86      0.86      1000
       truck       0.84      0.85      0.85      1000

    accuracy                           0.77     10000
   macro avg       0.76      0.77      0.77     10000
weighted avg       0.76      0.77      0.77     10000

END OF CODE
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 270470: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <access4> by user <ingap> in cluster <wexac> at Tue Feb 27 11:10:20 2024
Job was executed on host(s) <hgn41>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 11:10:30 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 11:10:30 2024
Terminated at Tue Feb 27 11:18:48 2024
Results reported at Tue Feb 27 11:18:48 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_LR_0005_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_LR_0005_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1516.00 sec.
    Max Memory :                                 3448 MB
    Average Memory :                             3253.89 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6792.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   499 sec.
    Turnaround time :                            508 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_LR_0005_err_270470> for stderr output of this job.

