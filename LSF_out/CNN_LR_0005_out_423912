loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.8820, train_acc: 0.2948, val_loss: 1.5777, val_acc: 0.4035, val_precision: 0.4384, val_recall: 0.4035, val_f1: 0.3793
Epoch [1], train_loss: 1.3611, train_acc: 0.4965, val_loss: 1.2082, val_acc: 0.5560, val_precision: 0.5664, val_recall: 0.5560, val_f1: 0.5478
Epoch [2], train_loss: 1.0833, train_acc: 0.6085, val_loss: 0.9848, val_acc: 0.6403, val_precision: 0.6495, val_recall: 0.6403, val_f1: 0.6350
Epoch [3], train_loss: 0.8899, train_acc: 0.6841, val_loss: 0.8479, val_acc: 0.6885, val_precision: 0.7021, val_recall: 0.6885, val_f1: 0.6894
Epoch [4], train_loss: 0.7539, train_acc: 0.7346, val_loss: 0.8467, val_acc: 0.6974, val_precision: 0.7409, val_recall: 0.6974, val_f1: 0.7071
Epoch [5], train_loss: 0.6545, train_acc: 0.7700, val_loss: 0.7306, val_acc: 0.7384, val_precision: 0.7482, val_recall: 0.7384, val_f1: 0.7353
Epoch [6], train_loss: 0.5493, train_acc: 0.8065, val_loss: 0.7346, val_acc: 0.7387, val_precision: 0.7531, val_recall: 0.7387, val_f1: 0.7371
Epoch [7], train_loss: 0.4581, train_acc: 0.8393, val_loss: 0.7136, val_acc: 0.7553, val_precision: 0.7736, val_recall: 0.7553, val_f1: 0.7577
Epoch [8], train_loss: 0.3736, train_acc: 0.8696, val_loss: 0.6977, val_acc: 0.7714, val_precision: 0.7760, val_recall: 0.7714, val_f1: 0.7699
Epoch [9], train_loss: 0.2915, train_acc: 0.8984, val_loss: 0.7490, val_acc: 0.7724, val_precision: 0.7847, val_recall: 0.7724, val_f1: 0.7709
Epoch [10], train_loss: 0.2236, train_acc: 0.9218, val_loss: 0.7698, val_acc: 0.7784, val_precision: 0.7853, val_recall: 0.7784, val_f1: 0.7759
Epoch [11], train_loss: 0.1603, train_acc: 0.9432, val_loss: 0.9245, val_acc: 0.7688, val_precision: 0.7735, val_recall: 0.7688, val_f1: 0.7646
Epoch [12], train_loss: 0.1309, train_acc: 0.9544, val_loss: 0.9468, val_acc: 0.7743, val_precision: 0.7786, val_recall: 0.7743, val_f1: 0.7717
Epoch [13], train_loss: 0.0877, train_acc: 0.9705, val_loss: 1.1120, val_acc: 0.7737, val_precision: 0.7859, val_recall: 0.7737, val_f1: 0.7715
Epoch [14], train_loss: 0.0832, train_acc: 0.9704, val_loss: 1.0646, val_acc: 0.7713, val_precision: 0.7793, val_recall: 0.7713, val_f1: 0.7711
Epoch [15], train_loss: 0.0668, train_acc: 0.9768, val_loss: 1.2547, val_acc: 0.7567, val_precision: 0.7758, val_recall: 0.7567, val_f1: 0.7587
Epoch [16], train_loss: 0.0571, train_acc: 0.9807, val_loss: 1.1787, val_acc: 0.7710, val_precision: 0.7800, val_recall: 0.7710, val_f1: 0.7689
Epoch [17], train_loss: 0.0601, train_acc: 0.9797, val_loss: 1.2415, val_acc: 0.7753, val_precision: 0.7864, val_recall: 0.7753, val_f1: 0.7746
Epoch [18], train_loss: 0.0514, train_acc: 0.9823, val_loss: 1.3399, val_acc: 0.7754, val_precision: 0.7827, val_recall: 0.7754, val_f1: 0.7738
Epoch [19], train_loss: 0.0527, train_acc: 0.9817, val_loss: 1.2622, val_acc: 0.7775, val_precision: 0.7876, val_recall: 0.7775, val_f1: 0.7781
Epoch [20], train_loss: 0.0466, train_acc: 0.9843, val_loss: 1.3504, val_acc: 0.7687, val_precision: 0.7814, val_recall: 0.7687, val_f1: 0.7699
Epoch [21], train_loss: 0.0463, train_acc: 0.9841, val_loss: 1.4644, val_acc: 0.7575, val_precision: 0.7764, val_recall: 0.7575, val_f1: 0.7578
Epoch [22], train_loss: 0.0478, train_acc: 0.9837, val_loss: 1.3390, val_acc: 0.7810, val_precision: 0.7890, val_recall: 0.7810, val_f1: 0.7813
Epoch [23], train_loss: 0.0414, train_acc: 0.9861, val_loss: 1.4642, val_acc: 0.7669, val_precision: 0.7882, val_recall: 0.7669, val_f1: 0.7707
Epoch [24], train_loss: 0.0362, train_acc: 0.9879, val_loss: 1.3900, val_acc: 0.7783, val_precision: 0.7929, val_recall: 0.7783, val_f1: 0.7798
Epoch [25], train_loss: 0.0463, train_acc: 0.9846, val_loss: 1.3800, val_acc: 0.7762, val_precision: 0.7856, val_recall: 0.7762, val_f1: 0.7750
Epoch [26], train_loss: 0.0365, train_acc: 0.9877, val_loss: 1.3334, val_acc: 0.7778, val_precision: 0.7918, val_recall: 0.7778, val_f1: 0.7804
Epoch [27], train_loss: 0.0331, train_acc: 0.9886, val_loss: 1.4300, val_acc: 0.7702, val_precision: 0.7848, val_recall: 0.7702, val_f1: 0.7725
Epoch [28], train_loss: 0.0427, train_acc: 0.9860, val_loss: 1.4330, val_acc: 0.7672, val_precision: 0.7778, val_recall: 0.7672, val_f1: 0.7668
Epoch [29], train_loss: 0.0338, train_acc: 0.9890, val_loss: 1.4515, val_acc: 0.7804, val_precision: 0.7933, val_recall: 0.7804, val_f1: 0.7797
Epoch [30], train_loss: 0.0276, train_acc: 0.9914, val_loss: 1.5698, val_acc: 0.7649, val_precision: 0.7805, val_recall: 0.7649, val_f1: 0.7648
Epoch [31], train_loss: 0.0337, train_acc: 0.9895, val_loss: 1.3812, val_acc: 0.7705, val_precision: 0.7752, val_recall: 0.7705, val_f1: 0.7694
Epoch [32], train_loss: 0.0342, train_acc: 0.9885, val_loss: 1.4443, val_acc: 0.7749, val_precision: 0.7797, val_recall: 0.7749, val_f1: 0.7730
Epoch [33], train_loss: 0.0333, train_acc: 0.9890, val_loss: 1.5490, val_acc: 0.7598, val_precision: 0.7746, val_recall: 0.7598, val_f1: 0.7610
Epoch [34], train_loss: 0.0339, train_acc: 0.9892, val_loss: 1.4017, val_acc: 0.7748, val_precision: 0.7821, val_recall: 0.7748, val_f1: 0.7735
Epoch [35], train_loss: 0.0275, train_acc: 0.9911, val_loss: 1.5642, val_acc: 0.7755, val_precision: 0.7894, val_recall: 0.7755, val_f1: 0.7765
Epoch [36], train_loss: 0.0311, train_acc: 0.9898, val_loss: 1.5039, val_acc: 0.7784, val_precision: 0.7868, val_recall: 0.7784, val_f1: 0.7774
Epoch [37], train_loss: 0.0276, train_acc: 0.9908, val_loss: 1.5688, val_acc: 0.7826, val_precision: 0.7862, val_recall: 0.7826, val_f1: 0.7798
Epoch [38], train_loss: 0.0305, train_acc: 0.9902, val_loss: 1.4233, val_acc: 0.7915, val_precision: 0.7991, val_recall: 0.7915, val_f1: 0.7907
Epoch [39], train_loss: 0.0269, train_acc: 0.9912, val_loss: 1.4646, val_acc: 0.7756, val_precision: 0.7841, val_recall: 0.7756, val_f1: 0.7754
Epoch [40], train_loss: 0.0285, train_acc: 0.9911, val_loss: 1.5110, val_acc: 0.7808, val_precision: 0.7863, val_recall: 0.7808, val_f1: 0.7789
Epoch [41], train_loss: 0.0236, train_acc: 0.9921, val_loss: 1.4750, val_acc: 0.7821, val_precision: 0.7921, val_recall: 0.7821, val_f1: 0.7826
Epoch [42], train_loss: 0.0272, train_acc: 0.9912, val_loss: 1.6249, val_acc: 0.7726, val_precision: 0.7888, val_recall: 0.7726, val_f1: 0.7744
Epoch [43], train_loss: 0.0295, train_acc: 0.9902, val_loss: 1.7270, val_acc: 0.7697, val_precision: 0.7845, val_recall: 0.7697, val_f1: 0.7712
Epoch [44], train_loss: 0.0217, train_acc: 0.9928, val_loss: 1.6312, val_acc: 0.7761, val_precision: 0.7815, val_recall: 0.7761, val_f1: 0.7743
Epoch [45], train_loss: 0.0277, train_acc: 0.9914, val_loss: 1.4432, val_acc: 0.7735, val_precision: 0.7836, val_recall: 0.7735, val_f1: 0.7732
Epoch [46], train_loss: 0.0225, train_acc: 0.9927, val_loss: 1.5322, val_acc: 0.7923, val_precision: 0.8002, val_recall: 0.7923, val_f1: 0.7920
Epoch [47], train_loss: 0.0213, train_acc: 0.9932, val_loss: 1.5331, val_acc: 0.7837, val_precision: 0.7921, val_recall: 0.7837, val_f1: 0.7840
Epoch [48], train_loss: 0.0260, train_acc: 0.9913, val_loss: 1.5798, val_acc: 0.7825, val_precision: 0.7901, val_recall: 0.7825, val_f1: 0.7826
Epoch [49], train_loss: 0.0253, train_acc: 0.9919, val_loss: 1.5087, val_acc: 0.7842, val_precision: 0.7939, val_recall: 0.7842, val_f1: 0.7847
Epoch [50], train_loss: 0.0228, train_acc: 0.9928, val_loss: 1.5691, val_acc: 0.7820, val_precision: 0.7900, val_recall: 0.7820, val_f1: 0.7823
Epoch [51], train_loss: 0.0221, train_acc: 0.9929, val_loss: 1.6111, val_acc: 0.7828, val_precision: 0.7964, val_recall: 0.7828, val_f1: 0.7835
Epoch [52], train_loss: 0.0193, train_acc: 0.9938, val_loss: 1.6065, val_acc: 0.7733, val_precision: 0.7826, val_recall: 0.7733, val_f1: 0.7731
Epoch [53], train_loss: 0.0242, train_acc: 0.9925, val_loss: 1.5050, val_acc: 0.7804, val_precision: 0.7880, val_recall: 0.7804, val_f1: 0.7798
Epoch [54], train_loss: 0.0241, train_acc: 0.9928, val_loss: 1.6011, val_acc: 0.7854, val_precision: 0.8017, val_recall: 0.7854, val_f1: 0.7883
Epoch [55], train_loss: 0.0169, train_acc: 0.9951, val_loss: 1.7897, val_acc: 0.7736, val_precision: 0.7803, val_recall: 0.7736, val_f1: 0.7726
Epoch [56], train_loss: 0.0251, train_acc: 0.9921, val_loss: 1.6788, val_acc: 0.7874, val_precision: 0.7976, val_recall: 0.7874, val_f1: 0.7878
Epoch [57], train_loss: 0.0245, train_acc: 0.9925, val_loss: 1.5752, val_acc: 0.7822, val_precision: 0.7870, val_recall: 0.7822, val_f1: 0.7815
Epoch [58], train_loss: 0.0189, train_acc: 0.9940, val_loss: 1.5073, val_acc: 0.7726, val_precision: 0.7810, val_recall: 0.7726, val_f1: 0.7728
Epoch [59], train_loss: 0.0184, train_acc: 0.9942, val_loss: 1.6466, val_acc: 0.7781, val_precision: 0.7806, val_recall: 0.7781, val_f1: 0.7751
Epoch [60], train_loss: 0.0201, train_acc: 0.9934, val_loss: 1.7333, val_acc: 0.7658, val_precision: 0.7816, val_recall: 0.7658, val_f1: 0.7663
Epoch [61], train_loss: 0.0270, train_acc: 0.9917, val_loss: 1.6004, val_acc: 0.7867, val_precision: 0.7918, val_recall: 0.7867, val_f1: 0.7853
Epoch [62], train_loss: 0.0168, train_acc: 0.9948, val_loss: 1.7157, val_acc: 0.7790, val_precision: 0.7835, val_recall: 0.7790, val_f1: 0.7760
Epoch [63], train_loss: 0.0272, train_acc: 0.9915, val_loss: 1.5695, val_acc: 0.7780, val_precision: 0.7862, val_recall: 0.7780, val_f1: 0.7780
Epoch [64], train_loss: 0.0117, train_acc: 0.9961, val_loss: 1.7438, val_acc: 0.7862, val_precision: 0.7968, val_recall: 0.7862, val_f1: 0.7871
Epoch [65], train_loss: 0.0189, train_acc: 0.9945, val_loss: 1.7175, val_acc: 0.7858, val_precision: 0.7928, val_recall: 0.7858, val_f1: 0.7854
Epoch [66], train_loss: 0.0228, train_acc: 0.9928, val_loss: 1.6823, val_acc: 0.7818, val_precision: 0.7892, val_recall: 0.7818, val_f1: 0.7817
Epoch [67], train_loss: 0.0215, train_acc: 0.9931, val_loss: 1.8618, val_acc: 0.7724, val_precision: 0.7909, val_recall: 0.7724, val_f1: 0.7765
Epoch [68], train_loss: 0.0209, train_acc: 0.9931, val_loss: 1.6655, val_acc: 0.7768, val_precision: 0.7846, val_recall: 0.7768, val_f1: 0.7770
Epoch [69], train_loss: 0.0199, train_acc: 0.9939, val_loss: 1.6115, val_acc: 0.7899, val_precision: 0.7983, val_recall: 0.7899, val_f1: 0.7895
Epoch [70], train_loss: 0.0105, train_acc: 0.9965, val_loss: 1.8750, val_acc: 0.7738, val_precision: 0.7852, val_recall: 0.7738, val_f1: 0.7735
Epoch [71], train_loss: 0.0197, train_acc: 0.9941, val_loss: 1.8116, val_acc: 0.7733, val_precision: 0.7892, val_recall: 0.7733, val_f1: 0.7719
Epoch [72], train_loss: 0.0146, train_acc: 0.9959, val_loss: 1.6468, val_acc: 0.7887, val_precision: 0.7957, val_recall: 0.7887, val_f1: 0.7874
Epoch [73], train_loss: 0.0217, train_acc: 0.9928, val_loss: 1.7226, val_acc: 0.7849, val_precision: 0.7907, val_recall: 0.7849, val_f1: 0.7821
Epoch [74], train_loss: 0.0145, train_acc: 0.9953, val_loss: 1.8245, val_acc: 0.7705, val_precision: 0.7887, val_recall: 0.7705, val_f1: 0.7736
Epoch [75], train_loss: 0.0200, train_acc: 0.9942, val_loss: 1.7125, val_acc: 0.7851, val_precision: 0.7920, val_recall: 0.7851, val_f1: 0.7846
Epoch [76], train_loss: 0.0201, train_acc: 0.9934, val_loss: 1.8007, val_acc: 0.7827, val_precision: 0.7897, val_recall: 0.7827, val_f1: 0.7819
Epoch [77], train_loss: 0.0141, train_acc: 0.9956, val_loss: 1.8410, val_acc: 0.7874, val_precision: 0.7956, val_recall: 0.7874, val_f1: 0.7866
Epoch [78], train_loss: 0.0246, train_acc: 0.9921, val_loss: 1.6752, val_acc: 0.7854, val_precision: 0.7917, val_recall: 0.7854, val_f1: 0.7844
Epoch [79], train_loss: 0.0140, train_acc: 0.9956, val_loss: 1.8646, val_acc: 0.7842, val_precision: 0.7905, val_recall: 0.7842, val_f1: 0.7822
Epoch [80], train_loss: 0.0158, train_acc: 0.9950, val_loss: 1.8125, val_acc: 0.7740, val_precision: 0.7827, val_recall: 0.7740, val_f1: 0.7730
Epoch [81], train_loss: 0.0160, train_acc: 0.9947, val_loss: 1.7553, val_acc: 0.7841, val_precision: 0.7935, val_recall: 0.7841, val_f1: 0.7851
Epoch [82], train_loss: 0.0122, train_acc: 0.9963, val_loss: 1.7956, val_acc: 0.7792, val_precision: 0.7919, val_recall: 0.7792, val_f1: 0.7811
Epoch [83], train_loss: 0.0235, train_acc: 0.9930, val_loss: 1.6871, val_acc: 0.7812, val_precision: 0.7930, val_recall: 0.7812, val_f1: 0.7822
Epoch [84], train_loss: 0.0108, train_acc: 0.9965, val_loss: 1.8303, val_acc: 0.7884, val_precision: 0.7955, val_recall: 0.7884, val_f1: 0.7879
Epoch [85], train_loss: 0.0218, train_acc: 0.9935, val_loss: 1.9423, val_acc: 0.7772, val_precision: 0.7890, val_recall: 0.7772, val_f1: 0.7782
Epoch [86], train_loss: 0.0179, train_acc: 0.9941, val_loss: 1.7881, val_acc: 0.7779, val_precision: 0.7891, val_recall: 0.7779, val_f1: 0.7782
Epoch [87], train_loss: 0.0125, train_acc: 0.9959, val_loss: 1.8313, val_acc: 0.7832, val_precision: 0.7901, val_recall: 0.7832, val_f1: 0.7828
Epoch [88], train_loss: 0.0182, train_acc: 0.9944, val_loss: 1.8470, val_acc: 0.7875, val_precision: 0.7931, val_recall: 0.7875, val_f1: 0.7859
Epoch [89], train_loss: 0.0142, train_acc: 0.9955, val_loss: 1.9695, val_acc: 0.7851, val_precision: 0.7926, val_recall: 0.7851, val_f1: 0.7853
Epoch [90], train_loss: 0.0129, train_acc: 0.9960, val_loss: 1.8739, val_acc: 0.7880, val_precision: 0.7948, val_recall: 0.7880, val_f1: 0.7874
Epoch [91], train_loss: 0.0195, train_acc: 0.9945, val_loss: 1.7127, val_acc: 0.7828, val_precision: 0.7954, val_recall: 0.7828, val_f1: 0.7836
Epoch [92], train_loss: 0.0146, train_acc: 0.9959, val_loss: 1.8098, val_acc: 0.7841, val_precision: 0.7931, val_recall: 0.7841, val_f1: 0.7834
Epoch [93], train_loss: 0.0120, train_acc: 0.9964, val_loss: 1.8706, val_acc: 0.7819, val_precision: 0.7876, val_recall: 0.7819, val_f1: 0.7815
Epoch [94], train_loss: 0.0190, train_acc: 0.9945, val_loss: 1.7018, val_acc: 0.7769, val_precision: 0.7864, val_recall: 0.7769, val_f1: 0.7779
Epoch [95], train_loss: 0.0122, train_acc: 0.9964, val_loss: 1.8144, val_acc: 0.7903, val_precision: 0.7987, val_recall: 0.7903, val_f1: 0.7905
Epoch [96], train_loss: 0.0092, train_acc: 0.9972, val_loss: 1.9687, val_acc: 0.7841, val_precision: 0.7915, val_recall: 0.7841, val_f1: 0.7832
Epoch [97], train_loss: 0.0217, train_acc: 0.9935, val_loss: 1.7816, val_acc: 0.7764, val_precision: 0.7881, val_recall: 0.7764, val_f1: 0.7782
Epoch [98], train_loss: 0.0147, train_acc: 0.9956, val_loss: 1.7870, val_acc: 0.7808, val_precision: 0.7933, val_recall: 0.7808, val_f1: 0.7819
Epoch [99], train_loss: 0.0099, train_acc: 0.9971, val_loss: 1.8610, val_acc: 0.7819, val_precision: 0.7889, val_recall: 0.7819, val_f1: 0.7813
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.7311, val_acc: 0.7628, val_precision: 0.7696, val_recall: 0.7628, val_f1: 0.7616
Summary result of test set => last model => val_loss: 1.9775, val_acc: 0.7763, val_precision: 0.7838, val_recall: 0.7763, val_f1: 0.7761
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7661
--
confusion matrix
[[815  18  47  16  14   4   8  11  45  22]
 [ 21 891   2   2   1   1   5   1  20  56]
 [ 54   8 675  54  77  44  48  20   8  12]
 [ 27  22  57 580  58 132  63  30  13  18]
 [ 13   6 108  44 691  28  35  65   5   5]
 [ 16   2  39 176  35 663  21  30   9   9]
 [  7   9  33  42  34  17 832   5  12   9]
 [ 15   4  31  40  41  45   7 803   2  12]
 [ 52  32  16  12   4   2   4   1 857  20]
 [ 35  69   3  10   3   3   3   8  12 854]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.77      0.81      0.79      1000
  automobile       0.84      0.89      0.86      1000
        bird       0.67      0.68      0.67      1000
         cat       0.59      0.58      0.59      1000
        deer       0.72      0.69      0.71      1000
         dog       0.71      0.66      0.68      1000
        frog       0.81      0.83      0.82      1000
       horse       0.82      0.80      0.81      1000
        ship       0.87      0.86      0.86      1000
       truck       0.84      0.85      0.85      1000

    accuracy                           0.77     10000
   macro avg       0.76      0.77      0.77     10000
weighted avg       0.76      0.77      0.77     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7702
--
confusion matrix
[[409  10  25   8   5   3   0   2  14  12]
 [  5 471   0   0   2   1   1   0   7  25]
 [ 22   2 371  28  33  23  30  11   7   5]
 [ 12   5  27 273  18  64  32  17   7  16]
 [  8   1  55  22 317  21  13  29   4   1]
 [  4   1  21 106  19 313  19  18   8   5]
 [  1   4  20  28  15   5 417   3   5   9]
 [  8   2  13  12  31  18   1 404   2   9]
 [ 37  11   5   2   2   0   2   1 437   7]
 [ 13  34   0   6   2   1   3   1   2 439]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.79      0.84      0.81       488
  automobile       0.87      0.92      0.89       512
        bird       0.69      0.70      0.69       532
         cat       0.56      0.58      0.57       471
        deer       0.71      0.67      0.69       471
         dog       0.70      0.61      0.65       514
        frog       0.81      0.82      0.81       507
       horse       0.83      0.81      0.82       500
        ship       0.89      0.87      0.88       504
       truck       0.83      0.88      0.85       501

    accuracy                           0.77      5000
   macro avg       0.77      0.77      0.77      5000
weighted avg       0.77      0.77      0.77      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423912: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:24:24 2024
Job was executed on host(s) <hgn53>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:26:09 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:26:09 2024
Terminated at Wed Feb 28 08:35:12 2024
Results reported at Wed Feb 28 08:35:12 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_LR_0005_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_LR_0005_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1557.00 sec.
    Max Memory :                                 3363 MB
    Average Memory :                             3227.86 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6877.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   544 sec.
    Turnaround time :                            648 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_LR_0005_err_423912> for stderr output of this job.

