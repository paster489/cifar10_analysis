loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 2.3450, train_acc: 0.1009, val_loss: 2.3035, val_acc: 0.0998, val_precision: 0.0998, val_recall: 0.0998, val_f1: 0.0998
Epoch [1], train_loss: 2.3029, train_acc: 0.0991, val_loss: 2.3031, val_acc: 0.1029, val_precision: 0.1029, val_recall: 0.1029, val_f1: 0.1029
Epoch [2], train_loss: 2.3030, train_acc: 0.0995, val_loss: 2.3036, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [3], train_loss: 2.3030, train_acc: 0.0987, val_loss: 2.3030, val_acc: 0.0998, val_precision: 0.0998, val_recall: 0.0998, val_f1: 0.0998
Epoch [4], train_loss: 2.3030, train_acc: 0.0999, val_loss: 2.3029, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [5], train_loss: 2.3030, train_acc: 0.0990, val_loss: 2.3026, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [6], train_loss: 2.3030, train_acc: 0.0989, val_loss: 2.3029, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [7], train_loss: 2.3030, train_acc: 0.0997, val_loss: 2.3029, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [8], train_loss: 2.3031, train_acc: 0.0985, val_loss: 2.3029, val_acc: 0.1029, val_precision: 0.1029, val_recall: 0.1029, val_f1: 0.1029
Epoch [9], train_loss: 2.3029, train_acc: 0.0995, val_loss: 2.3036, val_acc: 0.0998, val_precision: 0.0998, val_recall: 0.0998, val_f1: 0.0998
Epoch [10], train_loss: 2.3030, train_acc: 0.0985, val_loss: 2.3032, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [11], train_loss: 2.3030, train_acc: 0.0974, val_loss: 2.3034, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [12], train_loss: 2.3030, train_acc: 0.1001, val_loss: 2.3032, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [13], train_loss: 2.3030, train_acc: 0.0983, val_loss: 2.3029, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [14], train_loss: 2.3030, train_acc: 0.1008, val_loss: 2.3026, val_acc: 0.0998, val_precision: 0.0998, val_recall: 0.0998, val_f1: 0.0998
Epoch [15], train_loss: 2.3029, train_acc: 0.1021, val_loss: 2.3034, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [16], train_loss: 2.3029, train_acc: 0.0999, val_loss: 2.3032, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [17], train_loss: 2.3031, train_acc: 0.0989, val_loss: 2.3030, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [18], train_loss: 2.3030, train_acc: 0.0996, val_loss: 2.3029, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [19], train_loss: 2.3030, train_acc: 0.0983, val_loss: 2.3031, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [20], train_loss: 2.3030, train_acc: 0.0972, val_loss: 2.3032, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [21], train_loss: 2.3030, train_acc: 0.0977, val_loss: 2.3033, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [22], train_loss: 2.3029, train_acc: 0.1002, val_loss: 2.3025, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [23], train_loss: 2.3030, train_acc: 0.0973, val_loss: 2.3030, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [24], train_loss: 2.3030, train_acc: 0.0982, val_loss: 2.3025, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [25], train_loss: 2.3029, train_acc: 0.0992, val_loss: 2.3023, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [26], train_loss: 2.3031, train_acc: 0.0981, val_loss: 2.3029, val_acc: 0.0998, val_precision: 0.0998, val_recall: 0.0998, val_f1: 0.0998
Epoch [27], train_loss: 2.3030, train_acc: 0.0986, val_loss: 2.3029, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [28], train_loss: 2.3030, train_acc: 0.0980, val_loss: 2.3028, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [29], train_loss: 2.3029, train_acc: 0.0971, val_loss: 2.3035, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [30], train_loss: 2.3030, train_acc: 0.0990, val_loss: 2.3029, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [31], train_loss: 2.3030, train_acc: 0.0994, val_loss: 2.3033, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [32], train_loss: 2.3030, train_acc: 0.0993, val_loss: 2.3029, val_acc: 0.0998, val_precision: 0.0998, val_recall: 0.0998, val_f1: 0.0998
Epoch [33], train_loss: 2.3030, train_acc: 0.0982, val_loss: 2.3033, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [34], train_loss: 2.3030, train_acc: 0.1000, val_loss: 2.3031, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [35], train_loss: 2.3030, train_acc: 0.0996, val_loss: 2.3032, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [36], train_loss: 2.3029, train_acc: 0.0991, val_loss: 2.3032, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [37], train_loss: 2.3029, train_acc: 0.0994, val_loss: 2.3029, val_acc: 0.0998, val_precision: 0.0998, val_recall: 0.0998, val_f1: 0.0998
Epoch [38], train_loss: 2.3030, train_acc: 0.0979, val_loss: 2.3029, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [39], train_loss: 2.3030, train_acc: 0.0999, val_loss: 2.3032, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [40], train_loss: 2.3030, train_acc: 0.0987, val_loss: 2.3031, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [41], train_loss: 2.3030, train_acc: 0.0971, val_loss: 2.3028, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [42], train_loss: 2.3031, train_acc: 0.0985, val_loss: 2.3030, val_acc: 0.1029, val_precision: 0.1029, val_recall: 0.1029, val_f1: 0.1029
Epoch [43], train_loss: 2.3030, train_acc: 0.0995, val_loss: 2.3031, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [44], train_loss: 2.3030, train_acc: 0.0977, val_loss: 2.3032, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [45], train_loss: 2.3030, train_acc: 0.0997, val_loss: 2.3025, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [46], train_loss: 2.3031, train_acc: 0.1004, val_loss: 2.3028, val_acc: 0.1029, val_precision: 0.1029, val_recall: 0.1029, val_f1: 0.1029
Epoch [47], train_loss: 2.3031, train_acc: 0.0975, val_loss: 2.3028, val_acc: 0.0992, val_precision: 0.0992, val_recall: 0.0992, val_f1: 0.0992
Epoch [48], train_loss: 2.3029, train_acc: 0.0985, val_loss: 2.3032, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [49], train_loss: 2.3031, train_acc: 0.0972, val_loss: 2.3032, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [50], train_loss: 2.3029, train_acc: 0.1000, val_loss: 2.3028, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [51], train_loss: 2.3031, train_acc: 0.0982, val_loss: 2.3029, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [52], train_loss: 2.3029, train_acc: 0.1002, val_loss: 2.3031, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [53], train_loss: 2.3030, train_acc: 0.0991, val_loss: 2.3033, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [54], train_loss: 2.3029, train_acc: 0.1004, val_loss: 2.3035, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [55], train_loss: 2.3030, train_acc: 0.1012, val_loss: 2.3031, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [56], train_loss: 2.3030, train_acc: 0.1003, val_loss: 2.3033, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [57], train_loss: 2.3030, train_acc: 0.0995, val_loss: 2.3041, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [58], train_loss: 2.3031, train_acc: 0.0970, val_loss: 2.3031, val_acc: 0.0992, val_precision: 0.0992, val_recall: 0.0992, val_f1: 0.0992
Epoch [59], train_loss: 2.3031, train_acc: 0.0949, val_loss: 2.3028, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [60], train_loss: 2.3030, train_acc: 0.0994, val_loss: 2.3031, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [61], train_loss: 2.3030, train_acc: 0.0983, val_loss: 2.3030, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [62], train_loss: 2.3030, train_acc: 0.0993, val_loss: 2.3030, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [63], train_loss: 2.3029, train_acc: 0.0985, val_loss: 2.3035, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [64], train_loss: 2.3031, train_acc: 0.0971, val_loss: 2.3034, val_acc: 0.0998, val_precision: 0.0998, val_recall: 0.0998, val_f1: 0.0998
Epoch [65], train_loss: 2.3031, train_acc: 0.0991, val_loss: 2.3024, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [66], train_loss: 2.3030, train_acc: 0.0994, val_loss: 2.3030, val_acc: 0.1029, val_precision: 0.1029, val_recall: 0.1029, val_f1: 0.1029
Epoch [67], train_loss: 2.3029, train_acc: 0.0978, val_loss: 2.3027, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [68], train_loss: 2.3029, train_acc: 0.0991, val_loss: 2.3036, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [69], train_loss: 2.3030, train_acc: 0.1029, val_loss: 2.3023, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [70], train_loss: 2.3030, train_acc: 0.0989, val_loss: 2.3030, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [71], train_loss: 2.3030, train_acc: 0.0972, val_loss: 2.3026, val_acc: 0.0992, val_precision: 0.0992, val_recall: 0.0992, val_f1: 0.0992
Epoch [72], train_loss: 2.3030, train_acc: 0.0977, val_loss: 2.3027, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [73], train_loss: 2.3030, train_acc: 0.0996, val_loss: 2.3030, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [74], train_loss: 2.3030, train_acc: 0.1016, val_loss: 2.3034, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [75], train_loss: 2.3030, train_acc: 0.1008, val_loss: 2.3025, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [76], train_loss: 2.3028, train_acc: 0.0972, val_loss: 2.3039, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [77], train_loss: 2.3030, train_acc: 0.0998, val_loss: 2.3032, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [78], train_loss: 2.3030, train_acc: 0.0995, val_loss: 2.3027, val_acc: 0.0998, val_precision: 0.0998, val_recall: 0.0998, val_f1: 0.0998
Epoch [79], train_loss: 2.3029, train_acc: 0.0993, val_loss: 2.3029, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [80], train_loss: 2.3030, train_acc: 0.0987, val_loss: 2.3029, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [81], train_loss: 2.3030, train_acc: 0.0977, val_loss: 2.3029, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [82], train_loss: 2.3030, train_acc: 0.0984, val_loss: 2.3028, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [83], train_loss: 2.3030, train_acc: 0.0975, val_loss: 2.3031, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [84], train_loss: 2.3030, train_acc: 0.1000, val_loss: 2.3030, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [85], train_loss: 2.3030, train_acc: 0.0997, val_loss: 2.3031, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [86], train_loss: 2.3029, train_acc: 0.0990, val_loss: 2.3030, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [87], train_loss: 2.3030, train_acc: 0.0993, val_loss: 2.3024, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [88], train_loss: 2.3030, train_acc: 0.0985, val_loss: 2.3027, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
Epoch [89], train_loss: 2.3029, train_acc: 0.0993, val_loss: 2.3025, val_acc: 0.1016, val_precision: 0.1016, val_recall: 0.1016, val_f1: 0.1016
Epoch [90], train_loss: 2.3031, train_acc: 0.0983, val_loss: 2.3030, val_acc: 0.0939, val_precision: 0.0939, val_recall: 0.0939, val_f1: 0.0939
Epoch [91], train_loss: 2.3030, train_acc: 0.0990, val_loss: 2.3030, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [92], train_loss: 2.3031, train_acc: 0.0997, val_loss: 2.3029, val_acc: 0.1029, val_precision: 0.1029, val_recall: 0.1029, val_f1: 0.1029
Epoch [93], train_loss: 2.3029, train_acc: 0.1017, val_loss: 2.3037, val_acc: 0.0937, val_precision: 0.0937, val_recall: 0.0937, val_f1: 0.0937
Epoch [94], train_loss: 2.3030, train_acc: 0.0982, val_loss: 2.3029, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [95], train_loss: 2.3030, train_acc: 0.0988, val_loss: 2.3027, val_acc: 0.0998, val_precision: 0.0998, val_recall: 0.0998, val_f1: 0.0998
Epoch [96], train_loss: 2.3031, train_acc: 0.0988, val_loss: 2.3028, val_acc: 0.0986, val_precision: 0.0986, val_recall: 0.0986, val_f1: 0.0986
Epoch [97], train_loss: 2.3029, train_acc: 0.0973, val_loss: 2.3028, val_acc: 0.1029, val_precision: 0.1029, val_recall: 0.1029, val_f1: 0.1029
Epoch [98], train_loss: 2.3029, train_acc: 0.0998, val_loss: 2.3024, val_acc: 0.1037, val_precision: 0.1037, val_recall: 0.1037, val_f1: 0.1037
Epoch [99], train_loss: 2.3030, train_acc: 0.1002, val_loss: 2.3034, val_acc: 0.0999, val_precision: 0.0999, val_recall: 0.0999, val_f1: 0.0999
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 2.3031, val_acc: 0.0977, val_precision: 0.0977, val_recall: 0.0977, val_f1: 0.0977
Summary result of test set => last model => val_loss: 2.3029, val_acc: 0.1021, val_precision: 0.1021, val_recall: 0.1021, val_f1: 0.1021
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.1000
--
confusion matrix
[[   0    0    0    0    0    0 1000    0    0    0]
 [   0    0    0    0    0    0 1000    0    0    0]
 [   0    0    0    0    0    0 1000    0    0    0]
 [   0    0    0    0    0    0 1000    0    0    0]
 [   0    0    0    0    0    0 1000    0    0    0]
 [   0    0    0    0    0    0 1000    0    0    0]
 [   0    0    0    0    0    0 1000    0    0    0]
 [   0    0    0    0    0    0 1000    0    0    0]
 [   0    0    0    0    0    0 1000    0    0    0]
 [   0    0    0    0    0    0 1000    0    0    0]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.00      0.00      0.00      1000
  automobile       0.00      0.00      0.00      1000
        bird       0.00      0.00      0.00      1000
         cat       0.00      0.00      0.00      1000
        deer       0.00      0.00      0.00      1000
         dog       0.00      0.00      0.00      1000
        frog       0.10      1.00      0.18      1000
       horse       0.00      0.00      0.00      1000
        ship       0.00      0.00      0.00      1000
       truck       0.00      0.00      0.00      1000

    accuracy                           0.10     10000
   macro avg       0.01      0.10      0.02     10000
weighted avg       0.01      0.10      0.02     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.1014
--
confusion matrix
[[  0   0   0   0   0   0 488   0   0   0]
 [  0   0   0   0   0   0 512   0   0   0]
 [  0   0   0   0   0   0 532   0   0   0]
 [  0   0   0   0   0   0 471   0   0   0]
 [  0   0   0   0   0   0 471   0   0   0]
 [  0   0   0   0   0   0 514   0   0   0]
 [  0   0   0   0   0   0 507   0   0   0]
 [  0   0   0   0   0   0 500   0   0   0]
 [  0   0   0   0   0   0 504   0   0   0]
 [  0   0   0   0   0   0 501   0   0   0]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.00      0.00      0.00       488
  automobile       0.00      0.00      0.00       512
        bird       0.00      0.00      0.00       532
         cat       0.00      0.00      0.00       471
        deer       0.00      0.00      0.00       471
         dog       0.00      0.00      0.00       514
        frog       0.10      1.00      0.18       507
       horse       0.00      0.00      0.00       500
        ship       0.00      0.00      0.00       504
       truck       0.00      0.00      0.00       501

    accuracy                           0.10      5000
   macro avg       0.01      0.10      0.02      5000
weighted avg       0.01      0.10      0.02      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 298694: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Tue Feb 27 14:05:11 2024
Job was executed on host(s) <hgn54>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 14:05:26 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 14:05:26 2024
Terminated at Tue Feb 27 14:13:31 2024
Results reported at Tue Feb 27 14:13:31 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_LR_005_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_LR_005_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1447.00 sec.
    Max Memory :                                 3400 MB
    Average Memory :                             3226.42 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6840.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   485 sec.
    Turnaround time :                            500 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_LR_005_err_298694> for stderr output of this job.

