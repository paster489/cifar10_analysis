loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.7870, train_acc: 0.3194, val_loss: 1.5858, val_acc: 0.4127, val_precision: 0.4127, val_recall: 0.4127, val_f1: 0.4127
Epoch [1], train_loss: 1.2996, train_acc: 0.5209, val_loss: 1.1412, val_acc: 0.5950, val_precision: 0.5950, val_recall: 0.5950, val_f1: 0.5950
Epoch [2], train_loss: 1.0572, train_acc: 0.6183, val_loss: 0.9274, val_acc: 0.6655, val_precision: 0.6655, val_recall: 0.6655, val_f1: 0.6655
Epoch [3], train_loss: 0.8793, train_acc: 0.6869, val_loss: 0.8406, val_acc: 0.6918, val_precision: 0.6918, val_recall: 0.6918, val_f1: 0.6918
Epoch [4], train_loss: 0.7651, train_acc: 0.7301, val_loss: 0.8037, val_acc: 0.7167, val_precision: 0.7167, val_recall: 0.7167, val_f1: 0.7167
Epoch [5], train_loss: 0.6868, train_acc: 0.7589, val_loss: 0.6617, val_acc: 0.7720, val_precision: 0.7720, val_recall: 0.7720, val_f1: 0.7720
Epoch [6], train_loss: 0.6226, train_acc: 0.7820, val_loss: 0.6441, val_acc: 0.7737, val_precision: 0.7737, val_recall: 0.7737, val_f1: 0.7737
Epoch [7], train_loss: 0.5727, train_acc: 0.7994, val_loss: 0.6070, val_acc: 0.7910, val_precision: 0.7910, val_recall: 0.7910, val_f1: 0.7910
Epoch [8], train_loss: 0.5345, train_acc: 0.8146, val_loss: 0.6011, val_acc: 0.7945, val_precision: 0.7945, val_recall: 0.7945, val_f1: 0.7945
Epoch [9], train_loss: 0.5053, train_acc: 0.8235, val_loss: 0.5957, val_acc: 0.8036, val_precision: 0.8036, val_recall: 0.8036, val_f1: 0.8036
Epoch [10], train_loss: 0.4751, train_acc: 0.8340, val_loss: 0.5959, val_acc: 0.7905, val_precision: 0.7905, val_recall: 0.7905, val_f1: 0.7905
Epoch [11], train_loss: 0.4521, train_acc: 0.8439, val_loss: 0.5491, val_acc: 0.8102, val_precision: 0.8102, val_recall: 0.8102, val_f1: 0.8102
Epoch [12], train_loss: 0.4279, train_acc: 0.8521, val_loss: 0.5579, val_acc: 0.8086, val_precision: 0.8086, val_recall: 0.8086, val_f1: 0.8086
Epoch [13], train_loss: 0.4092, train_acc: 0.8575, val_loss: 0.5217, val_acc: 0.8237, val_precision: 0.8237, val_recall: 0.8237, val_f1: 0.8237
Epoch [14], train_loss: 0.3907, train_acc: 0.8656, val_loss: 0.5134, val_acc: 0.8248, val_precision: 0.8248, val_recall: 0.8248, val_f1: 0.8248
Epoch [15], train_loss: 0.3782, train_acc: 0.8683, val_loss: 0.5049, val_acc: 0.8313, val_precision: 0.8313, val_recall: 0.8313, val_f1: 0.8313
Epoch [16], train_loss: 0.3631, train_acc: 0.8751, val_loss: 0.5424, val_acc: 0.8200, val_precision: 0.8200, val_recall: 0.8200, val_f1: 0.8200
Epoch [17], train_loss: 0.3482, train_acc: 0.8792, val_loss: 0.5150, val_acc: 0.8382, val_precision: 0.8382, val_recall: 0.8382, val_f1: 0.8382
Epoch [18], train_loss: 0.3404, train_acc: 0.8827, val_loss: 0.5072, val_acc: 0.8254, val_precision: 0.8254, val_recall: 0.8254, val_f1: 0.8254
Epoch [19], train_loss: 0.3293, train_acc: 0.8849, val_loss: 0.5027, val_acc: 0.8386, val_precision: 0.8386, val_recall: 0.8386, val_f1: 0.8386
Epoch [20], train_loss: 0.3147, train_acc: 0.8901, val_loss: 0.5303, val_acc: 0.8270, val_precision: 0.8270, val_recall: 0.8270, val_f1: 0.8270
Epoch [21], train_loss: 0.3007, train_acc: 0.8937, val_loss: 0.5140, val_acc: 0.8329, val_precision: 0.8329, val_recall: 0.8329, val_f1: 0.8329
Epoch [22], train_loss: 0.2928, train_acc: 0.8977, val_loss: 0.4994, val_acc: 0.8410, val_precision: 0.8410, val_recall: 0.8410, val_f1: 0.8410
Epoch [23], train_loss: 0.2845, train_acc: 0.9008, val_loss: 0.4978, val_acc: 0.8486, val_precision: 0.8486, val_recall: 0.8486, val_f1: 0.8486
Epoch [24], train_loss: 0.2762, train_acc: 0.9030, val_loss: 0.5189, val_acc: 0.8351, val_precision: 0.8351, val_recall: 0.8351, val_f1: 0.8351
Epoch [25], train_loss: 0.2724, train_acc: 0.9046, val_loss: 0.4997, val_acc: 0.8352, val_precision: 0.8352, val_recall: 0.8352, val_f1: 0.8352
Epoch [26], train_loss: 0.2622, train_acc: 0.9085, val_loss: 0.5197, val_acc: 0.8397, val_precision: 0.8397, val_recall: 0.8397, val_f1: 0.8397
Epoch [27], train_loss: 0.2565, train_acc: 0.9098, val_loss: 0.5069, val_acc: 0.8397, val_precision: 0.8397, val_recall: 0.8397, val_f1: 0.8397
Epoch [28], train_loss: 0.2481, train_acc: 0.9128, val_loss: 0.5413, val_acc: 0.8343, val_precision: 0.8343, val_recall: 0.8343, val_f1: 0.8343
Epoch [29], train_loss: 0.2463, train_acc: 0.9131, val_loss: 0.5377, val_acc: 0.8430, val_precision: 0.8430, val_recall: 0.8430, val_f1: 0.8430
Epoch [30], train_loss: 0.2451, train_acc: 0.9150, val_loss: 0.5062, val_acc: 0.8447, val_precision: 0.8447, val_recall: 0.8447, val_f1: 0.8447
Epoch [31], train_loss: 0.2377, train_acc: 0.9182, val_loss: 0.4972, val_acc: 0.8450, val_precision: 0.8450, val_recall: 0.8450, val_f1: 0.8450
Epoch [32], train_loss: 0.2329, train_acc: 0.9185, val_loss: 0.5906, val_acc: 0.8297, val_precision: 0.8297, val_recall: 0.8297, val_f1: 0.8297
Epoch [33], train_loss: 0.2356, train_acc: 0.9180, val_loss: 0.5153, val_acc: 0.8395, val_precision: 0.8395, val_recall: 0.8395, val_f1: 0.8395
Epoch [34], train_loss: 0.2259, train_acc: 0.9214, val_loss: 0.5083, val_acc: 0.8422, val_precision: 0.8422, val_recall: 0.8422, val_f1: 0.8422
Epoch [35], train_loss: 0.2250, train_acc: 0.9211, val_loss: 0.4784, val_acc: 0.8522, val_precision: 0.8522, val_recall: 0.8522, val_f1: 0.8522
Epoch [36], train_loss: 0.2161, train_acc: 0.9243, val_loss: 0.5711, val_acc: 0.8449, val_precision: 0.8449, val_recall: 0.8449, val_f1: 0.8449
Epoch [37], train_loss: 0.2118, train_acc: 0.9268, val_loss: 0.5116, val_acc: 0.8556, val_precision: 0.8556, val_recall: 0.8556, val_f1: 0.8556
Epoch [38], train_loss: 0.2068, train_acc: 0.9284, val_loss: 0.5391, val_acc: 0.8500, val_precision: 0.8500, val_recall: 0.8500, val_f1: 0.8500
Epoch [39], train_loss: 0.2028, train_acc: 0.9300, val_loss: 0.5340, val_acc: 0.8453, val_precision: 0.8453, val_recall: 0.8453, val_f1: 0.8453
Epoch [40], train_loss: 0.1978, train_acc: 0.9320, val_loss: 0.5268, val_acc: 0.8517, val_precision: 0.8517, val_recall: 0.8517, val_f1: 0.8517
Epoch [41], train_loss: 0.1999, train_acc: 0.9298, val_loss: 0.5387, val_acc: 0.8542, val_precision: 0.8542, val_recall: 0.8542, val_f1: 0.8542
Epoch [42], train_loss: 0.1967, train_acc: 0.9321, val_loss: 0.5653, val_acc: 0.8421, val_precision: 0.8421, val_recall: 0.8421, val_f1: 0.8421
Epoch [43], train_loss: 0.1895, train_acc: 0.9322, val_loss: 0.5476, val_acc: 0.8537, val_precision: 0.8537, val_recall: 0.8537, val_f1: 0.8537
Epoch [44], train_loss: 0.1931, train_acc: 0.9326, val_loss: 0.5592, val_acc: 0.8516, val_precision: 0.8516, val_recall: 0.8516, val_f1: 0.8516
Epoch [45], train_loss: 0.1915, train_acc: 0.9339, val_loss: 0.5397, val_acc: 0.8541, val_precision: 0.8541, val_recall: 0.8541, val_f1: 0.8541
Epoch [46], train_loss: 0.1882, train_acc: 0.9350, val_loss: 0.5284, val_acc: 0.8554, val_precision: 0.8554, val_recall: 0.8554, val_f1: 0.8554
Epoch [47], train_loss: 0.1810, train_acc: 0.9377, val_loss: 0.5527, val_acc: 0.8495, val_precision: 0.8495, val_recall: 0.8495, val_f1: 0.8495
Epoch [48], train_loss: 0.1847, train_acc: 0.9371, val_loss: 0.5318, val_acc: 0.8487, val_precision: 0.8487, val_recall: 0.8487, val_f1: 0.8487
Epoch [49], train_loss: 0.1734, train_acc: 0.9385, val_loss: 0.4934, val_acc: 0.8577, val_precision: 0.8577, val_recall: 0.8577, val_f1: 0.8577
Epoch [50], train_loss: 0.1728, train_acc: 0.9389, val_loss: 0.5867, val_acc: 0.8569, val_precision: 0.8569, val_recall: 0.8569, val_f1: 0.8569
Epoch [51], train_loss: 0.1793, train_acc: 0.9383, val_loss: 0.5470, val_acc: 0.8503, val_precision: 0.8503, val_recall: 0.8503, val_f1: 0.8503
Epoch [52], train_loss: 0.1794, train_acc: 0.9380, val_loss: 0.4943, val_acc: 0.8662, val_precision: 0.8662, val_recall: 0.8662, val_f1: 0.8662
Epoch [53], train_loss: 0.1674, train_acc: 0.9421, val_loss: 0.5664, val_acc: 0.8556, val_precision: 0.8556, val_recall: 0.8556, val_f1: 0.8556
Epoch [54], train_loss: 0.1746, train_acc: 0.9399, val_loss: 0.5571, val_acc: 0.8510, val_precision: 0.8510, val_recall: 0.8510, val_f1: 0.8510
Epoch [55], train_loss: 0.1657, train_acc: 0.9429, val_loss: 0.5447, val_acc: 0.8542, val_precision: 0.8542, val_recall: 0.8542, val_f1: 0.8542
Epoch [56], train_loss: 0.1675, train_acc: 0.9426, val_loss: 0.5664, val_acc: 0.8429, val_precision: 0.8429, val_recall: 0.8429, val_f1: 0.8429
Epoch [57], train_loss: 0.1635, train_acc: 0.9438, val_loss: 0.5282, val_acc: 0.8539, val_precision: 0.8539, val_recall: 0.8539, val_f1: 0.8539
Epoch [58], train_loss: 0.1602, train_acc: 0.9452, val_loss: 0.5519, val_acc: 0.8494, val_precision: 0.8494, val_recall: 0.8494, val_f1: 0.8494
Epoch [59], train_loss: 0.1655, train_acc: 0.9438, val_loss: 0.5654, val_acc: 0.8530, val_precision: 0.8530, val_recall: 0.8530, val_f1: 0.8530
Epoch [60], train_loss: 0.1524, train_acc: 0.9475, val_loss: 0.5724, val_acc: 0.8572, val_precision: 0.8572, val_recall: 0.8572, val_f1: 0.8572
Epoch [61], train_loss: 0.1640, train_acc: 0.9441, val_loss: 0.5897, val_acc: 0.8525, val_precision: 0.8525, val_recall: 0.8525, val_f1: 0.8525
Epoch [62], train_loss: 0.1526, train_acc: 0.9473, val_loss: 0.5628, val_acc: 0.8602, val_precision: 0.8602, val_recall: 0.8602, val_f1: 0.8602
Epoch [63], train_loss: 0.1523, train_acc: 0.9479, val_loss: 0.5244, val_acc: 0.8565, val_precision: 0.8565, val_recall: 0.8565, val_f1: 0.8565
Epoch [64], train_loss: 0.1520, train_acc: 0.9483, val_loss: 0.5451, val_acc: 0.8552, val_precision: 0.8552, val_recall: 0.8552, val_f1: 0.8552
Epoch [65], train_loss: 0.1505, train_acc: 0.9488, val_loss: 0.6204, val_acc: 0.8464, val_precision: 0.8464, val_recall: 0.8464, val_f1: 0.8464
Epoch [66], train_loss: 0.1517, train_acc: 0.9485, val_loss: 0.5601, val_acc: 0.8542, val_precision: 0.8542, val_recall: 0.8542, val_f1: 0.8542
Epoch [67], train_loss: 0.1465, train_acc: 0.9503, val_loss: 0.5455, val_acc: 0.8539, val_precision: 0.8539, val_recall: 0.8539, val_f1: 0.8539
Epoch [68], train_loss: 0.1482, train_acc: 0.9497, val_loss: 0.6318, val_acc: 0.8452, val_precision: 0.8452, val_recall: 0.8452, val_f1: 0.8452
Epoch [69], train_loss: 0.1442, train_acc: 0.9507, val_loss: 0.5854, val_acc: 0.8513, val_precision: 0.8513, val_recall: 0.8513, val_f1: 0.8513
Epoch [70], train_loss: 0.1506, train_acc: 0.9487, val_loss: 0.6045, val_acc: 0.8423, val_precision: 0.8423, val_recall: 0.8423, val_f1: 0.8423
Epoch [71], train_loss: 0.1435, train_acc: 0.9518, val_loss: 0.6157, val_acc: 0.8488, val_precision: 0.8488, val_recall: 0.8488, val_f1: 0.8488
Epoch [72], train_loss: 0.1425, train_acc: 0.9523, val_loss: 0.5398, val_acc: 0.8608, val_precision: 0.8608, val_recall: 0.8608, val_f1: 0.8608
Epoch [73], train_loss: 0.1374, train_acc: 0.9529, val_loss: 0.5560, val_acc: 0.8541, val_precision: 0.8541, val_recall: 0.8541, val_f1: 0.8541
Epoch [74], train_loss: 0.1378, train_acc: 0.9527, val_loss: 0.5652, val_acc: 0.8559, val_precision: 0.8559, val_recall: 0.8559, val_f1: 0.8559
Epoch [75], train_loss: 0.1466, train_acc: 0.9506, val_loss: 0.6061, val_acc: 0.8523, val_precision: 0.8523, val_recall: 0.8523, val_f1: 0.8523
Epoch [76], train_loss: 0.1410, train_acc: 0.9539, val_loss: 0.5600, val_acc: 0.8546, val_precision: 0.8546, val_recall: 0.8546, val_f1: 0.8546
Epoch [77], train_loss: 0.1390, train_acc: 0.9539, val_loss: 0.5727, val_acc: 0.8552, val_precision: 0.8552, val_recall: 0.8552, val_f1: 0.8552
Epoch [78], train_loss: 0.1376, train_acc: 0.9541, val_loss: 0.5927, val_acc: 0.8608, val_precision: 0.8608, val_recall: 0.8608, val_f1: 0.8608
Epoch [79], train_loss: 0.1395, train_acc: 0.9524, val_loss: 0.5873, val_acc: 0.8566, val_precision: 0.8566, val_recall: 0.8566, val_f1: 0.8566
Epoch [80], train_loss: 0.1379, train_acc: 0.9529, val_loss: 0.5775, val_acc: 0.8601, val_precision: 0.8601, val_recall: 0.8601, val_f1: 0.8601
Epoch [81], train_loss: 0.1373, train_acc: 0.9544, val_loss: 0.5696, val_acc: 0.8546, val_precision: 0.8546, val_recall: 0.8546, val_f1: 0.8546
Epoch [82], train_loss: 0.1292, train_acc: 0.9574, val_loss: 0.5680, val_acc: 0.8574, val_precision: 0.8574, val_recall: 0.8574, val_f1: 0.8574
Epoch [83], train_loss: 0.1318, train_acc: 0.9559, val_loss: 0.5838, val_acc: 0.8519, val_precision: 0.8519, val_recall: 0.8519, val_f1: 0.8519
Epoch [84], train_loss: 0.1307, train_acc: 0.9562, val_loss: 0.5562, val_acc: 0.8552, val_precision: 0.8552, val_recall: 0.8552, val_f1: 0.8552
Epoch [85], train_loss: 0.1298, train_acc: 0.9562, val_loss: 0.5815, val_acc: 0.8549, val_precision: 0.8549, val_recall: 0.8549, val_f1: 0.8549
Epoch [86], train_loss: 0.1261, train_acc: 0.9568, val_loss: 0.5948, val_acc: 0.8586, val_precision: 0.8586, val_recall: 0.8586, val_f1: 0.8586
Epoch [87], train_loss: 0.1249, train_acc: 0.9573, val_loss: 0.6208, val_acc: 0.8582, val_precision: 0.8582, val_recall: 0.8582, val_f1: 0.8582
Epoch [88], train_loss: 0.1286, train_acc: 0.9590, val_loss: 0.5910, val_acc: 0.8604, val_precision: 0.8604, val_recall: 0.8604, val_f1: 0.8604
Epoch [89], train_loss: 0.1250, train_acc: 0.9585, val_loss: 0.5880, val_acc: 0.8584, val_precision: 0.8584, val_recall: 0.8584, val_f1: 0.8584
Epoch [90], train_loss: 0.1249, train_acc: 0.9589, val_loss: 0.5798, val_acc: 0.8678, val_precision: 0.8678, val_recall: 0.8678, val_f1: 0.8678
Epoch [91], train_loss: 0.1272, train_acc: 0.9586, val_loss: 0.6002, val_acc: 0.8606, val_precision: 0.8606, val_recall: 0.8606, val_f1: 0.8606
Epoch [92], train_loss: 0.1281, train_acc: 0.9577, val_loss: 0.5485, val_acc: 0.8608, val_precision: 0.8608, val_recall: 0.8608, val_f1: 0.8608
Epoch [93], train_loss: 0.1198, train_acc: 0.9606, val_loss: 0.6022, val_acc: 0.8553, val_precision: 0.8553, val_recall: 0.8553, val_f1: 0.8553
Epoch [94], train_loss: 0.1203, train_acc: 0.9605, val_loss: 0.6177, val_acc: 0.8569, val_precision: 0.8569, val_recall: 0.8569, val_f1: 0.8569
Epoch [95], train_loss: 0.1177, train_acc: 0.9616, val_loss: 0.6270, val_acc: 0.8571, val_precision: 0.8571, val_recall: 0.8571, val_f1: 0.8571
Epoch [96], train_loss: 0.1225, train_acc: 0.9595, val_loss: 0.6106, val_acc: 0.8605, val_precision: 0.8605, val_recall: 0.8605, val_f1: 0.8605
Epoch [97], train_loss: 0.1227, train_acc: 0.9598, val_loss: 0.5941, val_acc: 0.8617, val_precision: 0.8617, val_recall: 0.8617, val_f1: 0.8617
Epoch [98], train_loss: 0.1172, train_acc: 0.9612, val_loss: 0.5859, val_acc: 0.8522, val_precision: 0.8522, val_recall: 0.8522, val_f1: 0.8522
Epoch [99], train_loss: 0.1231, train_acc: 0.9594, val_loss: 0.6201, val_acc: 0.8582, val_precision: 0.8582, val_recall: 0.8582, val_f1: 0.8582
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.4679, val_acc: 0.8623, val_precision: 0.8623, val_recall: 0.8623, val_f1: 0.8623
Summary result of test set => last model => val_loss: 0.6154, val_acc: 0.8675, val_precision: 0.8675, val_recall: 0.8675, val_f1: 0.8675
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.8620
--
confusion matrix
[[856  14  43  10  10   1   0   4  49  13]
 [  5 953   1   2   0   0   1   0   8  30]
 [ 26   1 818  43  39  20  22  20   7   4]
 [ 11   8  48 758  39  78  21  17  12   8]
 [  7   2  37  33 860  13  13  29   5   1]
 [  6   3  37 127  29 764   5  22   6   1]
 [  6   3  39  53  15   5 871   2   3   3]
 [ 13   2  13  25  26  20   1 892   2   6]
 [ 30  11   3   4   1   0   3   2 937   9]
 [ 13  56   3   5   0   0   0   1  11 911]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.88      0.86      0.87      1000
  automobile       0.91      0.95      0.93      1000
        bird       0.79      0.82      0.80      1000
         cat       0.72      0.76      0.74      1000
        deer       0.84      0.86      0.85      1000
         dog       0.85      0.76      0.80      1000
        frog       0.93      0.87      0.90      1000
       horse       0.90      0.89      0.90      1000
        ship       0.90      0.94      0.92      1000
       truck       0.92      0.91      0.92      1000

    accuracy                           0.86     10000
   macro avg       0.86      0.86      0.86     10000
weighted avg       0.86      0.86      0.86     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.8434
--
confusion matrix
[[406   6  25   0   8   2   1   2  27  11]
 [  2 483   1   0   1   0   0   0   8  17]
 [ 22   5 432  18  19  11  11   6   5   3]
 [  7   3  23 344  20  46   7  12   6   3]
 [  3   1  13  24 396  10   6  14   1   3]
 [  1   4  15  80  18 371   7  17   1   0]
 [  1   6  19  15  14   4 439   2   6   1]
 [  4   1  15   9  15  19   0 432   1   4]
 [ 23   6   1   1   1   1   0   1 466   4]
 [  9  24   1   3   1   1   1   2  11 448]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.85      0.83      0.84       488
  automobile       0.90      0.94      0.92       512
        bird       0.79      0.81      0.80       532
         cat       0.70      0.73      0.71       471
        deer       0.80      0.84      0.82       471
         dog       0.80      0.72      0.76       514
        frog       0.93      0.87      0.90       507
       horse       0.89      0.86      0.87       500
        ship       0.88      0.92      0.90       504
       truck       0.91      0.89      0.90       501

    accuracy                           0.84      5000
   macro avg       0.84      0.84      0.84      5000
weighted avg       0.84      0.84      0.84      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 298657: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Tue Feb 27 14:04:37 2024
Job was executed on host(s) <hgn54>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 14:04:56 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 14:04:56 2024
Terminated at Tue Feb 27 14:20:28 2024
Results reported at Tue Feb 27 14:20:28 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_N1_aug_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_N1_aug_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2790.00 sec.
    Max Memory :                                 3572 MB
    Average Memory :                             3226.89 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6668.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                105
    Run time :                                   933 sec.
    Turnaround time :                            951 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_N1_aug_err_298657> for stderr output of this job.

