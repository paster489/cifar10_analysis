loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.7870, train_acc: 0.3194, val_loss: 1.5858, val_acc: 0.4127, val_precision: 0.4462, val_recall: 0.4127, val_f1: 0.3720
Epoch [1], train_loss: 1.2996, train_acc: 0.5209, val_loss: 1.1412, val_acc: 0.5950, val_precision: 0.6009, val_recall: 0.5950, val_f1: 0.5858
Epoch [2], train_loss: 1.0572, train_acc: 0.6183, val_loss: 0.9274, val_acc: 0.6655, val_precision: 0.6715, val_recall: 0.6655, val_f1: 0.6604
Epoch [3], train_loss: 0.8793, train_acc: 0.6869, val_loss: 0.8406, val_acc: 0.6918, val_precision: 0.7132, val_recall: 0.6918, val_f1: 0.6878
Epoch [4], train_loss: 0.7651, train_acc: 0.7301, val_loss: 0.8037, val_acc: 0.7167, val_precision: 0.7530, val_recall: 0.7167, val_f1: 0.7199
Epoch [5], train_loss: 0.6868, train_acc: 0.7589, val_loss: 0.6617, val_acc: 0.7720, val_precision: 0.7819, val_recall: 0.7720, val_f1: 0.7709
Epoch [6], train_loss: 0.6226, train_acc: 0.7820, val_loss: 0.6441, val_acc: 0.7737, val_precision: 0.7886, val_recall: 0.7737, val_f1: 0.7738
Epoch [7], train_loss: 0.5727, train_acc: 0.7994, val_loss: 0.6070, val_acc: 0.7910, val_precision: 0.7995, val_recall: 0.7910, val_f1: 0.7908
Epoch [8], train_loss: 0.5345, train_acc: 0.8146, val_loss: 0.6011, val_acc: 0.7945, val_precision: 0.8071, val_recall: 0.7945, val_f1: 0.7960
Epoch [9], train_loss: 0.5053, train_acc: 0.8235, val_loss: 0.5957, val_acc: 0.8036, val_precision: 0.8178, val_recall: 0.8036, val_f1: 0.8029
Epoch [10], train_loss: 0.4751, train_acc: 0.8340, val_loss: 0.5959, val_acc: 0.7905, val_precision: 0.8054, val_recall: 0.7905, val_f1: 0.7894
Epoch [11], train_loss: 0.4521, train_acc: 0.8439, val_loss: 0.5491, val_acc: 0.8102, val_precision: 0.8190, val_recall: 0.8102, val_f1: 0.8099
Epoch [12], train_loss: 0.4279, train_acc: 0.8521, val_loss: 0.5579, val_acc: 0.8086, val_precision: 0.8160, val_recall: 0.8086, val_f1: 0.8067
Epoch [13], train_loss: 0.4092, train_acc: 0.8575, val_loss: 0.5217, val_acc: 0.8237, val_precision: 0.8318, val_recall: 0.8237, val_f1: 0.8229
Epoch [14], train_loss: 0.3907, train_acc: 0.8656, val_loss: 0.5134, val_acc: 0.8248, val_precision: 0.8341, val_recall: 0.8248, val_f1: 0.8251
Epoch [15], train_loss: 0.3782, train_acc: 0.8683, val_loss: 0.5049, val_acc: 0.8313, val_precision: 0.8407, val_recall: 0.8313, val_f1: 0.8316
Epoch [16], train_loss: 0.3631, train_acc: 0.8751, val_loss: 0.5424, val_acc: 0.8200, val_precision: 0.8329, val_recall: 0.8200, val_f1: 0.8200
Epoch [17], train_loss: 0.3482, train_acc: 0.8792, val_loss: 0.5150, val_acc: 0.8382, val_precision: 0.8462, val_recall: 0.8382, val_f1: 0.8377
Epoch [18], train_loss: 0.3404, train_acc: 0.8827, val_loss: 0.5072, val_acc: 0.8254, val_precision: 0.8316, val_recall: 0.8254, val_f1: 0.8254
Epoch [19], train_loss: 0.3293, train_acc: 0.8849, val_loss: 0.5027, val_acc: 0.8386, val_precision: 0.8519, val_recall: 0.8386, val_f1: 0.8396
Epoch [20], train_loss: 0.3147, train_acc: 0.8901, val_loss: 0.5303, val_acc: 0.8270, val_precision: 0.8353, val_recall: 0.8270, val_f1: 0.8279
Epoch [21], train_loss: 0.3007, train_acc: 0.8937, val_loss: 0.5140, val_acc: 0.8329, val_precision: 0.8430, val_recall: 0.8329, val_f1: 0.8326
Epoch [22], train_loss: 0.2928, train_acc: 0.8977, val_loss: 0.4994, val_acc: 0.8410, val_precision: 0.8496, val_recall: 0.8410, val_f1: 0.8418
Epoch [23], train_loss: 0.2845, train_acc: 0.9008, val_loss: 0.4978, val_acc: 0.8486, val_precision: 0.8519, val_recall: 0.8486, val_f1: 0.8472
Epoch [24], train_loss: 0.2762, train_acc: 0.9030, val_loss: 0.5189, val_acc: 0.8351, val_precision: 0.8468, val_recall: 0.8351, val_f1: 0.8361
Epoch [25], train_loss: 0.2724, train_acc: 0.9046, val_loss: 0.4997, val_acc: 0.8352, val_precision: 0.8413, val_recall: 0.8352, val_f1: 0.8355
Epoch [26], train_loss: 0.2622, train_acc: 0.9085, val_loss: 0.5197, val_acc: 0.8397, val_precision: 0.8476, val_recall: 0.8397, val_f1: 0.8387
Epoch [27], train_loss: 0.2565, train_acc: 0.9098, val_loss: 0.5069, val_acc: 0.8397, val_precision: 0.8462, val_recall: 0.8397, val_f1: 0.8399
Epoch [28], train_loss: 0.2481, train_acc: 0.9128, val_loss: 0.5413, val_acc: 0.8343, val_precision: 0.8446, val_recall: 0.8343, val_f1: 0.8348
Epoch [29], train_loss: 0.2463, train_acc: 0.9131, val_loss: 0.5377, val_acc: 0.8430, val_precision: 0.8565, val_recall: 0.8430, val_f1: 0.8447
Epoch [30], train_loss: 0.2451, train_acc: 0.9150, val_loss: 0.5062, val_acc: 0.8447, val_precision: 0.8497, val_recall: 0.8447, val_f1: 0.8445
Epoch [31], train_loss: 0.2377, train_acc: 0.9182, val_loss: 0.4972, val_acc: 0.8450, val_precision: 0.8535, val_recall: 0.8450, val_f1: 0.8456
Epoch [32], train_loss: 0.2329, train_acc: 0.9185, val_loss: 0.5906, val_acc: 0.8297, val_precision: 0.8446, val_recall: 0.8297, val_f1: 0.8303
Epoch [33], train_loss: 0.2356, train_acc: 0.9180, val_loss: 0.5153, val_acc: 0.8395, val_precision: 0.8455, val_recall: 0.8395, val_f1: 0.8389
Epoch [34], train_loss: 0.2259, train_acc: 0.9214, val_loss: 0.5083, val_acc: 0.8422, val_precision: 0.8492, val_recall: 0.8422, val_f1: 0.8410
Epoch [35], train_loss: 0.2250, train_acc: 0.9211, val_loss: 0.4784, val_acc: 0.8522, val_precision: 0.8581, val_recall: 0.8522, val_f1: 0.8522
Epoch [36], train_loss: 0.2161, train_acc: 0.9243, val_loss: 0.5711, val_acc: 0.8449, val_precision: 0.8563, val_recall: 0.8449, val_f1: 0.8459
Epoch [37], train_loss: 0.2118, train_acc: 0.9268, val_loss: 0.5116, val_acc: 0.8556, val_precision: 0.8619, val_recall: 0.8556, val_f1: 0.8554
Epoch [38], train_loss: 0.2068, train_acc: 0.9284, val_loss: 0.5391, val_acc: 0.8500, val_precision: 0.8566, val_recall: 0.8500, val_f1: 0.8502
Epoch [39], train_loss: 0.2028, train_acc: 0.9300, val_loss: 0.5340, val_acc: 0.8453, val_precision: 0.8513, val_recall: 0.8453, val_f1: 0.8444
Epoch [40], train_loss: 0.1978, train_acc: 0.9320, val_loss: 0.5268, val_acc: 0.8517, val_precision: 0.8591, val_recall: 0.8517, val_f1: 0.8522
Epoch [41], train_loss: 0.1999, train_acc: 0.9298, val_loss: 0.5387, val_acc: 0.8542, val_precision: 0.8631, val_recall: 0.8542, val_f1: 0.8551
Epoch [42], train_loss: 0.1967, train_acc: 0.9321, val_loss: 0.5653, val_acc: 0.8421, val_precision: 0.8519, val_recall: 0.8421, val_f1: 0.8430
Epoch [43], train_loss: 0.1895, train_acc: 0.9322, val_loss: 0.5476, val_acc: 0.8537, val_precision: 0.8612, val_recall: 0.8537, val_f1: 0.8541
Epoch [44], train_loss: 0.1931, train_acc: 0.9326, val_loss: 0.5592, val_acc: 0.8516, val_precision: 0.8565, val_recall: 0.8516, val_f1: 0.8503
Epoch [45], train_loss: 0.1915, train_acc: 0.9339, val_loss: 0.5397, val_acc: 0.8541, val_precision: 0.8625, val_recall: 0.8541, val_f1: 0.8548
Epoch [46], train_loss: 0.1882, train_acc: 0.9350, val_loss: 0.5284, val_acc: 0.8554, val_precision: 0.8625, val_recall: 0.8554, val_f1: 0.8565
Epoch [47], train_loss: 0.1810, train_acc: 0.9377, val_loss: 0.5527, val_acc: 0.8495, val_precision: 0.8546, val_recall: 0.8495, val_f1: 0.8483
Epoch [48], train_loss: 0.1847, train_acc: 0.9371, val_loss: 0.5318, val_acc: 0.8487, val_precision: 0.8542, val_recall: 0.8487, val_f1: 0.8484
Epoch [49], train_loss: 0.1734, train_acc: 0.9385, val_loss: 0.4934, val_acc: 0.8577, val_precision: 0.8655, val_recall: 0.8577, val_f1: 0.8586
Epoch [50], train_loss: 0.1728, train_acc: 0.9389, val_loss: 0.5867, val_acc: 0.8569, val_precision: 0.8660, val_recall: 0.8569, val_f1: 0.8573
Epoch [51], train_loss: 0.1793, train_acc: 0.9383, val_loss: 0.5470, val_acc: 0.8503, val_precision: 0.8599, val_recall: 0.8503, val_f1: 0.8516
Epoch [52], train_loss: 0.1794, train_acc: 0.9380, val_loss: 0.4943, val_acc: 0.8662, val_precision: 0.8714, val_recall: 0.8662, val_f1: 0.8663
Epoch [53], train_loss: 0.1674, train_acc: 0.9421, val_loss: 0.5664, val_acc: 0.8556, val_precision: 0.8656, val_recall: 0.8556, val_f1: 0.8568
Epoch [54], train_loss: 0.1746, train_acc: 0.9399, val_loss: 0.5571, val_acc: 0.8510, val_precision: 0.8596, val_recall: 0.8510, val_f1: 0.8515
Epoch [55], train_loss: 0.1657, train_acc: 0.9429, val_loss: 0.5447, val_acc: 0.8542, val_precision: 0.8608, val_recall: 0.8542, val_f1: 0.8538
Epoch [56], train_loss: 0.1675, train_acc: 0.9426, val_loss: 0.5664, val_acc: 0.8429, val_precision: 0.8484, val_recall: 0.8429, val_f1: 0.8415
Epoch [57], train_loss: 0.1635, train_acc: 0.9438, val_loss: 0.5282, val_acc: 0.8539, val_precision: 0.8588, val_recall: 0.8539, val_f1: 0.8533
Epoch [58], train_loss: 0.1602, train_acc: 0.9452, val_loss: 0.5519, val_acc: 0.8494, val_precision: 0.8569, val_recall: 0.8494, val_f1: 0.8495
Epoch [59], train_loss: 0.1655, train_acc: 0.9438, val_loss: 0.5654, val_acc: 0.8530, val_precision: 0.8583, val_recall: 0.8530, val_f1: 0.8522
Epoch [60], train_loss: 0.1524, train_acc: 0.9475, val_loss: 0.5724, val_acc: 0.8572, val_precision: 0.8631, val_recall: 0.8572, val_f1: 0.8565
Epoch [61], train_loss: 0.1640, train_acc: 0.9441, val_loss: 0.5897, val_acc: 0.8525, val_precision: 0.8596, val_recall: 0.8525, val_f1: 0.8521
Epoch [62], train_loss: 0.1526, train_acc: 0.9473, val_loss: 0.5628, val_acc: 0.8602, val_precision: 0.8656, val_recall: 0.8602, val_f1: 0.8597
Epoch [63], train_loss: 0.1523, train_acc: 0.9479, val_loss: 0.5244, val_acc: 0.8565, val_precision: 0.8646, val_recall: 0.8565, val_f1: 0.8568
Epoch [64], train_loss: 0.1520, train_acc: 0.9483, val_loss: 0.5451, val_acc: 0.8552, val_precision: 0.8637, val_recall: 0.8552, val_f1: 0.8558
Epoch [65], train_loss: 0.1505, train_acc: 0.9488, val_loss: 0.6204, val_acc: 0.8464, val_precision: 0.8516, val_recall: 0.8464, val_f1: 0.8461
Epoch [66], train_loss: 0.1517, train_acc: 0.9485, val_loss: 0.5601, val_acc: 0.8542, val_precision: 0.8594, val_recall: 0.8542, val_f1: 0.8538
Epoch [67], train_loss: 0.1465, train_acc: 0.9503, val_loss: 0.5455, val_acc: 0.8539, val_precision: 0.8602, val_recall: 0.8539, val_f1: 0.8543
Epoch [68], train_loss: 0.1482, train_acc: 0.9497, val_loss: 0.6318, val_acc: 0.8452, val_precision: 0.8557, val_recall: 0.8452, val_f1: 0.8462
Epoch [69], train_loss: 0.1442, train_acc: 0.9507, val_loss: 0.5854, val_acc: 0.8513, val_precision: 0.8604, val_recall: 0.8513, val_f1: 0.8522
Epoch [70], train_loss: 0.1506, train_acc: 0.9487, val_loss: 0.6045, val_acc: 0.8423, val_precision: 0.8517, val_recall: 0.8423, val_f1: 0.8421
Epoch [71], train_loss: 0.1435, train_acc: 0.9518, val_loss: 0.6157, val_acc: 0.8488, val_precision: 0.8609, val_recall: 0.8488, val_f1: 0.8494
Epoch [72], train_loss: 0.1425, train_acc: 0.9523, val_loss: 0.5398, val_acc: 0.8608, val_precision: 0.8642, val_recall: 0.8608, val_f1: 0.8602
Epoch [73], train_loss: 0.1374, train_acc: 0.9529, val_loss: 0.5560, val_acc: 0.8541, val_precision: 0.8586, val_recall: 0.8541, val_f1: 0.8534
Epoch [74], train_loss: 0.1378, train_acc: 0.9527, val_loss: 0.5652, val_acc: 0.8559, val_precision: 0.8615, val_recall: 0.8559, val_f1: 0.8554
Epoch [75], train_loss: 0.1466, train_acc: 0.9506, val_loss: 0.6061, val_acc: 0.8523, val_precision: 0.8595, val_recall: 0.8523, val_f1: 0.8527
Epoch [76], train_loss: 0.1410, train_acc: 0.9539, val_loss: 0.5600, val_acc: 0.8546, val_precision: 0.8656, val_recall: 0.8546, val_f1: 0.8558
Epoch [77], train_loss: 0.1390, train_acc: 0.9539, val_loss: 0.5727, val_acc: 0.8552, val_precision: 0.8589, val_recall: 0.8552, val_f1: 0.8543
Epoch [78], train_loss: 0.1376, train_acc: 0.9541, val_loss: 0.5927, val_acc: 0.8608, val_precision: 0.8669, val_recall: 0.8608, val_f1: 0.8610
Epoch [79], train_loss: 0.1395, train_acc: 0.9524, val_loss: 0.5873, val_acc: 0.8566, val_precision: 0.8659, val_recall: 0.8566, val_f1: 0.8577
Epoch [80], train_loss: 0.1379, train_acc: 0.9529, val_loss: 0.5775, val_acc: 0.8601, val_precision: 0.8669, val_recall: 0.8601, val_f1: 0.8602
Epoch [81], train_loss: 0.1373, train_acc: 0.9544, val_loss: 0.5696, val_acc: 0.8546, val_precision: 0.8633, val_recall: 0.8546, val_f1: 0.8556
Epoch [82], train_loss: 0.1292, train_acc: 0.9574, val_loss: 0.5680, val_acc: 0.8574, val_precision: 0.8617, val_recall: 0.8574, val_f1: 0.8570
Epoch [83], train_loss: 0.1318, train_acc: 0.9559, val_loss: 0.5838, val_acc: 0.8519, val_precision: 0.8581, val_recall: 0.8519, val_f1: 0.8523
Epoch [84], train_loss: 0.1307, train_acc: 0.9562, val_loss: 0.5562, val_acc: 0.8552, val_precision: 0.8600, val_recall: 0.8552, val_f1: 0.8548
Epoch [85], train_loss: 0.1298, train_acc: 0.9562, val_loss: 0.5815, val_acc: 0.8549, val_precision: 0.8603, val_recall: 0.8549, val_f1: 0.8548
Epoch [86], train_loss: 0.1261, train_acc: 0.9568, val_loss: 0.5948, val_acc: 0.8586, val_precision: 0.8659, val_recall: 0.8586, val_f1: 0.8590
Epoch [87], train_loss: 0.1249, train_acc: 0.9573, val_loss: 0.6208, val_acc: 0.8582, val_precision: 0.8652, val_recall: 0.8582, val_f1: 0.8591
Epoch [88], train_loss: 0.1286, train_acc: 0.9590, val_loss: 0.5910, val_acc: 0.8604, val_precision: 0.8688, val_recall: 0.8604, val_f1: 0.8614
Epoch [89], train_loss: 0.1250, train_acc: 0.9585, val_loss: 0.5880, val_acc: 0.8584, val_precision: 0.8648, val_recall: 0.8584, val_f1: 0.8589
Epoch [90], train_loss: 0.1249, train_acc: 0.9589, val_loss: 0.5798, val_acc: 0.8678, val_precision: 0.8734, val_recall: 0.8678, val_f1: 0.8673
Epoch [91], train_loss: 0.1272, train_acc: 0.9586, val_loss: 0.6002, val_acc: 0.8606, val_precision: 0.8682, val_recall: 0.8606, val_f1: 0.8611
Epoch [92], train_loss: 0.1281, train_acc: 0.9577, val_loss: 0.5485, val_acc: 0.8608, val_precision: 0.8667, val_recall: 0.8608, val_f1: 0.8605
Epoch [93], train_loss: 0.1198, train_acc: 0.9606, val_loss: 0.6022, val_acc: 0.8553, val_precision: 0.8631, val_recall: 0.8553, val_f1: 0.8557
Epoch [94], train_loss: 0.1203, train_acc: 0.9605, val_loss: 0.6177, val_acc: 0.8569, val_precision: 0.8644, val_recall: 0.8569, val_f1: 0.8573
Epoch [95], train_loss: 0.1177, train_acc: 0.9616, val_loss: 0.6270, val_acc: 0.8571, val_precision: 0.8678, val_recall: 0.8571, val_f1: 0.8588
Epoch [96], train_loss: 0.1225, train_acc: 0.9595, val_loss: 0.6106, val_acc: 0.8605, val_precision: 0.8671, val_recall: 0.8605, val_f1: 0.8613
Epoch [97], train_loss: 0.1227, train_acc: 0.9598, val_loss: 0.5941, val_acc: 0.8617, val_precision: 0.8696, val_recall: 0.8617, val_f1: 0.8625
Epoch [98], train_loss: 0.1172, train_acc: 0.9612, val_loss: 0.5859, val_acc: 0.8522, val_precision: 0.8584, val_recall: 0.8522, val_f1: 0.8523
Epoch [99], train_loss: 0.1231, train_acc: 0.9594, val_loss: 0.6201, val_acc: 0.8582, val_precision: 0.8660, val_recall: 0.8582, val_f1: 0.8589
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.4679, val_acc: 0.8623, val_precision: 0.8687, val_recall: 0.8623, val_f1: 0.8623
Summary result of test set => last model => val_loss: 0.6154, val_acc: 0.8675, val_precision: 0.8741, val_recall: 0.8675, val_f1: 0.8676
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.8620
--
confusion matrix
[[856  14  43  10  10   1   0   4  49  13]
 [  5 953   1   2   0   0   1   0   8  30]
 [ 26   1 818  43  39  20  22  20   7   4]
 [ 11   8  48 758  39  78  21  17  12   8]
 [  7   2  37  33 860  13  13  29   5   1]
 [  6   3  37 127  29 764   5  22   6   1]
 [  6   3  39  53  15   5 871   2   3   3]
 [ 13   2  13  25  26  20   1 892   2   6]
 [ 30  11   3   4   1   0   3   2 937   9]
 [ 13  56   3   5   0   0   0   1  11 911]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.88      0.86      0.87      1000
  automobile       0.91      0.95      0.93      1000
        bird       0.79      0.82      0.80      1000
         cat       0.72      0.76      0.74      1000
        deer       0.84      0.86      0.85      1000
         dog       0.85      0.76      0.80      1000
        frog       0.93      0.87      0.90      1000
       horse       0.90      0.89      0.90      1000
        ship       0.90      0.94      0.92      1000
       truck       0.92      0.91      0.92      1000

    accuracy                           0.86     10000
   macro avg       0.86      0.86      0.86     10000
weighted avg       0.86      0.86      0.86     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.8434
--
confusion matrix
[[406   6  25   0   8   2   1   2  27  11]
 [  2 483   1   0   1   0   0   0   8  17]
 [ 22   5 432  18  19  11  11   6   5   3]
 [  7   3  23 344  20  46   7  12   6   3]
 [  3   1  13  24 396  10   6  14   1   3]
 [  1   4  15  80  18 371   7  17   1   0]
 [  1   6  19  15  14   4 439   2   6   1]
 [  4   1  15   9  15  19   0 432   1   4]
 [ 23   6   1   1   1   1   0   1 466   4]
 [  9  24   1   3   1   1   1   2  11 448]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.85      0.83      0.84       488
  automobile       0.90      0.94      0.92       512
        bird       0.79      0.81      0.80       532
         cat       0.70      0.73      0.71       471
        deer       0.80      0.84      0.82       471
         dog       0.80      0.72      0.76       514
        frog       0.93      0.87      0.90       507
       horse       0.89      0.86      0.87       500
        ship       0.88      0.92      0.90       504
       truck       0.91      0.89      0.90       501

    accuracy                           0.84      5000
   macro avg       0.84      0.84      0.84      5000
weighted avg       0.84      0.84      0.84      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423875: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:21:53 2024
Job was executed on host(s) <hgn48>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:22:12 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:22:12 2024
Terminated at Wed Feb 28 08:35:38 2024
Results reported at Wed Feb 28 08:35:38 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_N1_aug_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_N1_aug_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2874.00 sec.
    Max Memory :                                 3571 MB
    Average Memory :                             3259.38 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6669.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   806 sec.
    Turnaround time :                            825 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_N1_aug_err_423875> for stderr output of this job.

