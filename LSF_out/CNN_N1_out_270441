loading ...
loaded conda.sh
sh shell detected
main => start
 
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Train/Validation random split => start
 
DataLoader => start
 
To_device => start
 
Train => start
 
Epoch [0], train_loss: 1.6648, train_acc: 0.3761, val_loss: 1.2599, val_acc: 0.5333
Epoch [1], train_loss: 1.0659, train_acc: 0.6158, val_loss: 0.9072, val_acc: 0.6717
Epoch [2], train_loss: 0.8068, train_acc: 0.7160, val_loss: 0.7474, val_acc: 0.7328
Epoch [3], train_loss: 0.6439, train_acc: 0.7751, val_loss: 0.6967, val_acc: 0.7540
Epoch [4], train_loss: 0.5044, train_acc: 0.8235, val_loss: 0.7346, val_acc: 0.7520
Epoch [5], train_loss: 0.4021, train_acc: 0.8591, val_loss: 0.7290, val_acc: 0.7595
Epoch [6], train_loss: 0.3107, train_acc: 0.8920, val_loss: 0.6996, val_acc: 0.7760
Epoch [7], train_loss: 0.2229, train_acc: 0.9210, val_loss: 0.8980, val_acc: 0.7683
Epoch [8], train_loss: 0.1787, train_acc: 0.9366, val_loss: 0.9057, val_acc: 0.7706
Epoch [9], train_loss: 0.1371, train_acc: 0.9519, val_loss: 0.9479, val_acc: 0.7778
Epoch [10], train_loss: 0.1096, train_acc: 0.9633, val_loss: 0.9601, val_acc: 0.7823
Epoch [11], train_loss: 0.0995, train_acc: 0.9660, val_loss: 1.0562, val_acc: 0.7756
Epoch [12], train_loss: 0.0868, train_acc: 0.9715, val_loss: 1.1725, val_acc: 0.7665
Epoch [13], train_loss: 0.0835, train_acc: 0.9726, val_loss: 1.2109, val_acc: 0.7795
Epoch [14], train_loss: 0.0779, train_acc: 0.9747, val_loss: 1.1907, val_acc: 0.7731
Epoch [15], train_loss: 0.0676, train_acc: 0.9767, val_loss: 1.2067, val_acc: 0.7732
Epoch [16], train_loss: 0.0725, train_acc: 0.9770, val_loss: 1.3195, val_acc: 0.7779
Epoch [17], train_loss: 0.0686, train_acc: 0.9773, val_loss: 1.2639, val_acc: 0.7661
Epoch [18], train_loss: 0.0723, train_acc: 0.9765, val_loss: 1.1824, val_acc: 0.7779
Epoch [19], train_loss: 0.0542, train_acc: 0.9819, val_loss: 1.3217, val_acc: 0.7766
Epoch [20], train_loss: 0.0580, train_acc: 0.9813, val_loss: 1.2920, val_acc: 0.7797
Epoch [21], train_loss: 0.0626, train_acc: 0.9797, val_loss: 1.3580, val_acc: 0.7768
Epoch [22], train_loss: 0.0542, train_acc: 0.9823, val_loss: 1.2145, val_acc: 0.7856
Epoch [23], train_loss: 0.0510, train_acc: 0.9835, val_loss: 1.3694, val_acc: 0.7775
Epoch [24], train_loss: 0.0517, train_acc: 0.9837, val_loss: 1.4376, val_acc: 0.7803
Epoch [25], train_loss: 0.0505, train_acc: 0.9834, val_loss: 1.3139, val_acc: 0.7834
Epoch [26], train_loss: 0.0571, train_acc: 0.9818, val_loss: 1.4572, val_acc: 0.7815
Epoch [27], train_loss: 0.0594, train_acc: 0.9813, val_loss: 1.3221, val_acc: 0.7801
Epoch [28], train_loss: 0.0478, train_acc: 0.9855, val_loss: 1.3734, val_acc: 0.7777
Epoch [29], train_loss: 0.0539, train_acc: 0.9824, val_loss: 1.3017, val_acc: 0.7766
Epoch [30], train_loss: 0.0408, train_acc: 0.9871, val_loss: 1.6008, val_acc: 0.7726
Epoch [31], train_loss: 0.0544, train_acc: 0.9828, val_loss: 1.3868, val_acc: 0.7780
Epoch [32], train_loss: 0.0413, train_acc: 0.9871, val_loss: 1.5335, val_acc: 0.7855
Epoch [33], train_loss: 0.0433, train_acc: 0.9869, val_loss: 1.6652, val_acc: 0.7743
Epoch [34], train_loss: 0.0502, train_acc: 0.9836, val_loss: 1.5915, val_acc: 0.7700
Epoch [35], train_loss: 0.0491, train_acc: 0.9856, val_loss: 1.5257, val_acc: 0.7764
Epoch [36], train_loss: 0.0382, train_acc: 0.9878, val_loss: 1.5937, val_acc: 0.7823
Epoch [37], train_loss: 0.0449, train_acc: 0.9871, val_loss: 1.5766, val_acc: 0.7811
Epoch [38], train_loss: 0.0544, train_acc: 0.9835, val_loss: 1.3828, val_acc: 0.7715
Epoch [39], train_loss: 0.0480, train_acc: 0.9852, val_loss: 1.5208, val_acc: 0.7762
Epoch [40], train_loss: 0.0450, train_acc: 0.9866, val_loss: 1.6611, val_acc: 0.7641
Epoch [41], train_loss: 0.0420, train_acc: 0.9874, val_loss: 1.7109, val_acc: 0.7797
Epoch [42], train_loss: 0.0503, train_acc: 0.9855, val_loss: 1.5769, val_acc: 0.7731
Epoch [43], train_loss: 0.0362, train_acc: 0.9887, val_loss: 1.5272, val_acc: 0.7721
Epoch [44], train_loss: 0.0512, train_acc: 0.9850, val_loss: 1.5466, val_acc: 0.7732
Epoch [45], train_loss: 0.0393, train_acc: 0.9883, val_loss: 1.6666, val_acc: 0.7770
Epoch [46], train_loss: 0.0439, train_acc: 0.9869, val_loss: 1.5532, val_acc: 0.7679
Epoch [47], train_loss: 0.0424, train_acc: 0.9869, val_loss: 1.7528, val_acc: 0.7811
Epoch [48], train_loss: 0.0440, train_acc: 0.9870, val_loss: 1.5964, val_acc: 0.7803
Epoch [49], train_loss: 0.0411, train_acc: 0.9874, val_loss: 1.7207, val_acc: 0.7729
Epoch [50], train_loss: 0.0433, train_acc: 0.9877, val_loss: 1.7200, val_acc: 0.7710
Epoch [51], train_loss: 0.0409, train_acc: 0.9880, val_loss: 1.6980, val_acc: 0.7722
Epoch [52], train_loss: 0.0474, train_acc: 0.9871, val_loss: 1.6249, val_acc: 0.7614
Epoch [53], train_loss: 0.0367, train_acc: 0.9896, val_loss: 1.8522, val_acc: 0.7670
Epoch [54], train_loss: 0.0347, train_acc: 0.9893, val_loss: 1.8238, val_acc: 0.7745
Epoch [55], train_loss: 0.0445, train_acc: 0.9876, val_loss: 1.6861, val_acc: 0.7718
Epoch [56], train_loss: 0.0444, train_acc: 0.9871, val_loss: 1.7000, val_acc: 0.7755
Epoch [57], train_loss: 0.0372, train_acc: 0.9889, val_loss: 1.7101, val_acc: 0.7825
Epoch [58], train_loss: 0.0462, train_acc: 0.9875, val_loss: 1.8294, val_acc: 0.7730
Epoch [59], train_loss: 0.0373, train_acc: 0.9890, val_loss: 1.7862, val_acc: 0.7809
Epoch [60], train_loss: 0.0391, train_acc: 0.9884, val_loss: 1.9057, val_acc: 0.7792
Epoch [61], train_loss: 0.0479, train_acc: 0.9872, val_loss: 1.8710, val_acc: 0.7691
Epoch [62], train_loss: 0.0431, train_acc: 0.9881, val_loss: 1.9257, val_acc: 0.7763
Epoch [63], train_loss: 0.0364, train_acc: 0.9899, val_loss: 1.9008, val_acc: 0.7633
Epoch [64], train_loss: 0.0414, train_acc: 0.9887, val_loss: 1.7710, val_acc: 0.7789
Epoch [65], train_loss: 0.0414, train_acc: 0.9887, val_loss: 1.8143, val_acc: 0.7712
Epoch [66], train_loss: 0.0396, train_acc: 0.9892, val_loss: 1.6810, val_acc: 0.7675
Epoch [67], train_loss: 0.0372, train_acc: 0.9897, val_loss: 1.8847, val_acc: 0.7773
Epoch [68], train_loss: 0.0368, train_acc: 0.9899, val_loss: 1.9727, val_acc: 0.7770
Epoch [69], train_loss: 0.0371, train_acc: 0.9901, val_loss: 1.7646, val_acc: 0.7826
Epoch [70], train_loss: 0.0395, train_acc: 0.9893, val_loss: 1.8353, val_acc: 0.7840
Epoch [71], train_loss: 0.0402, train_acc: 0.9895, val_loss: 1.5327, val_acc: 0.7860
Epoch [72], train_loss: 0.0383, train_acc: 0.9894, val_loss: 1.6575, val_acc: 0.7835
Epoch [73], train_loss: 0.0345, train_acc: 0.9904, val_loss: 1.7807, val_acc: 0.7888
Epoch [74], train_loss: 0.0247, train_acc: 0.9928, val_loss: 1.8762, val_acc: 0.7880
Epoch [75], train_loss: 0.0442, train_acc: 0.9874, val_loss: 2.0113, val_acc: 0.7727
Epoch [76], train_loss: 0.0391, train_acc: 0.9892, val_loss: 1.8797, val_acc: 0.7778
Epoch [77], train_loss: 0.0364, train_acc: 0.9899, val_loss: 1.9357, val_acc: 0.7718
Epoch [78], train_loss: 0.0475, train_acc: 0.9881, val_loss: 1.7032, val_acc: 0.7740
Epoch [79], train_loss: 0.0310, train_acc: 0.9917, val_loss: 1.7527, val_acc: 0.7808
Epoch [80], train_loss: 0.0395, train_acc: 0.9896, val_loss: 1.7614, val_acc: 0.7778
Epoch [81], train_loss: 0.0302, train_acc: 0.9924, val_loss: 1.7673, val_acc: 0.7854
Epoch [82], train_loss: 0.0370, train_acc: 0.9904, val_loss: 1.8311, val_acc: 0.7788
Epoch [83], train_loss: 0.0342, train_acc: 0.9907, val_loss: 1.9187, val_acc: 0.7774
Epoch [84], train_loss: 0.0432, train_acc: 0.9890, val_loss: 1.7803, val_acc: 0.7658
Epoch [85], train_loss: 0.0313, train_acc: 0.9909, val_loss: 1.9623, val_acc: 0.7808
Epoch [86], train_loss: 0.0441, train_acc: 0.9890, val_loss: 2.0968, val_acc: 0.7599
Epoch [87], train_loss: 0.0389, train_acc: 0.9894, val_loss: 1.8883, val_acc: 0.7732
Epoch [88], train_loss: 0.0283, train_acc: 0.9925, val_loss: 2.0631, val_acc: 0.7776
Epoch [89], train_loss: 0.0374, train_acc: 0.9906, val_loss: 1.7360, val_acc: 0.7743
Epoch [90], train_loss: 0.0235, train_acc: 0.9937, val_loss: 1.9537, val_acc: 0.7722
Epoch [91], train_loss: 0.0494, train_acc: 0.9878, val_loss: 1.8632, val_acc: 0.7657
Epoch [92], train_loss: 0.0369, train_acc: 0.9901, val_loss: 1.9368, val_acc: 0.7713
Epoch [93], train_loss: 0.0287, train_acc: 0.9924, val_loss: 1.9543, val_acc: 0.7784
Epoch [94], train_loss: 0.0221, train_acc: 0.9945, val_loss: 1.9755, val_acc: 0.7682
Epoch [95], train_loss: 0.0358, train_acc: 0.9899, val_loss: 1.8536, val_acc: 0.7787
Epoch [96], train_loss: 0.0292, train_acc: 0.9920, val_loss: 1.9796, val_acc: 0.7774
Epoch [97], train_loss: 0.0409, train_acc: 0.9904, val_loss: 1.8466, val_acc: 0.7731
Epoch [98], train_loss: 0.0352, train_acc: 0.9915, val_loss: 1.8548, val_acc: 0.7757
Epoch [99], train_loss: 0.0361, train_acc: 0.9908, val_loss: 1.9048, val_acc: 0.7743
 
Visualize trining => save images
 
Load the model => start
 
Check best/last models => start
 
Summary result of test set => best model  {'val_loss': 0.7048559188842773, 'val_acc': 0.7603515386581421}
Summary result of test set => last model {'val_loss': 1.9254333972930908, 'val_acc': 0.7723633050918579}
Test set evaluation => save results for postprocessing
 
** accuracy: 0.761
--
confusion matrix
[[800  19  32  13  20   5   6  14  46  45]
 [  9 879   0   0   0   2   4   1  32  73]
 [ 70   4 520  44 157  85  73  29  11   7]
 [ 19   8  27 514  94 206  62  38  16  16]
 [  9   3  18  30 803  20  26  86   4   1]
 [  8   1  16 109  61 720  19  53   6   7]
 [  4   4  15  32  81  23 812   8  15   6]
 [  6   2  16  23  45  61   6 825   3  13]
 [ 64  11   5   9   8   6   6   3 865  23]
 [ 24  55   5   3   5   2   4  11  23 868]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.79      0.80      0.79      1000
  automobile       0.89      0.88      0.89      1000
        bird       0.80      0.52      0.63      1000
         cat       0.66      0.51      0.58      1000
        deer       0.63      0.80      0.71      1000
         dog       0.64      0.72      0.68      1000
        frog       0.80      0.81      0.80      1000
       horse       0.77      0.82      0.80      1000
        ship       0.85      0.86      0.86      1000
       truck       0.82      0.87      0.84      1000

    accuracy                           0.76     10000
   macro avg       0.76      0.76      0.76     10000
weighted avg       0.76      0.76      0.76     10000

END OF CODE
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 270441: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <access4> by user <ingap> in cluster <wexac> at Tue Feb 27 11:08:08 2024
Job was executed on host(s) <hgn41>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 11:08:37 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 11:08:37 2024
Terminated at Tue Feb 27 11:18:18 2024
Results reported at Tue Feb 27 11:18:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_N1_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_N1_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2086.00 sec.
    Max Memory :                                 3514 MB
    Average Memory :                             3111.43 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6726.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                75
    Run time :                                   582 sec.
    Turnaround time :                            610 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_N1_err_270441> for stderr output of this job.

