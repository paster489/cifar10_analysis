loading ...
loaded conda.sh
sh shell detected
main => start
 
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Train/Validation random split => start
 
DataLoader => start
 
To_device => start
 
Train => start
 
Epoch [0], train_loss: 1.9178, train_acc: 0.2727, val_loss: 1.6107, val_acc: 0.3922
Epoch [1], train_loss: 1.4262, train_acc: 0.4691, val_loss: 1.2940, val_acc: 0.5254
Epoch [2], train_loss: 1.1929, train_acc: 0.5654, val_loss: 1.1393, val_acc: 0.5947
Epoch [3], train_loss: 1.0358, train_acc: 0.6273, val_loss: 1.0106, val_acc: 0.6410
Epoch [4], train_loss: 0.9251, train_acc: 0.6752, val_loss: 0.9513, val_acc: 0.6701
Epoch [5], train_loss: 0.8436, train_acc: 0.7032, val_loss: 0.8227, val_acc: 0.7091
Epoch [6], train_loss: 0.7635, train_acc: 0.7303, val_loss: 0.7598, val_acc: 0.7313
Epoch [7], train_loss: 0.7024, train_acc: 0.7521, val_loss: 0.7417, val_acc: 0.7314
Epoch [8], train_loss: 0.6554, train_acc: 0.7703, val_loss: 0.6970, val_acc: 0.7619
Epoch [9], train_loss: 0.6130, train_acc: 0.7859, val_loss: 0.6806, val_acc: 0.7591
Epoch [10], train_loss: 0.5777, train_acc: 0.7974, val_loss: 0.6424, val_acc: 0.7726
Epoch [11], train_loss: 0.5562, train_acc: 0.8047, val_loss: 0.6123, val_acc: 0.7835
Epoch [12], train_loss: 0.5196, train_acc: 0.8178, val_loss: 0.6442, val_acc: 0.7782
Epoch [13], train_loss: 0.5057, train_acc: 0.8238, val_loss: 0.5747, val_acc: 0.8017
Epoch [14], train_loss: 0.4827, train_acc: 0.8306, val_loss: 0.5868, val_acc: 0.7916
Epoch [15], train_loss: 0.4597, train_acc: 0.8382, val_loss: 0.5717, val_acc: 0.8039
Epoch [16], train_loss: 0.4415, train_acc: 0.8459, val_loss: 0.5482, val_acc: 0.8117
Epoch [17], train_loss: 0.4243, train_acc: 0.8506, val_loss: 0.5411, val_acc: 0.8191
Epoch [18], train_loss: 0.4164, train_acc: 0.8542, val_loss: 0.5872, val_acc: 0.7969
Epoch [19], train_loss: 0.3975, train_acc: 0.8602, val_loss: 0.5577, val_acc: 0.8149
Epoch [20], train_loss: 0.3819, train_acc: 0.8664, val_loss: 0.5462, val_acc: 0.8123
Epoch [21], train_loss: 0.3731, train_acc: 0.8687, val_loss: 0.5268, val_acc: 0.8152
Epoch [22], train_loss: 0.3630, train_acc: 0.8726, val_loss: 0.5396, val_acc: 0.8162
Epoch [23], train_loss: 0.3495, train_acc: 0.8784, val_loss: 0.5473, val_acc: 0.8207
Epoch [24], train_loss: 0.3422, train_acc: 0.8798, val_loss: 0.6002, val_acc: 0.8066
Epoch [25], train_loss: 0.3309, train_acc: 0.8841, val_loss: 0.5481, val_acc: 0.8241
Epoch [26], train_loss: 0.3206, train_acc: 0.8891, val_loss: 0.5199, val_acc: 0.8275
Epoch [27], train_loss: 0.3190, train_acc: 0.8852, val_loss: 0.5612, val_acc: 0.8238
Epoch [28], train_loss: 0.3084, train_acc: 0.8912, val_loss: 0.5602, val_acc: 0.8092
Epoch [29], train_loss: 0.2980, train_acc: 0.8943, val_loss: 0.5543, val_acc: 0.8233
Epoch [30], train_loss: 0.2971, train_acc: 0.8945, val_loss: 0.5764, val_acc: 0.8248
Epoch [31], train_loss: 0.2891, train_acc: 0.8987, val_loss: 0.5434, val_acc: 0.8325
Epoch [32], train_loss: 0.2773, train_acc: 0.9024, val_loss: 0.5589, val_acc: 0.8214
Epoch [33], train_loss: 0.2749, train_acc: 0.9013, val_loss: 0.5630, val_acc: 0.8171
Epoch [34], train_loss: 0.2654, train_acc: 0.9062, val_loss: 0.5680, val_acc: 0.8310
Epoch [35], train_loss: 0.2646, train_acc: 0.9068, val_loss: 0.5610, val_acc: 0.8216
Epoch [36], train_loss: 0.2544, train_acc: 0.9090, val_loss: 0.6003, val_acc: 0.8241
Epoch [37], train_loss: 0.2472, train_acc: 0.9129, val_loss: 0.5544, val_acc: 0.8315
Epoch [38], train_loss: 0.2443, train_acc: 0.9144, val_loss: 0.6252, val_acc: 0.8143
Epoch [39], train_loss: 0.2453, train_acc: 0.9121, val_loss: 0.5595, val_acc: 0.8303
Epoch [40], train_loss: 0.2356, train_acc: 0.9168, val_loss: 0.6274, val_acc: 0.8237
Epoch [41], train_loss: 0.2320, train_acc: 0.9173, val_loss: 0.6007, val_acc: 0.8253
Epoch [42], train_loss: 0.2268, train_acc: 0.9203, val_loss: 0.5952, val_acc: 0.8303
Epoch [43], train_loss: 0.2229, train_acc: 0.9203, val_loss: 0.6299, val_acc: 0.8218
Epoch [44], train_loss: 0.2237, train_acc: 0.9206, val_loss: 0.6147, val_acc: 0.8285
Epoch [45], train_loss: 0.2205, train_acc: 0.9224, val_loss: 0.5728, val_acc: 0.8364
Epoch [46], train_loss: 0.2178, train_acc: 0.9233, val_loss: 0.6055, val_acc: 0.8222
Epoch [47], train_loss: 0.2146, train_acc: 0.9243, val_loss: 0.6246, val_acc: 0.8258
Epoch [48], train_loss: 0.2018, train_acc: 0.9294, val_loss: 0.6273, val_acc: 0.8339
Epoch [49], train_loss: 0.2107, train_acc: 0.9257, val_loss: 0.5855, val_acc: 0.8317
Epoch [50], train_loss: 0.2074, train_acc: 0.9275, val_loss: 0.5922, val_acc: 0.8376
Epoch [51], train_loss: 0.2014, train_acc: 0.9289, val_loss: 0.6012, val_acc: 0.8363
Epoch [52], train_loss: 0.2002, train_acc: 0.9292, val_loss: 0.6730, val_acc: 0.8203
Epoch [53], train_loss: 0.1948, train_acc: 0.9308, val_loss: 0.5907, val_acc: 0.8356
Epoch [54], train_loss: 0.2001, train_acc: 0.9300, val_loss: 0.6090, val_acc: 0.8403
Epoch [55], train_loss: 0.1966, train_acc: 0.9313, val_loss: 0.5906, val_acc: 0.8384
Epoch [56], train_loss: 0.1921, train_acc: 0.9327, val_loss: 0.5931, val_acc: 0.8423
Epoch [57], train_loss: 0.1880, train_acc: 0.9337, val_loss: 0.6052, val_acc: 0.8347
Epoch [58], train_loss: 0.1773, train_acc: 0.9386, val_loss: 0.6540, val_acc: 0.8336
Epoch [59], train_loss: 0.1880, train_acc: 0.9342, val_loss: 0.6155, val_acc: 0.8323
Epoch [60], train_loss: 0.1849, train_acc: 0.9355, val_loss: 0.6481, val_acc: 0.8246
Epoch [61], train_loss: 0.1766, train_acc: 0.9385, val_loss: 0.6248, val_acc: 0.8333
Epoch [62], train_loss: 0.1778, train_acc: 0.9378, val_loss: 0.6870, val_acc: 0.8289
Epoch [63], train_loss: 0.1767, train_acc: 0.9378, val_loss: 0.6359, val_acc: 0.8326
Epoch [64], train_loss: 0.1793, train_acc: 0.9366, val_loss: 0.6271, val_acc: 0.8370
Epoch [65], train_loss: 0.1682, train_acc: 0.9402, val_loss: 0.6578, val_acc: 0.8370
Epoch [66], train_loss: 0.1772, train_acc: 0.9378, val_loss: 0.6226, val_acc: 0.8352
Epoch [67], train_loss: 0.1761, train_acc: 0.9403, val_loss: 0.6445, val_acc: 0.8325
Epoch [68], train_loss: 0.1641, train_acc: 0.9436, val_loss: 0.6647, val_acc: 0.8207
Epoch [69], train_loss: 0.1685, train_acc: 0.9414, val_loss: 0.6756, val_acc: 0.8319
Epoch [70], train_loss: 0.1684, train_acc: 0.9414, val_loss: 0.6567, val_acc: 0.8310
Epoch [71], train_loss: 0.1604, train_acc: 0.9452, val_loss: 0.6296, val_acc: 0.8391
Epoch [72], train_loss: 0.1631, train_acc: 0.9442, val_loss: 0.6605, val_acc: 0.8359
Epoch [73], train_loss: 0.1614, train_acc: 0.9448, val_loss: 0.6241, val_acc: 0.8350
Epoch [74], train_loss: 0.1600, train_acc: 0.9455, val_loss: 0.5995, val_acc: 0.8445
Epoch [75], train_loss: 0.1651, train_acc: 0.9430, val_loss: 0.6466, val_acc: 0.8312
Epoch [76], train_loss: 0.1532, train_acc: 0.9471, val_loss: 0.7330, val_acc: 0.8266
Epoch [77], train_loss: 0.1622, train_acc: 0.9438, val_loss: 0.6610, val_acc: 0.8393
Epoch [78], train_loss: 0.1570, train_acc: 0.9479, val_loss: 0.6275, val_acc: 0.8417
Epoch [79], train_loss: 0.1528, train_acc: 0.9476, val_loss: 0.6847, val_acc: 0.8345
Epoch [80], train_loss: 0.1493, train_acc: 0.9491, val_loss: 0.6261, val_acc: 0.8450
Epoch [81], train_loss: 0.1612, train_acc: 0.9442, val_loss: 0.6604, val_acc: 0.8343
Epoch [82], train_loss: 0.1575, train_acc: 0.9454, val_loss: 0.6985, val_acc: 0.8314
Epoch [83], train_loss: 0.1547, train_acc: 0.9457, val_loss: 0.6479, val_acc: 0.8393
Epoch [84], train_loss: 0.1463, train_acc: 0.9496, val_loss: 0.6970, val_acc: 0.8346
Epoch [85], train_loss: 0.1471, train_acc: 0.9499, val_loss: 0.6666, val_acc: 0.8429
Epoch [86], train_loss: 0.1425, train_acc: 0.9510, val_loss: 0.7153, val_acc: 0.8438
Epoch [87], train_loss: 0.1488, train_acc: 0.9488, val_loss: 0.6762, val_acc: 0.8383
Epoch [88], train_loss: 0.1484, train_acc: 0.9487, val_loss: 0.6903, val_acc: 0.8354
Epoch [89], train_loss: 0.1464, train_acc: 0.9504, val_loss: 0.6601, val_acc: 0.8365
Epoch [90], train_loss: 0.1396, train_acc: 0.9508, val_loss: 0.6919, val_acc: 0.8307
Epoch [91], train_loss: 0.1472, train_acc: 0.9501, val_loss: 0.7664, val_acc: 0.8359
Epoch [92], train_loss: 0.1428, train_acc: 0.9497, val_loss: 0.6953, val_acc: 0.8344
Epoch [93], train_loss: 0.1414, train_acc: 0.9518, val_loss: 0.7027, val_acc: 0.8327
Epoch [94], train_loss: 0.1421, train_acc: 0.9523, val_loss: 0.6651, val_acc: 0.8293
Epoch [95], train_loss: 0.1454, train_acc: 0.9505, val_loss: 0.7554, val_acc: 0.8356
Epoch [96], train_loss: 0.1406, train_acc: 0.9508, val_loss: 0.7562, val_acc: 0.8318
Epoch [97], train_loss: 0.1367, train_acc: 0.9539, val_loss: 0.7361, val_acc: 0.8294
Epoch [98], train_loss: 0.1419, train_acc: 0.9513, val_loss: 0.6968, val_acc: 0.8334
Epoch [99], train_loss: 0.1396, train_acc: 0.9522, val_loss: 0.7461, val_acc: 0.8306
 
Visualize trining => save images
 
Load the model => start
 
Check best/last models => start
 
Summary result of test set => best model  {'val_loss': 0.5368286967277527, 'val_acc': 0.8331054449081421}
Summary result of test set => last model {'val_loss': 0.7699605822563171, 'val_acc': 0.8321288824081421}
Test set evaluation => save results for postprocessing
 
** accuracy: 0.832
--
confusion matrix
[[856   7  51   8   8   4   6   8  34  18]
 [ 16 913   2   1   0   2   2   1  14  49]
 [ 36   0 744  21  52  58  53  23   6   7]
 [ 25   0  38 589  52 175  50  40  17  14]
 [ 10   1  34  22 823  36  28  40   4   2]
 [  3   0  14  71  28 837   8  29   4   6]
 [  9   1  24  41  19  20 872   7   5   2]
 [ 19   1  16  27  27  23   3 876   2   6]
 [ 59  10   3   2   2   0   1   4 902  17]
 [ 30  41   2   3   0   0   1   1  13 909]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.81      0.86      0.83      1000
  automobile       0.94      0.91      0.93      1000
        bird       0.80      0.74      0.77      1000
         cat       0.75      0.59      0.66      1000
        deer       0.81      0.82      0.82      1000
         dog       0.72      0.84      0.78      1000
        frog       0.85      0.87      0.86      1000
       horse       0.85      0.88      0.86      1000
        ship       0.90      0.90      0.90      1000
       truck       0.88      0.91      0.90      1000

    accuracy                           0.83     10000
   macro avg       0.83      0.83      0.83     10000
weighted avg       0.83      0.83      0.83     10000

END OF CODE
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 270455: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <access4> by user <ingap> in cluster <wexac> at Tue Feb 27 11:09:09 2024
Job was executed on host(s) <hgn41>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 11:09:48 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 11:09:48 2024
Terminated at Tue Feb 27 11:22:08 2024
Results reported at Tue Feb 27 11:22:08 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_N2_aug_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_N2_aug_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2634.00 sec.
    Max Memory :                                 3572 MB
    Average Memory :                             3278.03 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6668.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                85
    Run time :                                   747 sec.
    Turnaround time :                            779 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_N2_aug_err_270455> for stderr output of this job.

