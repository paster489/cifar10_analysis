loading ...
loaded conda.sh
sh shell detected
main => start
 
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Train/Validation random split => start
 
DataLoader => start
 
To_device => start
 
Train => start
 
Epoch [0], train_loss: 1.9014, train_acc: 0.2822, val_loss: 1.5055, val_acc: 0.4414
Epoch [1], train_loss: 1.3065, train_acc: 0.5222, val_loss: 1.1419, val_acc: 0.5794
Epoch [2], train_loss: 1.0395, train_acc: 0.6266, val_loss: 0.9641, val_acc: 0.6569
Epoch [3], train_loss: 0.8718, train_acc: 0.6914, val_loss: 0.8501, val_acc: 0.6942
Epoch [4], train_loss: 0.7342, train_acc: 0.7426, val_loss: 0.7813, val_acc: 0.7202
Epoch [5], train_loss: 0.6327, train_acc: 0.7776, val_loss: 0.7778, val_acc: 0.7329
Epoch [6], train_loss: 0.5313, train_acc: 0.8134, val_loss: 0.7394, val_acc: 0.7406
Epoch [7], train_loss: 0.4391, train_acc: 0.8468, val_loss: 0.7517, val_acc: 0.7530
Epoch [8], train_loss: 0.3586, train_acc: 0.8741, val_loss: 0.7816, val_acc: 0.7505
Epoch [9], train_loss: 0.2897, train_acc: 0.8969, val_loss: 0.8338, val_acc: 0.7491
Epoch [10], train_loss: 0.2358, train_acc: 0.9163, val_loss: 0.8905, val_acc: 0.7635
Epoch [11], train_loss: 0.1908, train_acc: 0.9333, val_loss: 0.9769, val_acc: 0.7329
Epoch [12], train_loss: 0.1469, train_acc: 0.9482, val_loss: 1.1235, val_acc: 0.7449
Epoch [13], train_loss: 0.1234, train_acc: 0.9574, val_loss: 1.1537, val_acc: 0.7442
Epoch [14], train_loss: 0.1172, train_acc: 0.9597, val_loss: 1.2642, val_acc: 0.7419
Epoch [15], train_loss: 0.1003, train_acc: 0.9648, val_loss: 1.2858, val_acc: 0.7528
Epoch [16], train_loss: 0.0918, train_acc: 0.9690, val_loss: 1.3626, val_acc: 0.7497
Epoch [17], train_loss: 0.0871, train_acc: 0.9699, val_loss: 1.4362, val_acc: 0.7538
Epoch [18], train_loss: 0.0742, train_acc: 0.9744, val_loss: 1.4057, val_acc: 0.7491
Epoch [19], train_loss: 0.0779, train_acc: 0.9740, val_loss: 1.3245, val_acc: 0.7518
Epoch [20], train_loss: 0.0703, train_acc: 0.9768, val_loss: 1.5183, val_acc: 0.7395
Epoch [21], train_loss: 0.0795, train_acc: 0.9730, val_loss: 1.4048, val_acc: 0.7406
Epoch [22], train_loss: 0.0625, train_acc: 0.9791, val_loss: 1.5259, val_acc: 0.7588
Epoch [23], train_loss: 0.0672, train_acc: 0.9767, val_loss: 1.5374, val_acc: 0.7577
Epoch [24], train_loss: 0.0615, train_acc: 0.9797, val_loss: 1.5028, val_acc: 0.7506
Epoch [25], train_loss: 0.0613, train_acc: 0.9782, val_loss: 1.6898, val_acc: 0.7354
Epoch [26], train_loss: 0.0621, train_acc: 0.9799, val_loss: 1.5307, val_acc: 0.7496
Epoch [27], train_loss: 0.0506, train_acc: 0.9837, val_loss: 1.7198, val_acc: 0.7488
Epoch [28], train_loss: 0.0506, train_acc: 0.9836, val_loss: 1.6820, val_acc: 0.7453
Epoch [29], train_loss: 0.0671, train_acc: 0.9778, val_loss: 1.6570, val_acc: 0.7384
Epoch [30], train_loss: 0.0539, train_acc: 0.9824, val_loss: 1.6581, val_acc: 0.7411
Epoch [31], train_loss: 0.0583, train_acc: 0.9808, val_loss: 1.6791, val_acc: 0.7476
Epoch [32], train_loss: 0.0503, train_acc: 0.9836, val_loss: 1.7568, val_acc: 0.7385
Epoch [33], train_loss: 0.0580, train_acc: 0.9818, val_loss: 1.6392, val_acc: 0.7482
Epoch [34], train_loss: 0.0507, train_acc: 0.9832, val_loss: 1.6527, val_acc: 0.7452
Epoch [35], train_loss: 0.0403, train_acc: 0.9864, val_loss: 1.8888, val_acc: 0.7527
Epoch [36], train_loss: 0.0552, train_acc: 0.9828, val_loss: 1.6128, val_acc: 0.7520
Epoch [37], train_loss: 0.0511, train_acc: 0.9836, val_loss: 1.6603, val_acc: 0.7461
Epoch [38], train_loss: 0.0404, train_acc: 0.9869, val_loss: 1.6984, val_acc: 0.7458
Epoch [39], train_loss: 0.0582, train_acc: 0.9820, val_loss: 1.7220, val_acc: 0.7552
Epoch [40], train_loss: 0.0462, train_acc: 0.9852, val_loss: 1.6362, val_acc: 0.7547
Epoch [41], train_loss: 0.0496, train_acc: 0.9845, val_loss: 1.5871, val_acc: 0.7564
Epoch [42], train_loss: 0.0425, train_acc: 0.9861, val_loss: 1.9884, val_acc: 0.7462
Epoch [43], train_loss: 0.0473, train_acc: 0.9849, val_loss: 1.7546, val_acc: 0.7342
Epoch [44], train_loss: 0.0434, train_acc: 0.9862, val_loss: 1.8184, val_acc: 0.7457
Epoch [45], train_loss: 0.0494, train_acc: 0.9845, val_loss: 1.8257, val_acc: 0.7500
Epoch [46], train_loss: 0.0461, train_acc: 0.9857, val_loss: 1.7906, val_acc: 0.7523
Epoch [47], train_loss: 0.0395, train_acc: 0.9874, val_loss: 1.9330, val_acc: 0.7437
Epoch [48], train_loss: 0.0468, train_acc: 0.9849, val_loss: 1.8441, val_acc: 0.7514
Epoch [49], train_loss: 0.0420, train_acc: 0.9865, val_loss: 1.9417, val_acc: 0.7501
Epoch [50], train_loss: 0.0432, train_acc: 0.9866, val_loss: 1.8292, val_acc: 0.7475
Epoch [51], train_loss: 0.0418, train_acc: 0.9866, val_loss: 1.8635, val_acc: 0.7676
Epoch [52], train_loss: 0.0475, train_acc: 0.9847, val_loss: 2.0484, val_acc: 0.7459
Epoch [53], train_loss: 0.0443, train_acc: 0.9865, val_loss: 1.9224, val_acc: 0.7624
Epoch [54], train_loss: 0.0457, train_acc: 0.9858, val_loss: 1.8616, val_acc: 0.7590
Epoch [55], train_loss: 0.0342, train_acc: 0.9895, val_loss: 1.9480, val_acc: 0.7547
Epoch [56], train_loss: 0.0567, train_acc: 0.9830, val_loss: 1.8705, val_acc: 0.7510
Epoch [57], train_loss: 0.0335, train_acc: 0.9894, val_loss: 1.8625, val_acc: 0.7546
Epoch [58], train_loss: 0.0378, train_acc: 0.9882, val_loss: 2.1079, val_acc: 0.7419
Epoch [59], train_loss: 0.0530, train_acc: 0.9842, val_loss: 1.8984, val_acc: 0.7516
Epoch [60], train_loss: 0.0465, train_acc: 0.9851, val_loss: 1.9573, val_acc: 0.7449
Epoch [61], train_loss: 0.0430, train_acc: 0.9869, val_loss: 1.9100, val_acc: 0.7507
Epoch [62], train_loss: 0.0355, train_acc: 0.9892, val_loss: 1.9629, val_acc: 0.7558
Epoch [63], train_loss: 0.0330, train_acc: 0.9901, val_loss: 2.1761, val_acc: 0.7484
Epoch [64], train_loss: 0.0444, train_acc: 0.9872, val_loss: 2.0196, val_acc: 0.7575
Epoch [65], train_loss: 0.0326, train_acc: 0.9907, val_loss: 1.9899, val_acc: 0.7583
Epoch [66], train_loss: 0.0399, train_acc: 0.9876, val_loss: 1.9637, val_acc: 0.7532
Epoch [67], train_loss: 0.0403, train_acc: 0.9878, val_loss: 2.0294, val_acc: 0.7627
Epoch [68], train_loss: 0.0365, train_acc: 0.9895, val_loss: 2.1938, val_acc: 0.7496
Epoch [69], train_loss: 0.0334, train_acc: 0.9902, val_loss: 2.1814, val_acc: 0.7408
Epoch [70], train_loss: 0.0444, train_acc: 0.9870, val_loss: 2.1139, val_acc: 0.7370
Epoch [71], train_loss: 0.0494, train_acc: 0.9855, val_loss: 2.0815, val_acc: 0.7411
Epoch [72], train_loss: 0.0423, train_acc: 0.9869, val_loss: 1.9717, val_acc: 0.7570
Epoch [73], train_loss: 0.0357, train_acc: 0.9895, val_loss: 2.1516, val_acc: 0.7577
Epoch [74], train_loss: 0.0296, train_acc: 0.9912, val_loss: 1.9500, val_acc: 0.7508
Epoch [75], train_loss: 0.0340, train_acc: 0.9901, val_loss: 2.1411, val_acc: 0.7606
Epoch [76], train_loss: 0.0529, train_acc: 0.9847, val_loss: 2.0546, val_acc: 0.7476
Epoch [77], train_loss: 0.0382, train_acc: 0.9886, val_loss: 2.1189, val_acc: 0.7481
Epoch [78], train_loss: 0.0367, train_acc: 0.9891, val_loss: 2.2200, val_acc: 0.7598
Epoch [79], train_loss: 0.0337, train_acc: 0.9906, val_loss: 2.0331, val_acc: 0.7456
Epoch [80], train_loss: 0.0305, train_acc: 0.9909, val_loss: 2.2610, val_acc: 0.7514
Epoch [81], train_loss: 0.0463, train_acc: 0.9866, val_loss: 2.1225, val_acc: 0.7499
Epoch [82], train_loss: 0.0346, train_acc: 0.9902, val_loss: 2.0747, val_acc: 0.7594
Epoch [83], train_loss: 0.0349, train_acc: 0.9895, val_loss: 2.1283, val_acc: 0.7533
Epoch [84], train_loss: 0.0419, train_acc: 0.9878, val_loss: 2.1252, val_acc: 0.7457
Epoch [85], train_loss: 0.0344, train_acc: 0.9905, val_loss: 2.2289, val_acc: 0.7455
Epoch [86], train_loss: 0.0380, train_acc: 0.9899, val_loss: 2.3064, val_acc: 0.7415
Epoch [87], train_loss: 0.0341, train_acc: 0.9910, val_loss: 2.2089, val_acc: 0.7502
Epoch [88], train_loss: 0.0434, train_acc: 0.9876, val_loss: 2.0796, val_acc: 0.7489
Epoch [89], train_loss: 0.0339, train_acc: 0.9903, val_loss: 2.1560, val_acc: 0.7601
Epoch [90], train_loss: 0.0238, train_acc: 0.9930, val_loss: 2.2694, val_acc: 0.7514
Epoch [91], train_loss: 0.0475, train_acc: 0.9865, val_loss: 2.3228, val_acc: 0.7572
Epoch [92], train_loss: 0.0378, train_acc: 0.9892, val_loss: 2.2385, val_acc: 0.7515
Epoch [93], train_loss: 0.0408, train_acc: 0.9884, val_loss: 2.1161, val_acc: 0.7613
Epoch [94], train_loss: 0.0306, train_acc: 0.9913, val_loss: 2.4650, val_acc: 0.7455
Epoch [95], train_loss: 0.0446, train_acc: 0.9880, val_loss: 2.1146, val_acc: 0.7615
Epoch [96], train_loss: 0.0284, train_acc: 0.9920, val_loss: 2.1704, val_acc: 0.7595
Epoch [97], train_loss: 0.0393, train_acc: 0.9886, val_loss: 2.3172, val_acc: 0.7534
Epoch [98], train_loss: 0.0451, train_acc: 0.9869, val_loss: 2.2320, val_acc: 0.7431
Epoch [99], train_loss: 0.0360, train_acc: 0.9899, val_loss: 2.2378, val_acc: 0.7566
 
Visualize trining => save images
 
Load the model => start
 
Check best/last models => start
 
Summary result of test set => best model  {'val_loss': 0.7475205659866333, 'val_acc': 0.7431640625}
Summary result of test set => last model {'val_loss': 2.391453266143799, 'val_acc': 0.7470703125}
Test set evaluation => save results for postprocessing
 
** accuracy: 0.745
--
confusion matrix
[[818  13  50  15  20   3   3   8  45  25]
 [ 30 882   2   8   6   3   3   1  16  49]
 [ 55   4 686  52  86  50  24  24  14   5]
 [ 29   4  91 582  47 158  33  33   9  14]
 [ 19   2  92  59 739  29  12  40   5   3]
 [ 16   2  55 138  37 695  14  35   3   5]
 [ 10   4  77  95 106  34 651   6  11   6]
 [ 10   2  37  59  69  53   2 759   2   7]
 [ 84  20  17  17  13   3   2   3 831  10]
 [ 57  74   3  24   6   4   2  13  15 802]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.73      0.82      0.77      1000
  automobile       0.88      0.88      0.88      1000
        bird       0.62      0.69      0.65      1000
         cat       0.55      0.58      0.57      1000
        deer       0.65      0.74      0.69      1000
         dog       0.67      0.69      0.68      1000
        frog       0.87      0.65      0.75      1000
       horse       0.82      0.76      0.79      1000
        ship       0.87      0.83      0.85      1000
       truck       0.87      0.80      0.83      1000

    accuracy                           0.74     10000
   macro avg       0.75      0.74      0.75     10000
weighted avg       0.75      0.74      0.75     10000

END OF CODE
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 270443: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <access4> by user <ingap> in cluster <wexac> at Tue Feb 27 11:08:27 2024
Job was executed on host(s) <hgn53>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 11:08:53 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 11:08:53 2024
Terminated at Tue Feb 27 11:18:10 2024
Results reported at Tue Feb 27 11:18:10 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_N2_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_N2_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1999.00 sec.
    Max Memory :                                 3561 MB
    Average Memory :                             3234.57 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6679.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   559 sec.
    Turnaround time :                            583 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_N2_err_270443> for stderr output of this job.

