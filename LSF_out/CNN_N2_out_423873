loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.9014, train_acc: 0.2822, val_loss: 1.5055, val_acc: 0.4414, val_precision: 0.4655, val_recall: 0.4414, val_f1: 0.4229
Epoch [1], train_loss: 1.3065, train_acc: 0.5222, val_loss: 1.1419, val_acc: 0.5794, val_precision: 0.5871, val_recall: 0.5794, val_f1: 0.5726
Epoch [2], train_loss: 1.0395, train_acc: 0.6266, val_loss: 0.9641, val_acc: 0.6569, val_precision: 0.6617, val_recall: 0.6569, val_f1: 0.6517
Epoch [3], train_loss: 0.8718, train_acc: 0.6914, val_loss: 0.8501, val_acc: 0.6942, val_precision: 0.7140, val_recall: 0.6942, val_f1: 0.6930
Epoch [4], train_loss: 0.7342, train_acc: 0.7426, val_loss: 0.7813, val_acc: 0.7202, val_precision: 0.7381, val_recall: 0.7202, val_f1: 0.7234
Epoch [5], train_loss: 0.6327, train_acc: 0.7776, val_loss: 0.7778, val_acc: 0.7329, val_precision: 0.7508, val_recall: 0.7329, val_f1: 0.7309
Epoch [6], train_loss: 0.5313, train_acc: 0.8134, val_loss: 0.7394, val_acc: 0.7406, val_precision: 0.7576, val_recall: 0.7406, val_f1: 0.7426
Epoch [7], train_loss: 0.4391, train_acc: 0.8468, val_loss: 0.7517, val_acc: 0.7530, val_precision: 0.7633, val_recall: 0.7530, val_f1: 0.7534
Epoch [8], train_loss: 0.3586, train_acc: 0.8741, val_loss: 0.7816, val_acc: 0.7505, val_precision: 0.7662, val_recall: 0.7505, val_f1: 0.7524
Epoch [9], train_loss: 0.2897, train_acc: 0.8969, val_loss: 0.8338, val_acc: 0.7491, val_precision: 0.7619, val_recall: 0.7491, val_f1: 0.7489
Epoch [10], train_loss: 0.2358, train_acc: 0.9163, val_loss: 0.8905, val_acc: 0.7635, val_precision: 0.7685, val_recall: 0.7635, val_f1: 0.7621
Epoch [11], train_loss: 0.1908, train_acc: 0.9333, val_loss: 0.9769, val_acc: 0.7329, val_precision: 0.7429, val_recall: 0.7329, val_f1: 0.7325
Epoch [12], train_loss: 0.1469, train_acc: 0.9482, val_loss: 1.1235, val_acc: 0.7449, val_precision: 0.7590, val_recall: 0.7449, val_f1: 0.7467
Epoch [13], train_loss: 0.1234, train_acc: 0.9574, val_loss: 1.1537, val_acc: 0.7442, val_precision: 0.7578, val_recall: 0.7442, val_f1: 0.7455
Epoch [14], train_loss: 0.1172, train_acc: 0.9597, val_loss: 1.2642, val_acc: 0.7419, val_precision: 0.7518, val_recall: 0.7419, val_f1: 0.7419
Epoch [15], train_loss: 0.1003, train_acc: 0.9648, val_loss: 1.2858, val_acc: 0.7528, val_precision: 0.7643, val_recall: 0.7528, val_f1: 0.7531
Epoch [16], train_loss: 0.0918, train_acc: 0.9690, val_loss: 1.3626, val_acc: 0.7497, val_precision: 0.7585, val_recall: 0.7497, val_f1: 0.7502
Epoch [17], train_loss: 0.0871, train_acc: 0.9699, val_loss: 1.4362, val_acc: 0.7538, val_precision: 0.7660, val_recall: 0.7538, val_f1: 0.7546
Epoch [18], train_loss: 0.0742, train_acc: 0.9744, val_loss: 1.4057, val_acc: 0.7491, val_precision: 0.7642, val_recall: 0.7491, val_f1: 0.7509
Epoch [19], train_loss: 0.0779, train_acc: 0.9740, val_loss: 1.3245, val_acc: 0.7518, val_precision: 0.7580, val_recall: 0.7518, val_f1: 0.7504
Epoch [20], train_loss: 0.0703, train_acc: 0.9768, val_loss: 1.5183, val_acc: 0.7395, val_precision: 0.7551, val_recall: 0.7395, val_f1: 0.7422
Epoch [21], train_loss: 0.0795, train_acc: 0.9730, val_loss: 1.4048, val_acc: 0.7406, val_precision: 0.7480, val_recall: 0.7406, val_f1: 0.7401
Epoch [22], train_loss: 0.0625, train_acc: 0.9791, val_loss: 1.5259, val_acc: 0.7588, val_precision: 0.7665, val_recall: 0.7588, val_f1: 0.7593
Epoch [23], train_loss: 0.0672, train_acc: 0.9767, val_loss: 1.5374, val_acc: 0.7577, val_precision: 0.7638, val_recall: 0.7577, val_f1: 0.7555
Epoch [24], train_loss: 0.0615, train_acc: 0.9797, val_loss: 1.5028, val_acc: 0.7506, val_precision: 0.7543, val_recall: 0.7506, val_f1: 0.7475
Epoch [25], train_loss: 0.0613, train_acc: 0.9782, val_loss: 1.6898, val_acc: 0.7354, val_precision: 0.7457, val_recall: 0.7354, val_f1: 0.7352
Epoch [26], train_loss: 0.0621, train_acc: 0.9799, val_loss: 1.5307, val_acc: 0.7496, val_precision: 0.7544, val_recall: 0.7496, val_f1: 0.7480
Epoch [27], train_loss: 0.0506, train_acc: 0.9837, val_loss: 1.7198, val_acc: 0.7488, val_precision: 0.7605, val_recall: 0.7488, val_f1: 0.7491
Epoch [28], train_loss: 0.0506, train_acc: 0.9836, val_loss: 1.6820, val_acc: 0.7453, val_precision: 0.7542, val_recall: 0.7453, val_f1: 0.7444
Epoch [29], train_loss: 0.0671, train_acc: 0.9778, val_loss: 1.6570, val_acc: 0.7384, val_precision: 0.7514, val_recall: 0.7384, val_f1: 0.7386
Epoch [30], train_loss: 0.0539, train_acc: 0.9824, val_loss: 1.6581, val_acc: 0.7411, val_precision: 0.7472, val_recall: 0.7411, val_f1: 0.7370
Epoch [31], train_loss: 0.0583, train_acc: 0.9808, val_loss: 1.6791, val_acc: 0.7476, val_precision: 0.7628, val_recall: 0.7476, val_f1: 0.7503
Epoch [32], train_loss: 0.0503, train_acc: 0.9836, val_loss: 1.7568, val_acc: 0.7385, val_precision: 0.7568, val_recall: 0.7385, val_f1: 0.7396
Epoch [33], train_loss: 0.0580, train_acc: 0.9818, val_loss: 1.6392, val_acc: 0.7482, val_precision: 0.7517, val_recall: 0.7482, val_f1: 0.7450
Epoch [34], train_loss: 0.0507, train_acc: 0.9832, val_loss: 1.6527, val_acc: 0.7452, val_precision: 0.7579, val_recall: 0.7452, val_f1: 0.7465
Epoch [35], train_loss: 0.0403, train_acc: 0.9864, val_loss: 1.8888, val_acc: 0.7527, val_precision: 0.7643, val_recall: 0.7527, val_f1: 0.7539
Epoch [36], train_loss: 0.0552, train_acc: 0.9828, val_loss: 1.6128, val_acc: 0.7520, val_precision: 0.7616, val_recall: 0.7520, val_f1: 0.7522
Epoch [37], train_loss: 0.0511, train_acc: 0.9836, val_loss: 1.6603, val_acc: 0.7461, val_precision: 0.7521, val_recall: 0.7461, val_f1: 0.7430
Epoch [38], train_loss: 0.0404, train_acc: 0.9869, val_loss: 1.6984, val_acc: 0.7458, val_precision: 0.7529, val_recall: 0.7458, val_f1: 0.7439
Epoch [39], train_loss: 0.0582, train_acc: 0.9820, val_loss: 1.7220, val_acc: 0.7552, val_precision: 0.7611, val_recall: 0.7552, val_f1: 0.7538
Epoch [40], train_loss: 0.0462, train_acc: 0.9852, val_loss: 1.6362, val_acc: 0.7547, val_precision: 0.7626, val_recall: 0.7547, val_f1: 0.7540
Epoch [41], train_loss: 0.0496, train_acc: 0.9845, val_loss: 1.5871, val_acc: 0.7564, val_precision: 0.7653, val_recall: 0.7564, val_f1: 0.7566
Epoch [42], train_loss: 0.0425, train_acc: 0.9861, val_loss: 1.9884, val_acc: 0.7462, val_precision: 0.7558, val_recall: 0.7462, val_f1: 0.7429
Epoch [43], train_loss: 0.0473, train_acc: 0.9849, val_loss: 1.7546, val_acc: 0.7342, val_precision: 0.7472, val_recall: 0.7342, val_f1: 0.7364
Epoch [44], train_loss: 0.0434, train_acc: 0.9862, val_loss: 1.8184, val_acc: 0.7457, val_precision: 0.7528, val_recall: 0.7457, val_f1: 0.7452
Epoch [45], train_loss: 0.0494, train_acc: 0.9845, val_loss: 1.8257, val_acc: 0.7500, val_precision: 0.7598, val_recall: 0.7500, val_f1: 0.7508
Epoch [46], train_loss: 0.0461, train_acc: 0.9857, val_loss: 1.7906, val_acc: 0.7523, val_precision: 0.7634, val_recall: 0.7523, val_f1: 0.7536
Epoch [47], train_loss: 0.0395, train_acc: 0.9874, val_loss: 1.9330, val_acc: 0.7437, val_precision: 0.7516, val_recall: 0.7437, val_f1: 0.7417
Epoch [48], train_loss: 0.0468, train_acc: 0.9849, val_loss: 1.8441, val_acc: 0.7514, val_precision: 0.7620, val_recall: 0.7514, val_f1: 0.7529
Epoch [49], train_loss: 0.0420, train_acc: 0.9865, val_loss: 1.9417, val_acc: 0.7501, val_precision: 0.7606, val_recall: 0.7501, val_f1: 0.7510
Epoch [50], train_loss: 0.0432, train_acc: 0.9866, val_loss: 1.8292, val_acc: 0.7475, val_precision: 0.7608, val_recall: 0.7475, val_f1: 0.7493
Epoch [51], train_loss: 0.0418, train_acc: 0.9866, val_loss: 1.8635, val_acc: 0.7676, val_precision: 0.7722, val_recall: 0.7676, val_f1: 0.7659
Epoch [52], train_loss: 0.0475, train_acc: 0.9847, val_loss: 2.0484, val_acc: 0.7459, val_precision: 0.7592, val_recall: 0.7459, val_f1: 0.7470
Epoch [53], train_loss: 0.0443, train_acc: 0.9865, val_loss: 1.9224, val_acc: 0.7624, val_precision: 0.7663, val_recall: 0.7624, val_f1: 0.7603
Epoch [54], train_loss: 0.0457, train_acc: 0.9858, val_loss: 1.8616, val_acc: 0.7590, val_precision: 0.7663, val_recall: 0.7590, val_f1: 0.7582
Epoch [55], train_loss: 0.0342, train_acc: 0.9895, val_loss: 1.9480, val_acc: 0.7547, val_precision: 0.7617, val_recall: 0.7547, val_f1: 0.7540
Epoch [56], train_loss: 0.0567, train_acc: 0.9830, val_loss: 1.8705, val_acc: 0.7510, val_precision: 0.7612, val_recall: 0.7510, val_f1: 0.7518
Epoch [57], train_loss: 0.0335, train_acc: 0.9894, val_loss: 1.8625, val_acc: 0.7546, val_precision: 0.7623, val_recall: 0.7546, val_f1: 0.7544
Epoch [58], train_loss: 0.0378, train_acc: 0.9882, val_loss: 2.1079, val_acc: 0.7419, val_precision: 0.7495, val_recall: 0.7419, val_f1: 0.7385
Epoch [59], train_loss: 0.0530, train_acc: 0.9842, val_loss: 1.8984, val_acc: 0.7516, val_precision: 0.7605, val_recall: 0.7516, val_f1: 0.7517
Epoch [60], train_loss: 0.0465, train_acc: 0.9851, val_loss: 1.9573, val_acc: 0.7449, val_precision: 0.7564, val_recall: 0.7449, val_f1: 0.7432
Epoch [61], train_loss: 0.0430, train_acc: 0.9869, val_loss: 1.9100, val_acc: 0.7507, val_precision: 0.7653, val_recall: 0.7507, val_f1: 0.7535
Epoch [62], train_loss: 0.0355, train_acc: 0.9892, val_loss: 1.9629, val_acc: 0.7558, val_precision: 0.7649, val_recall: 0.7558, val_f1: 0.7560
Epoch [63], train_loss: 0.0330, train_acc: 0.9901, val_loss: 2.1761, val_acc: 0.7484, val_precision: 0.7568, val_recall: 0.7484, val_f1: 0.7470
Epoch [64], train_loss: 0.0444, train_acc: 0.9872, val_loss: 2.0196, val_acc: 0.7575, val_precision: 0.7685, val_recall: 0.7575, val_f1: 0.7574
Epoch [65], train_loss: 0.0326, train_acc: 0.9907, val_loss: 1.9899, val_acc: 0.7583, val_precision: 0.7669, val_recall: 0.7583, val_f1: 0.7590
Epoch [66], train_loss: 0.0399, train_acc: 0.9876, val_loss: 1.9637, val_acc: 0.7532, val_precision: 0.7617, val_recall: 0.7532, val_f1: 0.7524
Epoch [67], train_loss: 0.0403, train_acc: 0.9878, val_loss: 2.0294, val_acc: 0.7627, val_precision: 0.7750, val_recall: 0.7627, val_f1: 0.7644
Epoch [68], train_loss: 0.0365, train_acc: 0.9895, val_loss: 2.1938, val_acc: 0.7496, val_precision: 0.7563, val_recall: 0.7496, val_f1: 0.7466
Epoch [69], train_loss: 0.0334, train_acc: 0.9902, val_loss: 2.1814, val_acc: 0.7408, val_precision: 0.7470, val_recall: 0.7408, val_f1: 0.7385
Epoch [70], train_loss: 0.0444, train_acc: 0.9870, val_loss: 2.1139, val_acc: 0.7370, val_precision: 0.7552, val_recall: 0.7370, val_f1: 0.7383
Epoch [71], train_loss: 0.0494, train_acc: 0.9855, val_loss: 2.0815, val_acc: 0.7411, val_precision: 0.7554, val_recall: 0.7411, val_f1: 0.7412
Epoch [72], train_loss: 0.0423, train_acc: 0.9869, val_loss: 1.9717, val_acc: 0.7570, val_precision: 0.7649, val_recall: 0.7570, val_f1: 0.7558
Epoch [73], train_loss: 0.0357, train_acc: 0.9895, val_loss: 2.1516, val_acc: 0.7577, val_precision: 0.7650, val_recall: 0.7577, val_f1: 0.7573
Epoch [74], train_loss: 0.0296, train_acc: 0.9912, val_loss: 1.9500, val_acc: 0.7508, val_precision: 0.7632, val_recall: 0.7508, val_f1: 0.7516
Epoch [75], train_loss: 0.0340, train_acc: 0.9901, val_loss: 2.1411, val_acc: 0.7606, val_precision: 0.7720, val_recall: 0.7606, val_f1: 0.7622
Epoch [76], train_loss: 0.0529, train_acc: 0.9847, val_loss: 2.0546, val_acc: 0.7476, val_precision: 0.7595, val_recall: 0.7476, val_f1: 0.7476
Epoch [77], train_loss: 0.0382, train_acc: 0.9886, val_loss: 2.1189, val_acc: 0.7481, val_precision: 0.7568, val_recall: 0.7481, val_f1: 0.7480
Epoch [78], train_loss: 0.0367, train_acc: 0.9891, val_loss: 2.2200, val_acc: 0.7598, val_precision: 0.7664, val_recall: 0.7598, val_f1: 0.7591
Epoch [79], train_loss: 0.0337, train_acc: 0.9906, val_loss: 2.0331, val_acc: 0.7456, val_precision: 0.7524, val_recall: 0.7456, val_f1: 0.7452
Epoch [80], train_loss: 0.0305, train_acc: 0.9909, val_loss: 2.2610, val_acc: 0.7514, val_precision: 0.7598, val_recall: 0.7514, val_f1: 0.7510
Epoch [81], train_loss: 0.0463, train_acc: 0.9866, val_loss: 2.1225, val_acc: 0.7499, val_precision: 0.7619, val_recall: 0.7499, val_f1: 0.7519
Epoch [82], train_loss: 0.0346, train_acc: 0.9902, val_loss: 2.0747, val_acc: 0.7594, val_precision: 0.7680, val_recall: 0.7594, val_f1: 0.7580
Epoch [83], train_loss: 0.0349, train_acc: 0.9895, val_loss: 2.1283, val_acc: 0.7533, val_precision: 0.7600, val_recall: 0.7533, val_f1: 0.7523
Epoch [84], train_loss: 0.0419, train_acc: 0.9878, val_loss: 2.1252, val_acc: 0.7457, val_precision: 0.7616, val_recall: 0.7457, val_f1: 0.7489
Epoch [85], train_loss: 0.0344, train_acc: 0.9905, val_loss: 2.2289, val_acc: 0.7455, val_precision: 0.7541, val_recall: 0.7455, val_f1: 0.7447
Epoch [86], train_loss: 0.0380, train_acc: 0.9899, val_loss: 2.3064, val_acc: 0.7415, val_precision: 0.7515, val_recall: 0.7415, val_f1: 0.7419
Epoch [87], train_loss: 0.0341, train_acc: 0.9910, val_loss: 2.2089, val_acc: 0.7502, val_precision: 0.7579, val_recall: 0.7502, val_f1: 0.7486
Epoch [88], train_loss: 0.0434, train_acc: 0.9876, val_loss: 2.0796, val_acc: 0.7489, val_precision: 0.7536, val_recall: 0.7489, val_f1: 0.7462
Epoch [89], train_loss: 0.0339, train_acc: 0.9903, val_loss: 2.1560, val_acc: 0.7601, val_precision: 0.7665, val_recall: 0.7601, val_f1: 0.7598
Epoch [90], train_loss: 0.0238, train_acc: 0.9930, val_loss: 2.2694, val_acc: 0.7514, val_precision: 0.7622, val_recall: 0.7514, val_f1: 0.7526
Epoch [91], train_loss: 0.0475, train_acc: 0.9865, val_loss: 2.3228, val_acc: 0.7572, val_precision: 0.7673, val_recall: 0.7572, val_f1: 0.7581
Epoch [92], train_loss: 0.0378, train_acc: 0.9892, val_loss: 2.2385, val_acc: 0.7515, val_precision: 0.7617, val_recall: 0.7515, val_f1: 0.7517
Epoch [93], train_loss: 0.0408, train_acc: 0.9884, val_loss: 2.1161, val_acc: 0.7613, val_precision: 0.7669, val_recall: 0.7613, val_f1: 0.7603
Epoch [94], train_loss: 0.0306, train_acc: 0.9913, val_loss: 2.4650, val_acc: 0.7455, val_precision: 0.7604, val_recall: 0.7455, val_f1: 0.7465
Epoch [95], train_loss: 0.0446, train_acc: 0.9880, val_loss: 2.1146, val_acc: 0.7615, val_precision: 0.7671, val_recall: 0.7615, val_f1: 0.7607
Epoch [96], train_loss: 0.0284, train_acc: 0.9920, val_loss: 2.1704, val_acc: 0.7595, val_precision: 0.7671, val_recall: 0.7595, val_f1: 0.7595
Epoch [97], train_loss: 0.0393, train_acc: 0.9886, val_loss: 2.3172, val_acc: 0.7534, val_precision: 0.7609, val_recall: 0.7534, val_f1: 0.7532
Epoch [98], train_loss: 0.0451, train_acc: 0.9869, val_loss: 2.2320, val_acc: 0.7431, val_precision: 0.7550, val_recall: 0.7431, val_f1: 0.7445
Epoch [99], train_loss: 0.0360, train_acc: 0.9899, val_loss: 2.2378, val_acc: 0.7566, val_precision: 0.7692, val_recall: 0.7566, val_f1: 0.7566
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.7475, val_acc: 0.7432, val_precision: 0.7603, val_recall: 0.7432, val_f1: 0.7450
Summary result of test set => last model => val_loss: 2.3915, val_acc: 0.7471, val_precision: 0.7596, val_recall: 0.7471, val_f1: 0.7470
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7445
--
confusion matrix
[[818  13  50  15  20   3   3   8  45  25]
 [ 30 882   2   8   6   3   3   1  16  49]
 [ 55   4 686  52  86  50  24  24  14   5]
 [ 29   4  91 582  47 158  33  33   9  14]
 [ 19   2  92  59 739  29  12  40   5   3]
 [ 16   2  55 138  37 695  14  35   3   5]
 [ 10   4  77  95 106  34 651   6  11   6]
 [ 10   2  37  59  69  53   2 759   2   7]
 [ 84  20  17  17  13   3   2   3 831  10]
 [ 57  74   3  24   6   4   2  13  15 802]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.73      0.82      0.77      1000
  automobile       0.88      0.88      0.88      1000
        bird       0.62      0.69      0.65      1000
         cat       0.55      0.58      0.57      1000
        deer       0.65      0.74      0.69      1000
         dog       0.67      0.69      0.68      1000
        frog       0.87      0.65      0.75      1000
       horse       0.82      0.76      0.79      1000
        ship       0.87      0.83      0.85      1000
       truck       0.87      0.80      0.83      1000

    accuracy                           0.74     10000
   macro avg       0.75      0.74      0.75     10000
weighted avg       0.75      0.74      0.75     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7406
--
confusion matrix
[[405   6  25   7  11   2   2   4  13  13]
 [ 12 455   6   3   4   1   0   1  10  20]
 [ 26   1 381  33  40  11  16  11   7   6]
 [ 15   1  44 255  29  87  14  12   8   6]
 [  8   0  50  27 341  14   8  23   0   0]
 [  2   0  25  89  26 339   4  23   5   1]
 [  7   2  48  41  67  18 309   4   6   5]
 [  6   1  22  16  35  24   0 388   1   7]
 [ 48   7  10  11   1   1   1   0 418   7]
 [ 23  44   3   8   4   1   1   4   1 412]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.73      0.83      0.78       488
  automobile       0.88      0.89      0.88       512
        bird       0.62      0.72      0.66       532
         cat       0.52      0.54      0.53       471
        deer       0.61      0.72      0.66       471
         dog       0.68      0.66      0.67       514
        frog       0.87      0.61      0.72       507
       horse       0.83      0.78      0.80       500
        ship       0.89      0.83      0.86       504
       truck       0.86      0.82      0.84       501

    accuracy                           0.74      5000
   macro avg       0.75      0.74      0.74      5000
weighted avg       0.75      0.74      0.74      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423873: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:21:33 2024
Job was executed on host(s) <hgn52>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:21:44 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:21:44 2024
Terminated at Wed Feb 28 08:31:45 2024
Results reported at Wed Feb 28 08:31:45 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_N2_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_N2_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1971.00 sec.
    Max Memory :                                 3364 MB
    Average Memory :                             3251.26 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6876.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   602 sec.
    Turnaround time :                            612 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_N2_err_423873> for stderr output of this job.

