loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 2.3193, train_acc: 0.1052, val_loss: 2.2984, val_acc: 0.1035, val_precision: 0.0154, val_recall: 0.1035, val_f1: 0.0259
Epoch [1], train_loss: 2.1920, train_acc: 0.1745, val_loss: 2.0479, val_acc: 0.2338, val_precision: 0.2093, val_recall: 0.2338, val_f1: 0.1722
Epoch [2], train_loss: 1.8688, train_acc: 0.3182, val_loss: 1.7485, val_acc: 0.3596, val_precision: 0.3723, val_recall: 0.3596, val_f1: 0.3361
Epoch [3], train_loss: 1.6022, train_acc: 0.4093, val_loss: 1.5276, val_acc: 0.4317, val_precision: 0.4441, val_recall: 0.4317, val_f1: 0.4167
Epoch [4], train_loss: 1.4622, train_acc: 0.4645, val_loss: 1.4162, val_acc: 0.4820, val_precision: 0.4957, val_recall: 0.4820, val_f1: 0.4740
Epoch [5], train_loss: 1.3215, train_acc: 0.5195, val_loss: 1.2631, val_acc: 0.5429, val_precision: 0.5453, val_recall: 0.5429, val_f1: 0.5386
Epoch [6], train_loss: 1.2191, train_acc: 0.5576, val_loss: 1.1930, val_acc: 0.5650, val_precision: 0.5679, val_recall: 0.5650, val_f1: 0.5556
Epoch [7], train_loss: 1.1103, train_acc: 0.6003, val_loss: 1.1200, val_acc: 0.6012, val_precision: 0.6165, val_recall: 0.6012, val_f1: 0.6004
Epoch [8], train_loss: 1.0217, train_acc: 0.6324, val_loss: 1.0140, val_acc: 0.6369, val_precision: 0.6473, val_recall: 0.6369, val_f1: 0.6376
Epoch [9], train_loss: 0.9434, train_acc: 0.6642, val_loss: 0.9906, val_acc: 0.6486, val_precision: 0.6615, val_recall: 0.6486, val_f1: 0.6446
Epoch [10], train_loss: 0.8630, train_acc: 0.6932, val_loss: 0.9060, val_acc: 0.6796, val_precision: 0.6772, val_recall: 0.6796, val_f1: 0.6711
Epoch [11], train_loss: 0.7970, train_acc: 0.7164, val_loss: 0.8928, val_acc: 0.6789, val_precision: 0.6925, val_recall: 0.6789, val_f1: 0.6763
Epoch [12], train_loss: 0.7395, train_acc: 0.7380, val_loss: 0.8532, val_acc: 0.6977, val_precision: 0.7076, val_recall: 0.6977, val_f1: 0.6947
Epoch [13], train_loss: 0.6741, train_acc: 0.7623, val_loss: 0.8059, val_acc: 0.7213, val_precision: 0.7247, val_recall: 0.7213, val_f1: 0.7202
Epoch [14], train_loss: 0.6118, train_acc: 0.7837, val_loss: 0.8101, val_acc: 0.7141, val_precision: 0.7288, val_recall: 0.7141, val_f1: 0.7168
Epoch [15], train_loss: 0.5542, train_acc: 0.8054, val_loss: 0.7800, val_acc: 0.7352, val_precision: 0.7483, val_recall: 0.7352, val_f1: 0.7379
Epoch [16], train_loss: 0.5167, train_acc: 0.8176, val_loss: 0.7765, val_acc: 0.7378, val_precision: 0.7422, val_recall: 0.7378, val_f1: 0.7344
Epoch [17], train_loss: 0.4663, train_acc: 0.8339, val_loss: 0.7834, val_acc: 0.7398, val_precision: 0.7430, val_recall: 0.7398, val_f1: 0.7383
Epoch [18], train_loss: 0.4076, train_acc: 0.8571, val_loss: 0.7934, val_acc: 0.7413, val_precision: 0.7491, val_recall: 0.7413, val_f1: 0.7424
Epoch [19], train_loss: 0.3336, train_acc: 0.8834, val_loss: 0.8145, val_acc: 0.7432, val_precision: 0.7472, val_recall: 0.7432, val_f1: 0.7412
Epoch [20], train_loss: 0.2822, train_acc: 0.9013, val_loss: 0.8636, val_acc: 0.7468, val_precision: 0.7526, val_recall: 0.7468, val_f1: 0.7488
Epoch [21], train_loss: 0.2375, train_acc: 0.9171, val_loss: 0.9403, val_acc: 0.7426, val_precision: 0.7494, val_recall: 0.7426, val_f1: 0.7424
Epoch [22], train_loss: 0.2019, train_acc: 0.9285, val_loss: 0.9853, val_acc: 0.7325, val_precision: 0.7406, val_recall: 0.7325, val_f1: 0.7336
Epoch [23], train_loss: 0.1459, train_acc: 0.9502, val_loss: 1.0646, val_acc: 0.7412, val_precision: 0.7534, val_recall: 0.7412, val_f1: 0.7435
Epoch [24], train_loss: 0.1068, train_acc: 0.9637, val_loss: 1.2129, val_acc: 0.7424, val_precision: 0.7439, val_recall: 0.7424, val_f1: 0.7406
Epoch [25], train_loss: 0.0809, train_acc: 0.9734, val_loss: 1.3132, val_acc: 0.7421, val_precision: 0.7430, val_recall: 0.7421, val_f1: 0.7355
Epoch [26], train_loss: 0.0762, train_acc: 0.9736, val_loss: 1.3297, val_acc: 0.7412, val_precision: 0.7419, val_recall: 0.7412, val_f1: 0.7406
Epoch [27], train_loss: 0.0539, train_acc: 0.9827, val_loss: 1.4507, val_acc: 0.7400, val_precision: 0.7541, val_recall: 0.7400, val_f1: 0.7441
Epoch [28], train_loss: 0.0407, train_acc: 0.9867, val_loss: 1.5709, val_acc: 0.7391, val_precision: 0.7461, val_recall: 0.7391, val_f1: 0.7414
Epoch [29], train_loss: 0.0304, train_acc: 0.9908, val_loss: 1.5952, val_acc: 0.7435, val_precision: 0.7463, val_recall: 0.7435, val_f1: 0.7424
Epoch [30], train_loss: 0.0267, train_acc: 0.9917, val_loss: 1.7087, val_acc: 0.7440, val_precision: 0.7461, val_recall: 0.7440, val_f1: 0.7434
Epoch [31], train_loss: 0.0356, train_acc: 0.9877, val_loss: 1.6265, val_acc: 0.7391, val_precision: 0.7474, val_recall: 0.7391, val_f1: 0.7418
Epoch [32], train_loss: 0.0474, train_acc: 0.9843, val_loss: 1.6161, val_acc: 0.7526, val_precision: 0.7524, val_recall: 0.7526, val_f1: 0.7511
Epoch [33], train_loss: 0.0378, train_acc: 0.9871, val_loss: 1.7308, val_acc: 0.7358, val_precision: 0.7506, val_recall: 0.7358, val_f1: 0.7386
Epoch [34], train_loss: 0.0468, train_acc: 0.9842, val_loss: 1.6450, val_acc: 0.7424, val_precision: 0.7530, val_recall: 0.7424, val_f1: 0.7447
Epoch [35], train_loss: 0.0281, train_acc: 0.9907, val_loss: 1.7065, val_acc: 0.7434, val_precision: 0.7429, val_recall: 0.7434, val_f1: 0.7413
Epoch [36], train_loss: 0.0167, train_acc: 0.9949, val_loss: 1.8181, val_acc: 0.7467, val_precision: 0.7502, val_recall: 0.7467, val_f1: 0.7472
Epoch [37], train_loss: 0.0200, train_acc: 0.9937, val_loss: 1.8468, val_acc: 0.7474, val_precision: 0.7511, val_recall: 0.7474, val_f1: 0.7480
Epoch [38], train_loss: 0.0162, train_acc: 0.9950, val_loss: 1.9125, val_acc: 0.7358, val_precision: 0.7373, val_recall: 0.7358, val_f1: 0.7349
Epoch [39], train_loss: 0.0340, train_acc: 0.9880, val_loss: 1.8227, val_acc: 0.7329, val_precision: 0.7382, val_recall: 0.7329, val_f1: 0.7335
Epoch [40], train_loss: 0.0541, train_acc: 0.9814, val_loss: 1.6324, val_acc: 0.7463, val_precision: 0.7538, val_recall: 0.7463, val_f1: 0.7474
Epoch [41], train_loss: 0.0188, train_acc: 0.9934, val_loss: 1.8647, val_acc: 0.7420, val_precision: 0.7494, val_recall: 0.7420, val_f1: 0.7431
Epoch [42], train_loss: 0.0151, train_acc: 0.9947, val_loss: 1.9093, val_acc: 0.7491, val_precision: 0.7530, val_recall: 0.7491, val_f1: 0.7493
Epoch [43], train_loss: 0.0154, train_acc: 0.9951, val_loss: 1.9118, val_acc: 0.7484, val_precision: 0.7488, val_recall: 0.7484, val_f1: 0.7478
Epoch [44], train_loss: 0.0149, train_acc: 0.9951, val_loss: 1.9643, val_acc: 0.7423, val_precision: 0.7445, val_recall: 0.7423, val_f1: 0.7420
Epoch [45], train_loss: 0.0446, train_acc: 0.9848, val_loss: 1.6854, val_acc: 0.7416, val_precision: 0.7440, val_recall: 0.7416, val_f1: 0.7417
Epoch [46], train_loss: 0.0219, train_acc: 0.9929, val_loss: 1.7663, val_acc: 0.7466, val_precision: 0.7508, val_recall: 0.7466, val_f1: 0.7471
Epoch [47], train_loss: 0.0182, train_acc: 0.9938, val_loss: 1.9302, val_acc: 0.7457, val_precision: 0.7502, val_recall: 0.7457, val_f1: 0.7463
Epoch [48], train_loss: 0.0172, train_acc: 0.9947, val_loss: 1.9630, val_acc: 0.7434, val_precision: 0.7557, val_recall: 0.7434, val_f1: 0.7461
Epoch [49], train_loss: 0.0210, train_acc: 0.9929, val_loss: 1.8261, val_acc: 0.7513, val_precision: 0.7531, val_recall: 0.7513, val_f1: 0.7516
Epoch [50], train_loss: 0.0340, train_acc: 0.9882, val_loss: 1.7750, val_acc: 0.7426, val_precision: 0.7429, val_recall: 0.7426, val_f1: 0.7413
Epoch [51], train_loss: 0.0167, train_acc: 0.9942, val_loss: 1.9467, val_acc: 0.7461, val_precision: 0.7469, val_recall: 0.7461, val_f1: 0.7456
Epoch [52], train_loss: 0.0165, train_acc: 0.9947, val_loss: 1.9546, val_acc: 0.7527, val_precision: 0.7543, val_recall: 0.7527, val_f1: 0.7524
Epoch [53], train_loss: 0.0154, train_acc: 0.9946, val_loss: 1.9556, val_acc: 0.7489, val_precision: 0.7536, val_recall: 0.7489, val_f1: 0.7495
Epoch [54], train_loss: 0.0140, train_acc: 0.9953, val_loss: 1.9272, val_acc: 0.7529, val_precision: 0.7515, val_recall: 0.7529, val_f1: 0.7507
Epoch [55], train_loss: 0.0121, train_acc: 0.9959, val_loss: 2.1143, val_acc: 0.7535, val_precision: 0.7566, val_recall: 0.7535, val_f1: 0.7539
Epoch [56], train_loss: 0.0195, train_acc: 0.9934, val_loss: 1.9575, val_acc: 0.7452, val_precision: 0.7471, val_recall: 0.7452, val_f1: 0.7441
Epoch [57], train_loss: 0.0238, train_acc: 0.9918, val_loss: 2.0034, val_acc: 0.7406, val_precision: 0.7497, val_recall: 0.7406, val_f1: 0.7414
Epoch [58], train_loss: 0.0245, train_acc: 0.9915, val_loss: 1.8725, val_acc: 0.7479, val_precision: 0.7501, val_recall: 0.7479, val_f1: 0.7481
Epoch [59], train_loss: 0.0164, train_acc: 0.9944, val_loss: 1.9657, val_acc: 0.7444, val_precision: 0.7449, val_recall: 0.7444, val_f1: 0.7421
Epoch [60], train_loss: 0.0255, train_acc: 0.9913, val_loss: 1.9197, val_acc: 0.7483, val_precision: 0.7496, val_recall: 0.7483, val_f1: 0.7474
Epoch [61], train_loss: 0.0155, train_acc: 0.9952, val_loss: 1.8842, val_acc: 0.7541, val_precision: 0.7574, val_recall: 0.7541, val_f1: 0.7544
Epoch [62], train_loss: 0.0086, train_acc: 0.9971, val_loss: 2.0656, val_acc: 0.7526, val_precision: 0.7530, val_recall: 0.7526, val_f1: 0.7520
Epoch [63], train_loss: 0.0161, train_acc: 0.9945, val_loss: 2.0991, val_acc: 0.7533, val_precision: 0.7546, val_recall: 0.7533, val_f1: 0.7521
Epoch [64], train_loss: 0.0209, train_acc: 0.9934, val_loss: 1.9591, val_acc: 0.7506, val_precision: 0.7515, val_recall: 0.7506, val_f1: 0.7501
Epoch [65], train_loss: 0.0149, train_acc: 0.9950, val_loss: 2.0676, val_acc: 0.7563, val_precision: 0.7601, val_recall: 0.7563, val_f1: 0.7572
Epoch [66], train_loss: 0.0139, train_acc: 0.9956, val_loss: 2.1544, val_acc: 0.7441, val_precision: 0.7575, val_recall: 0.7441, val_f1: 0.7466
Epoch [67], train_loss: 0.0302, train_acc: 0.9895, val_loss: 1.8238, val_acc: 0.7445, val_precision: 0.7458, val_recall: 0.7445, val_f1: 0.7429
Epoch [68], train_loss: 0.0229, train_acc: 0.9921, val_loss: 1.9697, val_acc: 0.7546, val_precision: 0.7545, val_recall: 0.7546, val_f1: 0.7530
Epoch [69], train_loss: 0.0120, train_acc: 0.9960, val_loss: 2.1159, val_acc: 0.7514, val_precision: 0.7531, val_recall: 0.7514, val_f1: 0.7513
Epoch [70], train_loss: 0.0176, train_acc: 0.9939, val_loss: 2.0434, val_acc: 0.7516, val_precision: 0.7544, val_recall: 0.7516, val_f1: 0.7508
Epoch [71], train_loss: 0.0255, train_acc: 0.9912, val_loss: 1.9245, val_acc: 0.7454, val_precision: 0.7484, val_recall: 0.7454, val_f1: 0.7445
Epoch [72], train_loss: 0.0229, train_acc: 0.9920, val_loss: 1.9046, val_acc: 0.7561, val_precision: 0.7592, val_recall: 0.7561, val_f1: 0.7562
Epoch [73], train_loss: 0.0163, train_acc: 0.9942, val_loss: 1.9869, val_acc: 0.7572, val_precision: 0.7574, val_recall: 0.7572, val_f1: 0.7556
Epoch [74], train_loss: 0.0117, train_acc: 0.9962, val_loss: 1.9973, val_acc: 0.7507, val_precision: 0.7484, val_recall: 0.7507, val_f1: 0.7480
Epoch [75], train_loss: 0.0151, train_acc: 0.9952, val_loss: 1.9623, val_acc: 0.7498, val_precision: 0.7497, val_recall: 0.7498, val_f1: 0.7490
Epoch [76], train_loss: 0.0105, train_acc: 0.9968, val_loss: 2.0804, val_acc: 0.7535, val_precision: 0.7597, val_recall: 0.7535, val_f1: 0.7544
Epoch [77], train_loss: 0.0092, train_acc: 0.9966, val_loss: 2.2056, val_acc: 0.7448, val_precision: 0.7460, val_recall: 0.7448, val_f1: 0.7426
Epoch [78], train_loss: 0.0129, train_acc: 0.9958, val_loss: 2.1629, val_acc: 0.7540, val_precision: 0.7552, val_recall: 0.7540, val_f1: 0.7534
Epoch [79], train_loss: 0.0143, train_acc: 0.9951, val_loss: 1.9653, val_acc: 0.7555, val_precision: 0.7598, val_recall: 0.7555, val_f1: 0.7550
Epoch [80], train_loss: 0.0186, train_acc: 0.9936, val_loss: 2.0221, val_acc: 0.7475, val_precision: 0.7553, val_recall: 0.7475, val_f1: 0.7494
Epoch [81], train_loss: 0.0255, train_acc: 0.9912, val_loss: 2.0163, val_acc: 0.7526, val_precision: 0.7517, val_recall: 0.7526, val_f1: 0.7503
Epoch [82], train_loss: 0.0210, train_acc: 0.9928, val_loss: 2.0470, val_acc: 0.7508, val_precision: 0.7522, val_recall: 0.7508, val_f1: 0.7487
Epoch [83], train_loss: 0.0195, train_acc: 0.9937, val_loss: 2.0357, val_acc: 0.7468, val_precision: 0.7449, val_recall: 0.7468, val_f1: 0.7435
Epoch [84], train_loss: 0.0139, train_acc: 0.9955, val_loss: 2.1479, val_acc: 0.7584, val_precision: 0.7656, val_recall: 0.7584, val_f1: 0.7597
Epoch [85], train_loss: 0.0102, train_acc: 0.9965, val_loss: 2.0959, val_acc: 0.7542, val_precision: 0.7593, val_recall: 0.7542, val_f1: 0.7543
Epoch [86], train_loss: 0.0090, train_acc: 0.9968, val_loss: 2.3000, val_acc: 0.7527, val_precision: 0.7591, val_recall: 0.7527, val_f1: 0.7542
Epoch [87], train_loss: 0.0083, train_acc: 0.9970, val_loss: 2.1947, val_acc: 0.7510, val_precision: 0.7512, val_recall: 0.7510, val_f1: 0.7496
Epoch [88], train_loss: 0.0086, train_acc: 0.9972, val_loss: 2.2105, val_acc: 0.7489, val_precision: 0.7543, val_recall: 0.7489, val_f1: 0.7490
Epoch [89], train_loss: 0.0173, train_acc: 0.9938, val_loss: 2.1700, val_acc: 0.7479, val_precision: 0.7531, val_recall: 0.7479, val_f1: 0.7480
Epoch [90], train_loss: 0.0243, train_acc: 0.9917, val_loss: 1.9799, val_acc: 0.7422, val_precision: 0.7456, val_recall: 0.7422, val_f1: 0.7414
Epoch [91], train_loss: 0.0223, train_acc: 0.9929, val_loss: 2.0541, val_acc: 0.7523, val_precision: 0.7599, val_recall: 0.7523, val_f1: 0.7540
Epoch [92], train_loss: 0.0152, train_acc: 0.9946, val_loss: 1.9899, val_acc: 0.7503, val_precision: 0.7513, val_recall: 0.7503, val_f1: 0.7491
Epoch [93], train_loss: 0.0114, train_acc: 0.9960, val_loss: 2.0309, val_acc: 0.7510, val_precision: 0.7549, val_recall: 0.7510, val_f1: 0.7517
Epoch [94], train_loss: 0.0132, train_acc: 0.9958, val_loss: 2.0684, val_acc: 0.7511, val_precision: 0.7545, val_recall: 0.7511, val_f1: 0.7507
Epoch [95], train_loss: 0.0135, train_acc: 0.9957, val_loss: 2.0464, val_acc: 0.7626, val_precision: 0.7618, val_recall: 0.7626, val_f1: 0.7607
Epoch [96], train_loss: 0.0252, train_acc: 0.9916, val_loss: 1.9711, val_acc: 0.7437, val_precision: 0.7498, val_recall: 0.7437, val_f1: 0.7443
Epoch [97], train_loss: 0.0245, train_acc: 0.9915, val_loss: 1.9191, val_acc: 0.7451, val_precision: 0.7499, val_recall: 0.7451, val_f1: 0.7457
Epoch [98], train_loss: 0.0124, train_acc: 0.9958, val_loss: 2.0584, val_acc: 0.7517, val_precision: 0.7597, val_recall: 0.7517, val_f1: 0.7528
Epoch [99], train_loss: 0.0077, train_acc: 0.9975, val_loss: 2.2744, val_acc: 0.7498, val_precision: 0.7480, val_recall: 0.7498, val_f1: 0.7458
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.8104, val_acc: 0.7290, val_precision: 0.7333, val_recall: 0.7290, val_f1: 0.7256
Summary result of test set => last model => val_loss: 2.3786, val_acc: 0.7447, val_precision: 0.7430, val_recall: 0.7447, val_f1: 0.7415
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7293
--
confusion matrix
[[829  19  54  11   7   3  14   9  33  21]
 [ 18 907   5   2   0   3  11   3   8  43]
 [ 70   2 657  47  58  56  70  24   8   8]
 [ 38  11  95 426  58 189 132  28  11  12]
 [ 25   7 114  32 633  33  88  59   7   2]
 [ 17   9  61 103  38 672  58  33   5   4]
 [  9   5  37  25  21  10 884   5   2   2]
 [ 17   9  54  28  67  70  11 738   1   5]
 [ 92  44  16  14   5   5  11   1 791  21]
 [ 49 132  11  10   2   7  13  10  10 756]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.71      0.83      0.77      1000
  automobile       0.79      0.91      0.85      1000
        bird       0.60      0.66      0.62      1000
         cat       0.61      0.43      0.50      1000
        deer       0.71      0.63      0.67      1000
         dog       0.64      0.67      0.66      1000
        frog       0.68      0.88      0.77      1000
       horse       0.81      0.74      0.77      1000
        ship       0.90      0.79      0.84      1000
       truck       0.86      0.76      0.81      1000

    accuracy                           0.73     10000
   macro avg       0.73      0.73      0.73     10000
weighted avg       0.73      0.73      0.73     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7376
--
confusion matrix
[[400  15  29   1   4   0   5   6  15  13]
 [  9 470   2   1   2   2   1   3   5  17]
 [ 34   1 388  16  24  21  32   8   4   4]
 [ 15   5  60 192  13 102  55  14   9   6]
 [  8   3  68  18 300  18  28  25   1   2]
 [  6   3  28  56  21 341  37  17   2   3]
 [  2   3  30   9  10   8 437   1   4   3]
 [  9   3  30   8  26  33   4 378   3   6]
 [ 51  20  12   7   0   2   3   2 401   6]
 [ 17  70   4   4   2   1  14   5   3 381]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.73      0.82      0.77       488
  automobile       0.79      0.92      0.85       512
        bird       0.60      0.73      0.66       532
         cat       0.62      0.41      0.49       471
        deer       0.75      0.64      0.69       471
         dog       0.65      0.66      0.65       514
        frog       0.71      0.86      0.78       507
       horse       0.82      0.76      0.79       500
        ship       0.90      0.80      0.84       504
       truck       0.86      0.76      0.81       501

    accuracy                           0.74      5000
   macro avg       0.74      0.73      0.73      5000
weighted avg       0.74      0.74      0.73      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423979: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:29:15 2024
Job was executed on host(s) <hgn54>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:29:53 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:29:53 2024
Terminated at Wed Feb 28 08:36:34 2024
Results reported at Wed Feb 28 08:36:34 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_batch_1024_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_batch_1024_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1249.00 sec.
    Max Memory :                                 3390 MB
    Average Memory :                             3277.14 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6850.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   401 sec.
    Turnaround time :                            439 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_batch_1024_err_423979> for stderr output of this job.

