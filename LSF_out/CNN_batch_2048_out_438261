loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 2.3174, train_acc: 0.1130, val_loss: 2.2857, val_acc: 0.1304, val_precision: 0.0413, val_recall: 0.1304, val_f1: 0.0620
Epoch [1], train_loss: 2.2095, train_acc: 0.1597, val_loss: 2.1777, val_acc: 0.1805, val_precision: 0.1697, val_recall: 0.1805, val_f1: 0.1191
Epoch [2], train_loss: 2.0998, train_acc: 0.2278, val_loss: 2.0879, val_acc: 0.2343, val_precision: 0.2740, val_recall: 0.2343, val_f1: 0.1891
Epoch [3], train_loss: 1.9383, train_acc: 0.2868, val_loss: 1.8486, val_acc: 0.3193, val_precision: 0.3109, val_recall: 0.3193, val_f1: 0.2874
Epoch [4], train_loss: 1.7672, train_acc: 0.3430, val_loss: 1.7189, val_acc: 0.3602, val_precision: 0.3714, val_recall: 0.3602, val_f1: 0.3455
Epoch [5], train_loss: 1.7746, train_acc: 0.3526, val_loss: 1.7555, val_acc: 0.3520, val_precision: 0.3515, val_recall: 0.3520, val_f1: 0.3218
Epoch [6], train_loss: 1.6283, train_acc: 0.4009, val_loss: 1.5502, val_acc: 0.4282, val_precision: 0.4341, val_recall: 0.4282, val_f1: 0.4201
Epoch [7], train_loss: 1.4899, train_acc: 0.4509, val_loss: 1.4738, val_acc: 0.4548, val_precision: 0.4666, val_recall: 0.4548, val_f1: 0.4481
Epoch [8], train_loss: 1.3875, train_acc: 0.4922, val_loss: 1.3872, val_acc: 0.4790, val_precision: 0.4906, val_recall: 0.4790, val_f1: 0.4775
Epoch [9], train_loss: 1.3155, train_acc: 0.5216, val_loss: 1.2894, val_acc: 0.5262, val_precision: 0.5340, val_recall: 0.5262, val_f1: 0.5222
Epoch [10], train_loss: 1.2688, train_acc: 0.5410, val_loss: 1.2361, val_acc: 0.5492, val_precision: 0.5580, val_recall: 0.5492, val_f1: 0.5486
Epoch [11], train_loss: 1.1581, train_acc: 0.5833, val_loss: 1.2040, val_acc: 0.5634, val_precision: 0.5818, val_recall: 0.5634, val_f1: 0.5543
Epoch [12], train_loss: 1.0938, train_acc: 0.6090, val_loss: 1.1324, val_acc: 0.5925, val_precision: 0.6150, val_recall: 0.5925, val_f1: 0.5929
Epoch [13], train_loss: 1.0752, train_acc: 0.6157, val_loss: 1.0856, val_acc: 0.6115, val_precision: 0.6205, val_recall: 0.6115, val_f1: 0.6098
Epoch [14], train_loss: 0.9821, train_acc: 0.6516, val_loss: 1.0777, val_acc: 0.6256, val_precision: 0.6425, val_recall: 0.6256, val_f1: 0.6248
Epoch [15], train_loss: 0.9402, train_acc: 0.6671, val_loss: 1.0031, val_acc: 0.6507, val_precision: 0.6677, val_recall: 0.6507, val_f1: 0.6511
Epoch [16], train_loss: 0.8813, train_acc: 0.6859, val_loss: 0.9725, val_acc: 0.6513, val_precision: 0.6583, val_recall: 0.6513, val_f1: 0.6455
Epoch [17], train_loss: 0.8293, train_acc: 0.7074, val_loss: 0.9612, val_acc: 0.6677, val_precision: 0.6806, val_recall: 0.6677, val_f1: 0.6659
Epoch [18], train_loss: 0.7728, train_acc: 0.7267, val_loss: 0.9202, val_acc: 0.6786, val_precision: 0.6889, val_recall: 0.6786, val_f1: 0.6779
Epoch [19], train_loss: 0.7292, train_acc: 0.7442, val_loss: 0.8722, val_acc: 0.6896, val_precision: 0.7021, val_recall: 0.6896, val_f1: 0.6911
Epoch [20], train_loss: 0.6613, train_acc: 0.7695, val_loss: 0.8702, val_acc: 0.6952, val_precision: 0.7132, val_recall: 0.6952, val_f1: 0.6994
Epoch [21], train_loss: 0.6366, train_acc: 0.7761, val_loss: 0.8701, val_acc: 0.6981, val_precision: 0.7065, val_recall: 0.6981, val_f1: 0.6963
Epoch [22], train_loss: 0.6076, train_acc: 0.7867, val_loss: 0.8631, val_acc: 0.7033, val_precision: 0.7101, val_recall: 0.7033, val_f1: 0.7032
Epoch [23], train_loss: 0.5498, train_acc: 0.8096, val_loss: 0.8802, val_acc: 0.7064, val_precision: 0.7162, val_recall: 0.7064, val_f1: 0.7052
Epoch [24], train_loss: 0.4847, train_acc: 0.8319, val_loss: 0.8750, val_acc: 0.7110, val_precision: 0.7249, val_recall: 0.7110, val_f1: 0.7104
Epoch [25], train_loss: 0.4427, train_acc: 0.8458, val_loss: 0.9013, val_acc: 0.7125, val_precision: 0.7218, val_recall: 0.7125, val_f1: 0.7134
Epoch [26], train_loss: 0.3922, train_acc: 0.8643, val_loss: 0.9077, val_acc: 0.7149, val_precision: 0.7240, val_recall: 0.7149, val_f1: 0.7149
Epoch [27], train_loss: 0.3422, train_acc: 0.8830, val_loss: 0.9975, val_acc: 0.7173, val_precision: 0.7189, val_recall: 0.7173, val_f1: 0.7148
Epoch [28], train_loss: 0.3090, train_acc: 0.8927, val_loss: 1.0132, val_acc: 0.7093, val_precision: 0.7210, val_recall: 0.7093, val_f1: 0.7117
Epoch [29], train_loss: 0.2587, train_acc: 0.9124, val_loss: 1.0747, val_acc: 0.7179, val_precision: 0.7230, val_recall: 0.7179, val_f1: 0.7179
Epoch [30], train_loss: 0.2036, train_acc: 0.9328, val_loss: 1.1399, val_acc: 0.7173, val_precision: 0.7224, val_recall: 0.7173, val_f1: 0.7172
Epoch [31], train_loss: 0.1524, train_acc: 0.9499, val_loss: 1.2670, val_acc: 0.7289, val_precision: 0.7367, val_recall: 0.7289, val_f1: 0.7302
Epoch [32], train_loss: 0.1129, train_acc: 0.9631, val_loss: 1.5771, val_acc: 0.6932, val_precision: 0.7161, val_recall: 0.6932, val_f1: 0.6856
Epoch [33], train_loss: 0.3863, train_acc: 0.8649, val_loss: 1.0681, val_acc: 0.7136, val_precision: 0.7179, val_recall: 0.7136, val_f1: 0.7146
Epoch [34], train_loss: 0.1525, train_acc: 0.9499, val_loss: 1.4037, val_acc: 0.7153, val_precision: 0.7216, val_recall: 0.7153, val_f1: 0.7155
Epoch [35], train_loss: 0.0694, train_acc: 0.9781, val_loss: 1.5905, val_acc: 0.7224, val_precision: 0.7298, val_recall: 0.7224, val_f1: 0.7243
Epoch [36], train_loss: 0.0407, train_acc: 0.9894, val_loss: 1.7557, val_acc: 0.7226, val_precision: 0.7257, val_recall: 0.7226, val_f1: 0.7222
Epoch [37], train_loss: 0.0262, train_acc: 0.9935, val_loss: 1.8532, val_acc: 0.7228, val_precision: 0.7237, val_recall: 0.7228, val_f1: 0.7222
Epoch [38], train_loss: 0.0215, train_acc: 0.9946, val_loss: 2.1177, val_acc: 0.7028, val_precision: 0.7246, val_recall: 0.7028, val_f1: 0.7066
Epoch [39], train_loss: 0.0328, train_acc: 0.9897, val_loss: 2.0276, val_acc: 0.7139, val_precision: 0.7209, val_recall: 0.7139, val_f1: 0.7148
Epoch [40], train_loss: 0.0377, train_acc: 0.9877, val_loss: 1.9743, val_acc: 0.7166, val_precision: 0.7210, val_recall: 0.7166, val_f1: 0.7165
Epoch [41], train_loss: 0.0500, train_acc: 0.9833, val_loss: 1.9065, val_acc: 0.7154, val_precision: 0.7197, val_recall: 0.7154, val_f1: 0.7144
Epoch [42], train_loss: 0.0469, train_acc: 0.9844, val_loss: 1.9277, val_acc: 0.7133, val_precision: 0.7302, val_recall: 0.7133, val_f1: 0.7188
Epoch [43], train_loss: 0.0303, train_acc: 0.9909, val_loss: 2.0228, val_acc: 0.7218, val_precision: 0.7303, val_recall: 0.7218, val_f1: 0.7247
Epoch [44], train_loss: 0.0161, train_acc: 0.9957, val_loss: 2.1573, val_acc: 0.7104, val_precision: 0.7143, val_recall: 0.7104, val_f1: 0.7108
Epoch [45], train_loss: 0.0099, train_acc: 0.9978, val_loss: 2.1657, val_acc: 0.7265, val_precision: 0.7293, val_recall: 0.7265, val_f1: 0.7272
Epoch [46], train_loss: 0.0056, train_acc: 0.9987, val_loss: 2.3360, val_acc: 0.7219, val_precision: 0.7263, val_recall: 0.7219, val_f1: 0.7228
Epoch [47], train_loss: 0.0026, train_acc: 0.9997, val_loss: 2.4086, val_acc: 0.7234, val_precision: 0.7249, val_recall: 0.7234, val_f1: 0.7226
Epoch [48], train_loss: 0.0011, train_acc: 1.0000, val_loss: 2.4680, val_acc: 0.7270, val_precision: 0.7280, val_recall: 0.7270, val_f1: 0.7266
Epoch [49], train_loss: 0.0005, train_acc: 1.0000, val_loss: 2.5068, val_acc: 0.7246, val_precision: 0.7267, val_recall: 0.7246, val_f1: 0.7251
Epoch [50], train_loss: 0.0003, train_acc: 1.0000, val_loss: 2.5531, val_acc: 0.7254, val_precision: 0.7272, val_recall: 0.7254, val_f1: 0.7258
Epoch [51], train_loss: 0.0002, train_acc: 1.0000, val_loss: 2.5956, val_acc: 0.7267, val_precision: 0.7287, val_recall: 0.7267, val_f1: 0.7271
Epoch [52], train_loss: 0.0002, train_acc: 1.0000, val_loss: 2.6243, val_acc: 0.7273, val_precision: 0.7289, val_recall: 0.7273, val_f1: 0.7275
Epoch [53], train_loss: 0.0002, train_acc: 1.0000, val_loss: 2.6547, val_acc: 0.7274, val_precision: 0.7292, val_recall: 0.7274, val_f1: 0.7277
Epoch [54], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.6807, val_acc: 0.7269, val_precision: 0.7288, val_recall: 0.7269, val_f1: 0.7273
Epoch [55], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.7038, val_acc: 0.7261, val_precision: 0.7278, val_recall: 0.7261, val_f1: 0.7263
Epoch [56], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.7251, val_acc: 0.7278, val_precision: 0.7295, val_recall: 0.7278, val_f1: 0.7281
Epoch [57], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.7459, val_acc: 0.7274, val_precision: 0.7291, val_recall: 0.7274, val_f1: 0.7277
Epoch [58], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.7658, val_acc: 0.7274, val_precision: 0.7289, val_recall: 0.7274, val_f1: 0.7276
Epoch [59], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.7851, val_acc: 0.7263, val_precision: 0.7280, val_recall: 0.7263, val_f1: 0.7266
Epoch [60], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.8020, val_acc: 0.7270, val_precision: 0.7284, val_recall: 0.7270, val_f1: 0.7272
Epoch [61], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.8187, val_acc: 0.7265, val_precision: 0.7281, val_recall: 0.7265, val_f1: 0.7268
Epoch [62], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.8356, val_acc: 0.7272, val_precision: 0.7287, val_recall: 0.7272, val_f1: 0.7274
Epoch [63], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.8500, val_acc: 0.7273, val_precision: 0.7289, val_recall: 0.7273, val_f1: 0.7275
Epoch [64], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.8636, val_acc: 0.7272, val_precision: 0.7287, val_recall: 0.7272, val_f1: 0.7274
Epoch [65], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.8791, val_acc: 0.7276, val_precision: 0.7292, val_recall: 0.7276, val_f1: 0.7279
Epoch [66], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.8935, val_acc: 0.7277, val_precision: 0.7292, val_recall: 0.7277, val_f1: 0.7279
Epoch [67], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.9068, val_acc: 0.7276, val_precision: 0.7292, val_recall: 0.7276, val_f1: 0.7279
Epoch [68], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.9196, val_acc: 0.7289, val_precision: 0.7304, val_recall: 0.7289, val_f1: 0.7291
Epoch [69], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.9325, val_acc: 0.7294, val_precision: 0.7307, val_recall: 0.7294, val_f1: 0.7295
Epoch [70], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.9447, val_acc: 0.7289, val_precision: 0.7301, val_recall: 0.7289, val_f1: 0.7290
Epoch [71], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.9562, val_acc: 0.7291, val_precision: 0.7303, val_recall: 0.7291, val_f1: 0.7292
Epoch [72], train_loss: 0.0001, train_acc: 1.0000, val_loss: 2.9692, val_acc: 0.7294, val_precision: 0.7306, val_recall: 0.7294, val_f1: 0.7295
Epoch [73], train_loss: 0.0000, train_acc: 1.0000, val_loss: 2.9802, val_acc: 0.7297, val_precision: 0.7308, val_recall: 0.7297, val_f1: 0.7297
Epoch [74], train_loss: 0.0000, train_acc: 1.0000, val_loss: 2.9924, val_acc: 0.7297, val_precision: 0.7308, val_recall: 0.7297, val_f1: 0.7297
Epoch [75], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.0028, val_acc: 0.7294, val_precision: 0.7305, val_recall: 0.7294, val_f1: 0.7295
Epoch [76], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.0137, val_acc: 0.7297, val_precision: 0.7308, val_recall: 0.7297, val_f1: 0.7297
Epoch [77], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.0239, val_acc: 0.7294, val_precision: 0.7306, val_recall: 0.7294, val_f1: 0.7295
Epoch [78], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.0351, val_acc: 0.7293, val_precision: 0.7304, val_recall: 0.7293, val_f1: 0.7293
Epoch [79], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.0453, val_acc: 0.7295, val_precision: 0.7307, val_recall: 0.7295, val_f1: 0.7296
Epoch [80], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.0552, val_acc: 0.7294, val_precision: 0.7305, val_recall: 0.7294, val_f1: 0.7295
Epoch [81], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.0653, val_acc: 0.7295, val_precision: 0.7306, val_recall: 0.7295, val_f1: 0.7296
Epoch [82], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.0744, val_acc: 0.7291, val_precision: 0.7301, val_recall: 0.7291, val_f1: 0.7291
Epoch [83], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.0845, val_acc: 0.7291, val_precision: 0.7302, val_recall: 0.7291, val_f1: 0.7291
Epoch [84], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.0940, val_acc: 0.7294, val_precision: 0.7306, val_recall: 0.7294, val_f1: 0.7295
Epoch [85], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1030, val_acc: 0.7292, val_precision: 0.7303, val_recall: 0.7292, val_f1: 0.7292
Epoch [86], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1121, val_acc: 0.7289, val_precision: 0.7302, val_recall: 0.7289, val_f1: 0.7290
Epoch [87], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1214, val_acc: 0.7292, val_precision: 0.7304, val_recall: 0.7292, val_f1: 0.7293
Epoch [88], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1305, val_acc: 0.7291, val_precision: 0.7303, val_recall: 0.7291, val_f1: 0.7292
Epoch [89], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1388, val_acc: 0.7288, val_precision: 0.7301, val_recall: 0.7288, val_f1: 0.7289
Epoch [90], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1473, val_acc: 0.7288, val_precision: 0.7301, val_recall: 0.7288, val_f1: 0.7289
Epoch [91], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1554, val_acc: 0.7287, val_precision: 0.7299, val_recall: 0.7287, val_f1: 0.7287
Epoch [92], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1640, val_acc: 0.7286, val_precision: 0.7298, val_recall: 0.7286, val_f1: 0.7286
Epoch [93], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1726, val_acc: 0.7284, val_precision: 0.7297, val_recall: 0.7284, val_f1: 0.7285
Epoch [94], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1810, val_acc: 0.7283, val_precision: 0.7295, val_recall: 0.7283, val_f1: 0.7283
Epoch [95], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1887, val_acc: 0.7279, val_precision: 0.7291, val_recall: 0.7279, val_f1: 0.7280
Epoch [96], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.1965, val_acc: 0.7278, val_precision: 0.7289, val_recall: 0.7278, val_f1: 0.7278
Epoch [97], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.2050, val_acc: 0.7272, val_precision: 0.7284, val_recall: 0.7272, val_f1: 0.7273
Epoch [98], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.2126, val_acc: 0.7273, val_precision: 0.7285, val_recall: 0.7273, val_f1: 0.7274
Epoch [99], train_loss: 0.0000, train_acc: 1.0000, val_loss: 3.2199, val_acc: 0.7270, val_precision: 0.7282, val_recall: 0.7270, val_f1: 0.7271
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.8815, val_acc: 0.7019, val_precision: 0.7085, val_recall: 0.7019, val_f1: 0.7007
Summary result of test set => last model => val_loss: 3.2723, val_acc: 0.7223, val_precision: 0.7232, val_recall: 0.7223, val_f1: 0.7224
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7021
--
confusion matrix
[[699  28  57  30  33  14  23  13  57  46]
 [ 11 852   8   6   8   8  10   3  18  76]
 [ 49   4 475  74 158  89  83  50   4  14]
 [  8   8  44 476  92 241  70  48   3  10]
 [  7   2  37  46 719  45  65  72   6   1]
 [  3   1  30 121  68 676  28  66   2   5]
 [  4   2  19  54  58  38 806   8   3   8]
 [ 13   3  17  39  70  83   8 750   3  14]
 [ 63  40  19  28  13  17  15   4 780  21]
 [ 28  95   2  31   4  17  10  12  13 788]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.79      0.70      0.74      1000
  automobile       0.82      0.85      0.84      1000
        bird       0.67      0.47      0.56      1000
         cat       0.53      0.48      0.50      1000
        deer       0.59      0.72      0.65      1000
         dog       0.55      0.68      0.61      1000
        frog       0.72      0.81      0.76      1000
       horse       0.73      0.75      0.74      1000
        ship       0.88      0.78      0.83      1000
       truck       0.80      0.79      0.79      1000

    accuracy                           0.70     10000
   macro avg       0.71      0.70      0.70     10000
weighted avg       0.71      0.70      0.70     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7024
--
confusion matrix
[[356   8  30  18  16   9   8   5  20  18]
 [  4 447   3   7   2   2   1   2  10  34]
 [ 25   5 265  47  81  42  42  19   1   5]
 [  2   4  15 210  33 135  33  24   5  10]
 [  9   5  20  26 339   9  22  32   7   2]
 [  0   2  17  75  28 342  20  24   3   3]
 [  1   1  14  34  38  20 389   2   5   3]
 [  6   3  11  22  49  28   5 369   0   7]
 [ 31  19   3  16   6   5   5   3 403  13]
 [ 11  49   5  16   4   6   6   9   3 392]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.80      0.73      0.76       488
  automobile       0.82      0.87      0.85       512
        bird       0.69      0.50      0.58       532
         cat       0.45      0.45      0.45       471
        deer       0.57      0.72      0.64       471
         dog       0.57      0.67      0.62       514
        frog       0.73      0.77      0.75       507
       horse       0.75      0.74      0.75       500
        ship       0.88      0.80      0.84       504
       truck       0.80      0.78      0.79       501

    accuracy                           0.70      5000
   macro avg       0.71      0.70      0.70      5000
weighted avg       0.71      0.70      0.70      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 438261: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 10:46:07 2024
Job was executed on host(s) <hgn41>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 10:46:08 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 10:46:08 2024
Terminated at Wed Feb 28 10:53:38 2024
Results reported at Wed Feb 28 10:53:38 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_batch_2048_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_batch_2048_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.00001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_00001" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.002 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0.002" --model "CNN"


# Optimizer

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1380.00 sec.
    Max Memory :                                 3678 MB
    Average Memory :                             3349.23 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6562.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   451 sec.
    Turnaround time :                            451 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_batch_2048_err_438261> for stderr output of this job.

