loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.7740, train_acc: 0.3287, val_loss: 1.4066, val_acc: 0.4775, val_precision: 0.5092, val_recall: 0.4775, val_f1: 0.4573
Epoch [1], train_loss: 1.2325, train_acc: 0.5491, val_loss: 1.0866, val_acc: 0.6104, val_precision: 0.6582, val_recall: 0.6104, val_f1: 0.6124
Epoch [2], train_loss: 0.9942, train_acc: 0.6436, val_loss: 0.9668, val_acc: 0.6521, val_precision: 0.7021, val_recall: 0.6521, val_f1: 0.6489
Epoch [3], train_loss: 0.8366, train_acc: 0.7054, val_loss: 0.8610, val_acc: 0.6913, val_precision: 0.7266, val_recall: 0.6913, val_f1: 0.6886
Epoch [4], train_loss: 0.7352, train_acc: 0.7415, val_loss: 0.8826, val_acc: 0.6895, val_precision: 0.7508, val_recall: 0.6895, val_f1: 0.6890
Epoch [5], train_loss: 0.6464, train_acc: 0.7735, val_loss: 0.8212, val_acc: 0.7160, val_precision: 0.7605, val_recall: 0.7160, val_f1: 0.7179
Epoch [6], train_loss: 0.5729, train_acc: 0.7977, val_loss: 0.7902, val_acc: 0.7286, val_precision: 0.7602, val_recall: 0.7286, val_f1: 0.7300
Epoch [7], train_loss: 0.5007, train_acc: 0.8226, val_loss: 0.8405, val_acc: 0.7267, val_precision: 0.7577, val_recall: 0.7267, val_f1: 0.7258
Epoch [8], train_loss: 0.4309, train_acc: 0.8463, val_loss: 0.9192, val_acc: 0.7261, val_precision: 0.7572, val_recall: 0.7261, val_f1: 0.7225
Epoch [9], train_loss: 0.3836, train_acc: 0.8634, val_loss: 0.9449, val_acc: 0.7261, val_precision: 0.7607, val_recall: 0.7261, val_f1: 0.7269
Epoch [10], train_loss: 0.3349, train_acc: 0.8824, val_loss: 0.9736, val_acc: 0.7308, val_precision: 0.7612, val_recall: 0.7308, val_f1: 0.7296
Epoch [11], train_loss: 0.2973, train_acc: 0.8948, val_loss: 1.0030, val_acc: 0.7189, val_precision: 0.7463, val_recall: 0.7189, val_f1: 0.7153
Epoch [12], train_loss: 0.2668, train_acc: 0.9065, val_loss: 1.1184, val_acc: 0.7338, val_precision: 0.7642, val_recall: 0.7338, val_f1: 0.7321
Epoch [13], train_loss: 0.2406, train_acc: 0.9160, val_loss: 1.2441, val_acc: 0.7207, val_precision: 0.7570, val_recall: 0.7207, val_f1: 0.7205
Epoch [14], train_loss: 0.2289, train_acc: 0.9212, val_loss: 1.2727, val_acc: 0.7116, val_precision: 0.7481, val_recall: 0.7116, val_f1: 0.7138
Epoch [15], train_loss: 0.2126, train_acc: 0.9264, val_loss: 1.3165, val_acc: 0.7199, val_precision: 0.7559, val_recall: 0.7199, val_f1: 0.7226
Epoch [16], train_loss: 0.1978, train_acc: 0.9326, val_loss: 1.3027, val_acc: 0.7296, val_precision: 0.7667, val_recall: 0.7296, val_f1: 0.7318
Epoch [17], train_loss: 0.1854, train_acc: 0.9373, val_loss: 1.3346, val_acc: 0.7257, val_precision: 0.7613, val_recall: 0.7257, val_f1: 0.7290
Epoch [18], train_loss: 0.1761, train_acc: 0.9402, val_loss: 1.3494, val_acc: 0.7269, val_precision: 0.7601, val_recall: 0.7269, val_f1: 0.7275
Epoch [19], train_loss: 0.1715, train_acc: 0.9425, val_loss: 1.5332, val_acc: 0.7278, val_precision: 0.7573, val_recall: 0.7278, val_f1: 0.7260
Epoch [20], train_loss: 0.1798, train_acc: 0.9415, val_loss: 1.4505, val_acc: 0.7271, val_precision: 0.7647, val_recall: 0.7271, val_f1: 0.7308
Epoch [21], train_loss: 0.1661, train_acc: 0.9448, val_loss: 1.5073, val_acc: 0.7150, val_precision: 0.7467, val_recall: 0.7150, val_f1: 0.7144
Epoch [22], train_loss: 0.1648, train_acc: 0.9460, val_loss: 1.3542, val_acc: 0.7203, val_precision: 0.7522, val_recall: 0.7203, val_f1: 0.7196
Epoch [23], train_loss: 0.1594, train_acc: 0.9486, val_loss: 1.5750, val_acc: 0.7215, val_precision: 0.7543, val_recall: 0.7215, val_f1: 0.7213
Epoch [24], train_loss: 0.1462, train_acc: 0.9530, val_loss: 1.5636, val_acc: 0.7245, val_precision: 0.7603, val_recall: 0.7245, val_f1: 0.7249
Epoch [25], train_loss: 0.1487, train_acc: 0.9520, val_loss: 1.6714, val_acc: 0.7168, val_precision: 0.7500, val_recall: 0.7168, val_f1: 0.7169
Epoch [26], train_loss: 0.1564, train_acc: 0.9498, val_loss: 1.5155, val_acc: 0.7178, val_precision: 0.7475, val_recall: 0.7178, val_f1: 0.7159
Epoch [27], train_loss: 0.1436, train_acc: 0.9545, val_loss: 1.7178, val_acc: 0.7271, val_precision: 0.7612, val_recall: 0.7271, val_f1: 0.7263
Epoch [28], train_loss: 0.1453, train_acc: 0.9533, val_loss: 1.6542, val_acc: 0.7215, val_precision: 0.7564, val_recall: 0.7215, val_f1: 0.7225
Epoch [29], train_loss: 0.1444, train_acc: 0.9556, val_loss: 1.5060, val_acc: 0.7203, val_precision: 0.7529, val_recall: 0.7203, val_f1: 0.7185
Epoch [30], train_loss: 0.1398, train_acc: 0.9563, val_loss: 1.7703, val_acc: 0.7259, val_precision: 0.7591, val_recall: 0.7259, val_f1: 0.7253
Epoch [31], train_loss: 0.1360, train_acc: 0.9584, val_loss: 1.7685, val_acc: 0.7251, val_precision: 0.7656, val_recall: 0.7251, val_f1: 0.7275
Epoch [32], train_loss: 0.1405, train_acc: 0.9563, val_loss: 1.7960, val_acc: 0.7186, val_precision: 0.7497, val_recall: 0.7186, val_f1: 0.7176
Epoch [33], train_loss: 0.1449, train_acc: 0.9548, val_loss: 1.7747, val_acc: 0.7277, val_precision: 0.7610, val_recall: 0.7277, val_f1: 0.7280
Epoch [34], train_loss: 0.1376, train_acc: 0.9578, val_loss: 1.8415, val_acc: 0.7188, val_precision: 0.7528, val_recall: 0.7188, val_f1: 0.7180
Epoch [35], train_loss: 0.1309, train_acc: 0.9597, val_loss: 1.8698, val_acc: 0.7197, val_precision: 0.7473, val_recall: 0.7197, val_f1: 0.7176
Epoch [36], train_loss: 0.1320, train_acc: 0.9599, val_loss: 1.8664, val_acc: 0.7146, val_precision: 0.7549, val_recall: 0.7146, val_f1: 0.7151
Epoch [37], train_loss: 0.1327, train_acc: 0.9595, val_loss: 1.8489, val_acc: 0.7239, val_precision: 0.7547, val_recall: 0.7239, val_f1: 0.7228
Epoch [38], train_loss: 0.1391, train_acc: 0.9578, val_loss: 1.8342, val_acc: 0.7322, val_precision: 0.7619, val_recall: 0.7322, val_f1: 0.7315
Epoch [39], train_loss: 0.1342, train_acc: 0.9600, val_loss: 1.7337, val_acc: 0.7245, val_precision: 0.7503, val_recall: 0.7245, val_f1: 0.7228
Epoch [40], train_loss: 0.1242, train_acc: 0.9628, val_loss: 1.8173, val_acc: 0.7278, val_precision: 0.7632, val_recall: 0.7278, val_f1: 0.7290
Epoch [41], train_loss: 0.1179, train_acc: 0.9639, val_loss: 1.9927, val_acc: 0.7184, val_precision: 0.7484, val_recall: 0.7184, val_f1: 0.7165
Epoch [42], train_loss: 0.1370, train_acc: 0.9594, val_loss: 1.8265, val_acc: 0.7102, val_precision: 0.7493, val_recall: 0.7102, val_f1: 0.7113
Epoch [43], train_loss: 0.1267, train_acc: 0.9625, val_loss: 2.0281, val_acc: 0.7205, val_precision: 0.7514, val_recall: 0.7205, val_f1: 0.7191
Epoch [44], train_loss: 0.1376, train_acc: 0.9605, val_loss: 1.9121, val_acc: 0.7211, val_precision: 0.7508, val_recall: 0.7211, val_f1: 0.7179
Epoch [45], train_loss: 0.1254, train_acc: 0.9632, val_loss: 1.9655, val_acc: 0.7110, val_precision: 0.7464, val_recall: 0.7110, val_f1: 0.7108
Epoch [46], train_loss: 0.1189, train_acc: 0.9644, val_loss: 2.1730, val_acc: 0.7298, val_precision: 0.7601, val_recall: 0.7298, val_f1: 0.7282
Epoch [47], train_loss: 0.1280, train_acc: 0.9633, val_loss: 1.8508, val_acc: 0.7188, val_precision: 0.7471, val_recall: 0.7188, val_f1: 0.7163
Epoch [48], train_loss: 0.1220, train_acc: 0.9652, val_loss: 1.9448, val_acc: 0.7176, val_precision: 0.7446, val_recall: 0.7176, val_f1: 0.7160
Epoch [49], train_loss: 0.1067, train_acc: 0.9682, val_loss: 2.1123, val_acc: 0.7253, val_precision: 0.7538, val_recall: 0.7253, val_f1: 0.7236
Epoch [50], train_loss: 0.1223, train_acc: 0.9647, val_loss: 2.0375, val_acc: 0.7265, val_precision: 0.7583, val_recall: 0.7265, val_f1: 0.7261
Epoch [51], train_loss: 0.1154, train_acc: 0.9681, val_loss: 1.8813, val_acc: 0.7273, val_precision: 0.7616, val_recall: 0.7273, val_f1: 0.7271
Epoch [52], train_loss: 0.1262, train_acc: 0.9638, val_loss: 1.9584, val_acc: 0.7140, val_precision: 0.7468, val_recall: 0.7140, val_f1: 0.7140
Epoch [53], train_loss: 0.1321, train_acc: 0.9635, val_loss: 1.9515, val_acc: 0.7306, val_precision: 0.7616, val_recall: 0.7306, val_f1: 0.7312
Epoch [54], train_loss: 0.1086, train_acc: 0.9687, val_loss: 1.8563, val_acc: 0.7172, val_precision: 0.7484, val_recall: 0.7172, val_f1: 0.7166
Epoch [55], train_loss: 0.1106, train_acc: 0.9688, val_loss: 2.0984, val_acc: 0.7275, val_precision: 0.7632, val_recall: 0.7275, val_f1: 0.7304
Epoch [56], train_loss: 0.1243, train_acc: 0.9655, val_loss: 1.9015, val_acc: 0.7239, val_precision: 0.7556, val_recall: 0.7239, val_f1: 0.7235
Epoch [57], train_loss: 0.1183, train_acc: 0.9667, val_loss: 2.0905, val_acc: 0.7162, val_precision: 0.7506, val_recall: 0.7162, val_f1: 0.7175
Epoch [58], train_loss: 0.1200, train_acc: 0.9653, val_loss: 1.9681, val_acc: 0.7239, val_precision: 0.7517, val_recall: 0.7239, val_f1: 0.7220
Epoch [59], train_loss: 0.0957, train_acc: 0.9732, val_loss: 1.9600, val_acc: 0.7282, val_precision: 0.7616, val_recall: 0.7282, val_f1: 0.7290
Epoch [60], train_loss: 0.1218, train_acc: 0.9668, val_loss: 2.1812, val_acc: 0.7277, val_precision: 0.7623, val_recall: 0.7277, val_f1: 0.7269
Epoch [61], train_loss: 0.1085, train_acc: 0.9699, val_loss: 2.1541, val_acc: 0.7235, val_precision: 0.7526, val_recall: 0.7235, val_f1: 0.7214
Epoch [62], train_loss: 0.1040, train_acc: 0.9715, val_loss: 2.3727, val_acc: 0.7008, val_precision: 0.7400, val_recall: 0.7008, val_f1: 0.7018
Epoch [63], train_loss: 0.1204, train_acc: 0.9669, val_loss: 2.1899, val_acc: 0.7233, val_precision: 0.7559, val_recall: 0.7233, val_f1: 0.7234
Epoch [64], train_loss: 0.1062, train_acc: 0.9703, val_loss: 2.0399, val_acc: 0.7253, val_precision: 0.7585, val_recall: 0.7253, val_f1: 0.7266
Epoch [65], train_loss: 0.1095, train_acc: 0.9700, val_loss: 1.9739, val_acc: 0.7184, val_precision: 0.7539, val_recall: 0.7184, val_f1: 0.7198
Epoch [66], train_loss: 0.0995, train_acc: 0.9725, val_loss: 2.1454, val_acc: 0.7278, val_precision: 0.7650, val_recall: 0.7278, val_f1: 0.7302
Epoch [67], train_loss: 0.1051, train_acc: 0.9723, val_loss: 1.8968, val_acc: 0.7138, val_precision: 0.7428, val_recall: 0.7138, val_f1: 0.7096
Epoch [68], train_loss: 0.1045, train_acc: 0.9712, val_loss: 1.9054, val_acc: 0.7160, val_precision: 0.7525, val_recall: 0.7160, val_f1: 0.7190
Epoch [69], train_loss: 0.1110, train_acc: 0.9701, val_loss: 1.8963, val_acc: 0.7265, val_precision: 0.7614, val_recall: 0.7265, val_f1: 0.7268
Epoch [70], train_loss: 0.0920, train_acc: 0.9758, val_loss: 2.2364, val_acc: 0.7255, val_precision: 0.7581, val_recall: 0.7255, val_f1: 0.7254
Epoch [71], train_loss: 0.1113, train_acc: 0.9702, val_loss: 1.9404, val_acc: 0.7132, val_precision: 0.7531, val_recall: 0.7132, val_f1: 0.7148
Epoch [72], train_loss: 0.0922, train_acc: 0.9759, val_loss: 2.2441, val_acc: 0.7138, val_precision: 0.7525, val_recall: 0.7138, val_f1: 0.7127
Epoch [73], train_loss: 0.1010, train_acc: 0.9732, val_loss: 2.0019, val_acc: 0.7267, val_precision: 0.7597, val_recall: 0.7267, val_f1: 0.7276
Epoch [74], train_loss: 0.1055, train_acc: 0.9720, val_loss: 2.0293, val_acc: 0.7160, val_precision: 0.7472, val_recall: 0.7160, val_f1: 0.7160
Epoch [75], train_loss: 0.1045, train_acc: 0.9716, val_loss: 1.8952, val_acc: 0.7233, val_precision: 0.7545, val_recall: 0.7233, val_f1: 0.7246
Epoch [76], train_loss: 0.0896, train_acc: 0.9765, val_loss: 1.8976, val_acc: 0.7245, val_precision: 0.7587, val_recall: 0.7245, val_f1: 0.7260
Epoch [77], train_loss: 0.1072, train_acc: 0.9729, val_loss: 1.9009, val_acc: 0.7269, val_precision: 0.7594, val_recall: 0.7269, val_f1: 0.7273
Epoch [78], train_loss: 0.0951, train_acc: 0.9750, val_loss: 2.0290, val_acc: 0.7140, val_precision: 0.7470, val_recall: 0.7140, val_f1: 0.7155
Epoch [79], train_loss: 0.0840, train_acc: 0.9779, val_loss: 2.0168, val_acc: 0.7269, val_precision: 0.7600, val_recall: 0.7269, val_f1: 0.7271
Epoch [80], train_loss: 0.1082, train_acc: 0.9710, val_loss: 2.2923, val_acc: 0.7241, val_precision: 0.7558, val_recall: 0.7241, val_f1: 0.7232
Epoch [81], train_loss: 0.1005, train_acc: 0.9747, val_loss: 2.0123, val_acc: 0.7184, val_precision: 0.7594, val_recall: 0.7184, val_f1: 0.7219
Epoch [82], train_loss: 0.0821, train_acc: 0.9779, val_loss: 1.9247, val_acc: 0.7369, val_precision: 0.7700, val_recall: 0.7369, val_f1: 0.7381
Epoch [83], train_loss: 0.0959, train_acc: 0.9751, val_loss: 2.1737, val_acc: 0.7328, val_precision: 0.7633, val_recall: 0.7328, val_f1: 0.7317
Epoch [84], train_loss: 0.0848, train_acc: 0.9782, val_loss: 1.9218, val_acc: 0.7259, val_precision: 0.7577, val_recall: 0.7259, val_f1: 0.7259
Epoch [85], train_loss: 0.1053, train_acc: 0.9732, val_loss: 2.0188, val_acc: 0.7233, val_precision: 0.7548, val_recall: 0.7233, val_f1: 0.7229
Epoch [86], train_loss: 0.1092, train_acc: 0.9726, val_loss: 2.2519, val_acc: 0.7217, val_precision: 0.7550, val_recall: 0.7217, val_f1: 0.7217
Epoch [87], train_loss: 0.0741, train_acc: 0.9811, val_loss: 2.0643, val_acc: 0.7296, val_precision: 0.7639, val_recall: 0.7296, val_f1: 0.7312
Epoch [88], train_loss: 0.0821, train_acc: 0.9788, val_loss: 2.2551, val_acc: 0.7193, val_precision: 0.7620, val_recall: 0.7193, val_f1: 0.7220
Epoch [89], train_loss: 0.0923, train_acc: 0.9765, val_loss: 2.0645, val_acc: 0.7332, val_precision: 0.7609, val_recall: 0.7332, val_f1: 0.7327
Epoch [90], train_loss: 0.0849, train_acc: 0.9789, val_loss: 2.0886, val_acc: 0.7373, val_precision: 0.7641, val_recall: 0.7373, val_f1: 0.7355
Epoch [91], train_loss: 0.0807, train_acc: 0.9793, val_loss: 2.0947, val_acc: 0.7100, val_precision: 0.7452, val_recall: 0.7100, val_f1: 0.7078
Epoch [92], train_loss: 0.0914, train_acc: 0.9762, val_loss: 1.7997, val_acc: 0.7168, val_precision: 0.7489, val_recall: 0.7168, val_f1: 0.7161
Epoch [93], train_loss: 0.0814, train_acc: 0.9793, val_loss: 2.0013, val_acc: 0.7211, val_precision: 0.7592, val_recall: 0.7211, val_f1: 0.7215
Epoch [94], train_loss: 0.0835, train_acc: 0.9799, val_loss: 2.0909, val_acc: 0.7334, val_precision: 0.7638, val_recall: 0.7334, val_f1: 0.7338
Epoch [95], train_loss: 0.0880, train_acc: 0.9790, val_loss: 2.0023, val_acc: 0.7031, val_precision: 0.7419, val_recall: 0.7031, val_f1: 0.7024
Epoch [96], train_loss: 0.0831, train_acc: 0.9795, val_loss: 1.9709, val_acc: 0.7265, val_precision: 0.7600, val_recall: 0.7265, val_f1: 0.7260
Epoch [97], train_loss: 0.0913, train_acc: 0.9788, val_loss: 2.0194, val_acc: 0.7221, val_precision: 0.7578, val_recall: 0.7221, val_f1: 0.7228
Epoch [98], train_loss: 0.0797, train_acc: 0.9804, val_loss: 2.1883, val_acc: 0.7318, val_precision: 0.7673, val_recall: 0.7318, val_f1: 0.7339
Epoch [99], train_loss: 0.0869, train_acc: 0.9798, val_loss: 1.9740, val_acc: 0.7221, val_precision: 0.7601, val_recall: 0.7221, val_f1: 0.7241
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.7930, val_acc: 0.7335, val_precision: 0.7620, val_recall: 0.7335, val_f1: 0.7341
Summary result of test set => last model => val_loss: 2.1868, val_acc: 0.7131, val_precision: 0.7451, val_recall: 0.7131, val_f1: 0.7128
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7331
--
confusion matrix
[[763  12  68  21  29   6   4   7  53  37]
 [ 13 873   1   7   2   4   2   3  15  80]
 [ 58   6 663  53  98  54  36  17   8   7]
 [ 31   8  84 559  74 145  46  31   9  13]
 [ 17   4  89  54 694  38  34  61   7   2]
 [ 10   5  63 181  48 628  17  37   5   6]
 [  7  10  67  78  55  19 740   6  11   7]
 [ 15   5  24  37  87  60   5 744   8  15]
 [ 80  40  14   9  10   3   5   1 826  12]
 [ 35  47   6  22   8   6   4  13  18 841]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.74      0.76      0.75      1000
  automobile       0.86      0.87      0.87      1000
        bird       0.61      0.66      0.64      1000
         cat       0.55      0.56      0.55      1000
        deer       0.63      0.69      0.66      1000
         dog       0.65      0.63      0.64      1000
        frog       0.83      0.74      0.78      1000
       horse       0.81      0.74      0.78      1000
        ship       0.86      0.83      0.84      1000
       truck       0.82      0.84      0.83      1000

    accuracy                           0.73     10000
   macro avg       0.74      0.73      0.73     10000
weighted avg       0.74      0.73      0.73     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7270
--
confusion matrix
[[359  11  36   8  12   3   2   4  35  18]
 [  6 453   2   1   0   3   1   1   8  37]
 [ 26   2 353  21  55  27  17  16   9   6]
 [ 11   5  43 247  26  82  27  14   7   9]
 [  8   0  36  25 343  16  14  25   4   0]
 [  4   2  32 108  30 303  10  17   5   3]
 [  5   8  40  31  31  11 368   3   6   4]
 [  8   1  19  21  39  27   0 375   4   6]
 [ 47  11   7   6   3   1   1   0 419   9]
 [  7  30   0  14   4   4   2   9  16 415]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.75      0.74      0.74       488
  automobile       0.87      0.88      0.88       512
        bird       0.62      0.66      0.64       532
         cat       0.51      0.52      0.52       471
        deer       0.63      0.73      0.68       471
         dog       0.64      0.59      0.61       514
        frog       0.83      0.73      0.78       507
       horse       0.81      0.75      0.78       500
        ship       0.82      0.83      0.82       504
       truck       0.82      0.83      0.82       501

    accuracy                           0.73      5000
   macro avg       0.73      0.73      0.73      5000
weighted avg       0.73      0.73      0.73      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423953: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:27:40 2024
Job was executed on host(s) <hgn54>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:29:29 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:29:29 2024
Terminated at Wed Feb 28 08:46:37 2024
Results reported at Wed Feb 28 08:46:37 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_batch_32_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_batch_32_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2260.00 sec.
    Max Memory :                                 3435 MB
    Average Memory :                             3314.74 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6805.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                76
    Run time :                                   1035 sec.
    Turnaround time :                            1137 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_batch_32_err_423953> for stderr output of this job.

