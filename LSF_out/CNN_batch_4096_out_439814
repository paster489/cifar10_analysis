loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 2.3008, train_acc: 0.1029, val_loss: 2.2993, val_acc: 0.1410, val_precision: 0.0392, val_recall: 0.1410, val_f1: 0.0602
Epoch [1], train_loss: 2.2940, train_acc: 0.1254, val_loss: 2.2883, val_acc: 0.1130, val_precision: 0.0609, val_recall: 0.1130, val_f1: 0.0382
Epoch [2], train_loss: 2.2500, train_acc: 0.1451, val_loss: 2.2104, val_acc: 0.1592, val_precision: 0.0831, val_recall: 0.1592, val_f1: 0.1008
Epoch [3], train_loss: 2.1684, train_acc: 0.1829, val_loss: 2.1169, val_acc: 0.2264, val_precision: 0.2111, val_recall: 0.2264, val_f1: 0.1873
Epoch [4], train_loss: 2.1076, train_acc: 0.2142, val_loss: 2.0666, val_acc: 0.2402, val_precision: 0.2507, val_recall: 0.2402, val_f1: 0.1870
Epoch [5], train_loss: 2.0061, train_acc: 0.2603, val_loss: 2.0052, val_acc: 0.2644, val_precision: 0.2628, val_recall: 0.2644, val_f1: 0.2185
Epoch [6], train_loss: 1.9313, train_acc: 0.2854, val_loss: 1.8747, val_acc: 0.2894, val_precision: 0.2438, val_recall: 0.2894, val_f1: 0.2344
Epoch [7], train_loss: 1.8694, train_acc: 0.3115, val_loss: 1.9187, val_acc: 0.3168, val_precision: 0.3587, val_recall: 0.3168, val_f1: 0.2705
Epoch [8], train_loss: 1.8669, train_acc: 0.3200, val_loss: 1.8026, val_acc: 0.3320, val_precision: 0.3205, val_recall: 0.3320, val_f1: 0.2977
Epoch [9], train_loss: 1.7349, train_acc: 0.3552, val_loss: 1.6853, val_acc: 0.3712, val_precision: 0.3513, val_recall: 0.3712, val_f1: 0.3413
Epoch [10], train_loss: 1.6435, train_acc: 0.3951, val_loss: 1.5914, val_acc: 0.4108, val_precision: 0.4093, val_recall: 0.4108, val_f1: 0.3992
Epoch [11], train_loss: 1.5637, train_acc: 0.4215, val_loss: 1.5493, val_acc: 0.4272, val_precision: 0.4301, val_recall: 0.4272, val_f1: 0.4071
Epoch [12], train_loss: 1.5057, train_acc: 0.4412, val_loss: 1.5377, val_acc: 0.4310, val_precision: 0.4629, val_recall: 0.4310, val_f1: 0.4231
Epoch [13], train_loss: 1.4760, train_acc: 0.4514, val_loss: 1.4537, val_acc: 0.4628, val_precision: 0.4649, val_recall: 0.4628, val_f1: 0.4511
Epoch [14], train_loss: 1.4229, train_acc: 0.4749, val_loss: 1.4346, val_acc: 0.4776, val_precision: 0.4911, val_recall: 0.4776, val_f1: 0.4752
Epoch [15], train_loss: 1.3772, train_acc: 0.4920, val_loss: 1.3600, val_acc: 0.4996, val_precision: 0.5044, val_recall: 0.4996, val_f1: 0.4910
Epoch [16], train_loss: 1.4073, train_acc: 0.4868, val_loss: 1.3893, val_acc: 0.4854, val_precision: 0.4897, val_recall: 0.4854, val_f1: 0.4788
Epoch [17], train_loss: 1.3434, train_acc: 0.5060, val_loss: 1.3317, val_acc: 0.5120, val_precision: 0.5193, val_recall: 0.5120, val_f1: 0.5081
Epoch [18], train_loss: 1.2750, train_acc: 0.5351, val_loss: 1.2773, val_acc: 0.5306, val_precision: 0.5413, val_recall: 0.5306, val_f1: 0.5317
Epoch [19], train_loss: 1.2192, train_acc: 0.5559, val_loss: 1.2380, val_acc: 0.5458, val_precision: 0.5450, val_recall: 0.5458, val_f1: 0.5390
Epoch [20], train_loss: 1.2526, train_acc: 0.5478, val_loss: 1.2715, val_acc: 0.5390, val_precision: 0.5540, val_recall: 0.5390, val_f1: 0.5360
Epoch [21], train_loss: 1.1823, train_acc: 0.5713, val_loss: 1.1913, val_acc: 0.5636, val_precision: 0.5733, val_recall: 0.5636, val_f1: 0.5622
Epoch [22], train_loss: 1.1422, train_acc: 0.5899, val_loss: 1.2138, val_acc: 0.5622, val_precision: 0.5796, val_recall: 0.5622, val_f1: 0.5591
Epoch [23], train_loss: 1.1728, train_acc: 0.5783, val_loss: 1.1753, val_acc: 0.5816, val_precision: 0.5849, val_recall: 0.5816, val_f1: 0.5782
Epoch [24], train_loss: 1.0840, train_acc: 0.6110, val_loss: 1.1019, val_acc: 0.6004, val_precision: 0.6038, val_recall: 0.6004, val_f1: 0.5995
Epoch [25], train_loss: 1.0256, train_acc: 0.6338, val_loss: 1.1415, val_acc: 0.5932, val_precision: 0.6195, val_recall: 0.5932, val_f1: 0.5983
Epoch [26], train_loss: 1.0132, train_acc: 0.6361, val_loss: 1.0545, val_acc: 0.6230, val_precision: 0.6374, val_recall: 0.6230, val_f1: 0.6249
Epoch [27], train_loss: 0.9659, train_acc: 0.6564, val_loss: 1.0304, val_acc: 0.6252, val_precision: 0.6361, val_recall: 0.6252, val_f1: 0.6247
Epoch [28], train_loss: 0.9150, train_acc: 0.6753, val_loss: 0.9835, val_acc: 0.6410, val_precision: 0.6509, val_recall: 0.6410, val_f1: 0.6433
Epoch [29], train_loss: 0.9052, train_acc: 0.6804, val_loss: 0.9798, val_acc: 0.6536, val_precision: 0.6673, val_recall: 0.6536, val_f1: 0.6524
Epoch [30], train_loss: 0.8893, train_acc: 0.6849, val_loss: 1.0060, val_acc: 0.6400, val_precision: 0.6659, val_recall: 0.6400, val_f1: 0.6395
Epoch [31], train_loss: 0.8450, train_acc: 0.7011, val_loss: 0.9258, val_acc: 0.6662, val_precision: 0.6707, val_recall: 0.6662, val_f1: 0.6641
Epoch [32], train_loss: 0.7927, train_acc: 0.7203, val_loss: 0.9220, val_acc: 0.6768, val_precision: 0.6803, val_recall: 0.6768, val_f1: 0.6731
Epoch [33], train_loss: 0.7740, train_acc: 0.7272, val_loss: 0.9065, val_acc: 0.6730, val_precision: 0.6825, val_recall: 0.6730, val_f1: 0.6739
Epoch [34], train_loss: 0.7306, train_acc: 0.7433, val_loss: 0.8837, val_acc: 0.6860, val_precision: 0.6910, val_recall: 0.6860, val_f1: 0.6846
Epoch [35], train_loss: 0.6896, train_acc: 0.7589, val_loss: 0.8913, val_acc: 0.6836, val_precision: 0.6909, val_recall: 0.6836, val_f1: 0.6820
Epoch [36], train_loss: 0.7042, train_acc: 0.7517, val_loss: 0.9662, val_acc: 0.6592, val_precision: 0.6808, val_recall: 0.6592, val_f1: 0.6597
Epoch [37], train_loss: 0.7044, train_acc: 0.7530, val_loss: 0.8698, val_acc: 0.6926, val_precision: 0.7071, val_recall: 0.6926, val_f1: 0.6954
Epoch [38], train_loss: 0.6381, train_acc: 0.7788, val_loss: 0.9111, val_acc: 0.6904, val_precision: 0.7124, val_recall: 0.6904, val_f1: 0.6915
Epoch [39], train_loss: 0.6141, train_acc: 0.7856, val_loss: 0.8439, val_acc: 0.7056, val_precision: 0.7053, val_recall: 0.7056, val_f1: 0.7028
Epoch [40], train_loss: 0.5730, train_acc: 0.8033, val_loss: 0.8375, val_acc: 0.7092, val_precision: 0.7125, val_recall: 0.7092, val_f1: 0.7082
Epoch [41], train_loss: 0.5419, train_acc: 0.8113, val_loss: 0.8723, val_acc: 0.7002, val_precision: 0.7141, val_recall: 0.7002, val_f1: 0.6994
Epoch [42], train_loss: 0.5232, train_acc: 0.8174, val_loss: 0.8697, val_acc: 0.7116, val_precision: 0.7108, val_recall: 0.7116, val_f1: 0.7072
Epoch [43], train_loss: 0.5182, train_acc: 0.8187, val_loss: 0.8982, val_acc: 0.7004, val_precision: 0.7143, val_recall: 0.7004, val_f1: 0.7022
Epoch [44], train_loss: 0.5217, train_acc: 0.8184, val_loss: 0.8479, val_acc: 0.7172, val_precision: 0.7337, val_recall: 0.7172, val_f1: 0.7192
Epoch [45], train_loss: 0.4926, train_acc: 0.8280, val_loss: 0.9087, val_acc: 0.7026, val_precision: 0.7226, val_recall: 0.7026, val_f1: 0.7025
Epoch [46], train_loss: 0.4409, train_acc: 0.8479, val_loss: 0.8850, val_acc: 0.7152, val_precision: 0.7269, val_recall: 0.7152, val_f1: 0.7142
Epoch [47], train_loss: 0.3980, train_acc: 0.8623, val_loss: 0.8708, val_acc: 0.7254, val_precision: 0.7316, val_recall: 0.7254, val_f1: 0.7267
Epoch [48], train_loss: 0.3621, train_acc: 0.8767, val_loss: 0.9064, val_acc: 0.7232, val_precision: 0.7270, val_recall: 0.7232, val_f1: 0.7212
Epoch [49], train_loss: 0.3906, train_acc: 0.8647, val_loss: 0.8924, val_acc: 0.7186, val_precision: 0.7279, val_recall: 0.7186, val_f1: 0.7200
Epoch [50], train_loss: 0.3388, train_acc: 0.8846, val_loss: 0.9346, val_acc: 0.7200, val_precision: 0.7179, val_recall: 0.7200, val_f1: 0.7164
Epoch [51], train_loss: 0.2943, train_acc: 0.9018, val_loss: 0.9463, val_acc: 0.7292, val_precision: 0.7342, val_recall: 0.7292, val_f1: 0.7284
Epoch [52], train_loss: 0.2814, train_acc: 0.9049, val_loss: 0.9795, val_acc: 0.7184, val_precision: 0.7239, val_recall: 0.7184, val_f1: 0.7176
Epoch [53], train_loss: 0.2328, train_acc: 0.9251, val_loss: 1.0201, val_acc: 0.7278, val_precision: 0.7322, val_recall: 0.7278, val_f1: 0.7262
Epoch [54], train_loss: 0.2648, train_acc: 0.9104, val_loss: 1.3060, val_acc: 0.6738, val_precision: 0.6987, val_recall: 0.6738, val_f1: 0.6630
Epoch [55], train_loss: 0.4420, train_acc: 0.8434, val_loss: 0.9296, val_acc: 0.7094, val_precision: 0.7195, val_recall: 0.7094, val_f1: 0.7106
Epoch [56], train_loss: 0.2829, train_acc: 0.9044, val_loss: 1.0157, val_acc: 0.7316, val_precision: 0.7343, val_recall: 0.7316, val_f1: 0.7313
Epoch [57], train_loss: 0.1920, train_acc: 0.9384, val_loss: 1.0644, val_acc: 0.7334, val_precision: 0.7395, val_recall: 0.7334, val_f1: 0.7355
Epoch [58], train_loss: 0.1581, train_acc: 0.9501, val_loss: 1.1977, val_acc: 0.7124, val_precision: 0.7259, val_recall: 0.7124, val_f1: 0.7148
Epoch [59], train_loss: 0.1472, train_acc: 0.9527, val_loss: 1.2031, val_acc: 0.7170, val_precision: 0.7239, val_recall: 0.7170, val_f1: 0.7179
Epoch [60], train_loss: 0.1150, train_acc: 0.9660, val_loss: 1.2359, val_acc: 0.7310, val_precision: 0.7296, val_recall: 0.7310, val_f1: 0.7281
Epoch [61], train_loss: 0.0896, train_acc: 0.9750, val_loss: 1.3369, val_acc: 0.7242, val_precision: 0.7321, val_recall: 0.7242, val_f1: 0.7268
Epoch [62], train_loss: 0.0705, train_acc: 0.9816, val_loss: 1.4484, val_acc: 0.7230, val_precision: 0.7231, val_recall: 0.7230, val_f1: 0.7223
Epoch [63], train_loss: 0.0546, train_acc: 0.9873, val_loss: 1.4951, val_acc: 0.7254, val_precision: 0.7251, val_recall: 0.7254, val_f1: 0.7235
Epoch [64], train_loss: 0.0480, train_acc: 0.9886, val_loss: 1.6917, val_acc: 0.7148, val_precision: 0.7346, val_recall: 0.7148, val_f1: 0.7180
Epoch [65], train_loss: 2.0136, train_acc: 0.6645, val_loss: 2.2285, val_acc: 0.2208, val_precision: 0.2116, val_recall: 0.2208, val_f1: 0.1562
Epoch [66], train_loss: 2.1824, train_acc: 0.1929, val_loss: 1.9696, val_acc: 0.2620, val_precision: 0.3330, val_recall: 0.2620, val_f1: 0.2271
Epoch [67], train_loss: 1.8741, train_acc: 0.3081, val_loss: 1.7830, val_acc: 0.3460, val_precision: 0.3815, val_recall: 0.3460, val_f1: 0.3222
Epoch [68], train_loss: 1.7087, train_acc: 0.3718, val_loss: 1.7003, val_acc: 0.3738, val_precision: 0.4157, val_recall: 0.3738, val_f1: 0.3602
Epoch [69], train_loss: 1.5835, train_acc: 0.4212, val_loss: 1.5028, val_acc: 0.4544, val_precision: 0.4512, val_recall: 0.4544, val_f1: 0.4463
Epoch [70], train_loss: 1.4494, train_acc: 0.4693, val_loss: 1.4086, val_acc: 0.4822, val_precision: 0.4948, val_recall: 0.4822, val_f1: 0.4809
Epoch [71], train_loss: 1.3529, train_acc: 0.5076, val_loss: 1.7995, val_acc: 0.3962, val_precision: 0.4790, val_recall: 0.3962, val_f1: 0.3706
Epoch [72], train_loss: 1.5477, train_acc: 0.4553, val_loss: 1.4218, val_acc: 0.4828, val_precision: 0.4838, val_recall: 0.4828, val_f1: 0.4726
Epoch [73], train_loss: 1.3068, train_acc: 0.5280, val_loss: 1.2624, val_acc: 0.5462, val_precision: 0.5491, val_recall: 0.5462, val_f1: 0.5434
Epoch [74], train_loss: 1.1810, train_acc: 0.5755, val_loss: 1.1875, val_acc: 0.5696, val_precision: 0.5780, val_recall: 0.5696, val_f1: 0.5671
Epoch [75], train_loss: 1.0951, train_acc: 0.6072, val_loss: 1.1004, val_acc: 0.6092, val_precision: 0.6159, val_recall: 0.6092, val_f1: 0.6093
Epoch [76], train_loss: 1.0122, train_acc: 0.6391, val_loss: 1.0427, val_acc: 0.6260, val_precision: 0.6388, val_recall: 0.6260, val_f1: 0.6269
Epoch [77], train_loss: 0.9570, train_acc: 0.6582, val_loss: 1.0230, val_acc: 0.6282, val_precision: 0.6424, val_recall: 0.6282, val_f1: 0.6303
Epoch [78], train_loss: 0.9097, train_acc: 0.6790, val_loss: 0.9918, val_acc: 0.6458, val_precision: 0.6596, val_recall: 0.6458, val_f1: 0.6381
Epoch [79], train_loss: 0.8810, train_acc: 0.6881, val_loss: 0.9467, val_acc: 0.6618, val_precision: 0.6692, val_recall: 0.6618, val_f1: 0.6613
Epoch [80], train_loss: 0.8142, train_acc: 0.7138, val_loss: 1.0116, val_acc: 0.6502, val_precision: 0.6731, val_recall: 0.6502, val_f1: 0.6453
Epoch [81], train_loss: 0.8066, train_acc: 0.7172, val_loss: 0.9114, val_acc: 0.6840, val_precision: 0.6833, val_recall: 0.6840, val_f1: 0.6787
Epoch [82], train_loss: 0.7386, train_acc: 0.7411, val_loss: 0.8824, val_acc: 0.6862, val_precision: 0.6944, val_recall: 0.6862, val_f1: 0.6874
Epoch [83], train_loss: 0.6824, train_acc: 0.7623, val_loss: 0.8643, val_acc: 0.7014, val_precision: 0.7055, val_recall: 0.7014, val_f1: 0.7013
Epoch [84], train_loss: 0.6426, train_acc: 0.7757, val_loss: 0.8721, val_acc: 0.7026, val_precision: 0.7170, val_recall: 0.7026, val_f1: 0.7059
Epoch [85], train_loss: 0.6098, train_acc: 0.7878, val_loss: 0.8410, val_acc: 0.7106, val_precision: 0.7125, val_recall: 0.7106, val_f1: 0.7093
Epoch [86], train_loss: 0.6108, train_acc: 0.7869, val_loss: 0.8562, val_acc: 0.7076, val_precision: 0.7155, val_recall: 0.7076, val_f1: 0.7080
Epoch [87], train_loss: 0.5824, train_acc: 0.7966, val_loss: 0.8843, val_acc: 0.7056, val_precision: 0.7163, val_recall: 0.7056, val_f1: 0.7048
Epoch [88], train_loss: 0.5381, train_acc: 0.8154, val_loss: 0.8549, val_acc: 0.7160, val_precision: 0.7132, val_recall: 0.7160, val_f1: 0.7129
Epoch [89], train_loss: 0.4983, train_acc: 0.8295, val_loss: 0.8561, val_acc: 0.7146, val_precision: 0.7191, val_recall: 0.7146, val_f1: 0.7147
Epoch [90], train_loss: 0.4795, train_acc: 0.8342, val_loss: 0.9018, val_acc: 0.7030, val_precision: 0.7230, val_recall: 0.7030, val_f1: 0.7058
Epoch [91], train_loss: 0.4478, train_acc: 0.8460, val_loss: 0.8737, val_acc: 0.7218, val_precision: 0.7247, val_recall: 0.7218, val_f1: 0.7214
Epoch [92], train_loss: 0.4074, train_acc: 0.8631, val_loss: 0.8984, val_acc: 0.7244, val_precision: 0.7273, val_recall: 0.7244, val_f1: 0.7243
Epoch [93], train_loss: 0.3700, train_acc: 0.8777, val_loss: 0.9106, val_acc: 0.7200, val_precision: 0.7201, val_recall: 0.7200, val_f1: 0.7185
Epoch [94], train_loss: 0.3620, train_acc: 0.8770, val_loss: 0.9409, val_acc: 0.7146, val_precision: 0.7204, val_recall: 0.7146, val_f1: 0.7134
Epoch [95], train_loss: 0.3425, train_acc: 0.8839, val_loss: 0.9656, val_acc: 0.7168, val_precision: 0.7265, val_recall: 0.7168, val_f1: 0.7193
Epoch [96], train_loss: 0.3074, train_acc: 0.8989, val_loss: 0.9900, val_acc: 0.7200, val_precision: 0.7219, val_recall: 0.7200, val_f1: 0.7195
Epoch [97], train_loss: 0.2888, train_acc: 0.9047, val_loss: 1.0271, val_acc: 0.7264, val_precision: 0.7300, val_recall: 0.7264, val_f1: 0.7267
Epoch [98], train_loss: 0.2672, train_acc: 0.9097, val_loss: 1.0419, val_acc: 0.7116, val_precision: 0.7198, val_recall: 0.7116, val_f1: 0.7140
Epoch [99], train_loss: 0.2522, train_acc: 0.9168, val_loss: 1.0463, val_acc: 0.7172, val_precision: 0.7186, val_recall: 0.7172, val_f1: 0.7167
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.8654, val_acc: 0.7110, val_precision: 0.7137, val_recall: 0.7110, val_f1: 0.7103
Summary result of test set => last model => val_loss: 1.1000, val_acc: 0.7019, val_precision: 0.7049, val_recall: 0.7019, val_f1: 0.7016
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7101
--
confusion matrix
[[731  11  90  14  19  12  18  12  66  27]
 [ 25 786   6   8   3   9  13   3  30 117]
 [ 41   4 630  61  73  63  80  25  11  12]
 [ 18   6  80 454  54 217 102  27  16  26]
 [ 13   2  94  54 604  55 107  58  12   1]
 [  7   2  62 121  41 669  43  38   8   9]
 [  6   3  31  51  24  25 839   6   7   8]
 [ 10   0  38  50  63  87  11 722   4  15]
 [ 62  17  15  12   5   9  10   1 850  19]
 [ 40  49  10  16   2  13  10  16  28 816]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.77      0.73      0.75      1000
  automobile       0.89      0.79      0.84      1000
        bird       0.60      0.63      0.61      1000
         cat       0.54      0.45      0.49      1000
        deer       0.68      0.60      0.64      1000
         dog       0.58      0.67      0.62      1000
        frog       0.68      0.84      0.75      1000
       horse       0.80      0.72      0.76      1000
        ship       0.82      0.85      0.84      1000
       truck       0.78      0.82      0.80      1000

    accuracy                           0.71     10000
   macro avg       0.71      0.71      0.71     10000
weighted avg       0.71      0.71      0.71     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7092
--
confusion matrix
[[369   5  43  10   8   3   3   5  24  18]
 [ 12 414   5   4   3   3   5   2  18  46]
 [ 26   3 333  25  41  30  51  12   6   5]
 [  8   1  41 208  12 123  44  11   7  16]
 [  9   1  48  26 278  26  48  23   7   5]
 [  2   0  33  60  20 337  34  20   4   4]
 [  1   2  20  24  12  15 421   3   5   4]
 [  8   0  28  16  39  42   9 346   2  10]
 [ 40   6   7   9   2   2   4   3 420  11]
 [ 11  23   3   8   5   5   9   5  12 420]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.76      0.76      0.76       488
  automobile       0.91      0.81      0.86       512
        bird       0.59      0.63      0.61       532
         cat       0.53      0.44      0.48       471
        deer       0.66      0.59      0.62       471
         dog       0.58      0.66      0.61       514
        frog       0.67      0.83      0.74       507
       horse       0.80      0.69      0.74       500
        ship       0.83      0.83      0.83       504
       truck       0.78      0.84      0.81       501

    accuracy                           0.71      5000
   macro avg       0.71      0.71      0.71      5000
weighted avg       0.71      0.71      0.71      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 439814: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 10:57:45 2024
Job was executed on host(s) <hgn43>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 10:57:56 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 10:57:56 2024
Terminated at Wed Feb 28 11:06:16 2024
Results reported at Wed Feb 28 11:06:16 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_batch_4096_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_batch_4096_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.00001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_00001" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.002 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0.002" --model "CNN"


# Optimizer

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1482.00 sec.
    Max Memory :                                 3737 MB
    Average Memory :                             3432.23 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6503.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   500 sec.
    Turnaround time :                            511 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_batch_4096_err_439814> for stderr output of this job.

