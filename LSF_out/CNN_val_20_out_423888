loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.9251, train_acc: 0.2743, val_loss: 1.6323, val_acc: 0.4092, val_precision: 0.4491, val_recall: 0.4092, val_f1: 0.4004
Epoch [1], train_loss: 1.3623, train_acc: 0.5009, val_loss: 1.2032, val_acc: 0.5629, val_precision: 0.5801, val_recall: 0.5629, val_f1: 0.5464
Epoch [2], train_loss: 1.0824, train_acc: 0.6094, val_loss: 1.0356, val_acc: 0.6342, val_precision: 0.6704, val_recall: 0.6342, val_f1: 0.6371
Epoch [3], train_loss: 0.9011, train_acc: 0.6794, val_loss: 0.8625, val_acc: 0.6968, val_precision: 0.7175, val_recall: 0.6968, val_f1: 0.6991
Epoch [4], train_loss: 0.7664, train_acc: 0.7290, val_loss: 0.7953, val_acc: 0.7197, val_precision: 0.7280, val_recall: 0.7197, val_f1: 0.7175
Epoch [5], train_loss: 0.6462, train_acc: 0.7735, val_loss: 0.7520, val_acc: 0.7428, val_precision: 0.7485, val_recall: 0.7428, val_f1: 0.7397
Epoch [6], train_loss: 0.5401, train_acc: 0.8111, val_loss: 0.7411, val_acc: 0.7493, val_precision: 0.7627, val_recall: 0.7493, val_f1: 0.7483
Epoch [7], train_loss: 0.4342, train_acc: 0.8458, val_loss: 0.7170, val_acc: 0.7593, val_precision: 0.7749, val_recall: 0.7593, val_f1: 0.7607
Epoch [8], train_loss: 0.3350, train_acc: 0.8817, val_loss: 0.7977, val_acc: 0.7589, val_precision: 0.7639, val_recall: 0.7589, val_f1: 0.7560
Epoch [9], train_loss: 0.2557, train_acc: 0.9077, val_loss: 0.8761, val_acc: 0.7547, val_precision: 0.7619, val_recall: 0.7547, val_f1: 0.7529
Epoch [10], train_loss: 0.1994, train_acc: 0.9299, val_loss: 0.9919, val_acc: 0.7570, val_precision: 0.7638, val_recall: 0.7570, val_f1: 0.7547
Epoch [11], train_loss: 0.1479, train_acc: 0.9478, val_loss: 1.1238, val_acc: 0.7479, val_precision: 0.7615, val_recall: 0.7479, val_f1: 0.7472
Epoch [12], train_loss: 0.1283, train_acc: 0.9555, val_loss: 1.1373, val_acc: 0.7634, val_precision: 0.7669, val_recall: 0.7634, val_f1: 0.7605
Epoch [13], train_loss: 0.0979, train_acc: 0.9660, val_loss: 1.2322, val_acc: 0.7416, val_precision: 0.7559, val_recall: 0.7416, val_f1: 0.7422
Epoch [14], train_loss: 0.1046, train_acc: 0.9644, val_loss: 1.2462, val_acc: 0.7430, val_precision: 0.7488, val_recall: 0.7430, val_f1: 0.7370
Epoch [15], train_loss: 0.0774, train_acc: 0.9731, val_loss: 1.4076, val_acc: 0.7519, val_precision: 0.7581, val_recall: 0.7519, val_f1: 0.7492
Epoch [16], train_loss: 0.0775, train_acc: 0.9739, val_loss: 1.2914, val_acc: 0.7507, val_precision: 0.7628, val_recall: 0.7507, val_f1: 0.7514
Epoch [17], train_loss: 0.0648, train_acc: 0.9784, val_loss: 1.4181, val_acc: 0.7458, val_precision: 0.7505, val_recall: 0.7458, val_f1: 0.7411
Epoch [18], train_loss: 0.0657, train_acc: 0.9776, val_loss: 1.4366, val_acc: 0.7492, val_precision: 0.7547, val_recall: 0.7492, val_f1: 0.7466
Epoch [19], train_loss: 0.0685, train_acc: 0.9770, val_loss: 1.4495, val_acc: 0.7543, val_precision: 0.7611, val_recall: 0.7543, val_f1: 0.7536
Epoch [20], train_loss: 0.0637, train_acc: 0.9800, val_loss: 1.5460, val_acc: 0.7511, val_precision: 0.7581, val_recall: 0.7511, val_f1: 0.7480
Epoch [21], train_loss: 0.0573, train_acc: 0.9815, val_loss: 1.5572, val_acc: 0.7510, val_precision: 0.7604, val_recall: 0.7510, val_f1: 0.7510
Epoch [22], train_loss: 0.0506, train_acc: 0.9833, val_loss: 1.4686, val_acc: 0.7502, val_precision: 0.7616, val_recall: 0.7502, val_f1: 0.7511
Epoch [23], train_loss: 0.0511, train_acc: 0.9833, val_loss: 1.5321, val_acc: 0.7602, val_precision: 0.7715, val_recall: 0.7602, val_f1: 0.7611
Epoch [24], train_loss: 0.0618, train_acc: 0.9803, val_loss: 1.4459, val_acc: 0.7542, val_precision: 0.7635, val_recall: 0.7542, val_f1: 0.7539
Epoch [25], train_loss: 0.0537, train_acc: 0.9817, val_loss: 1.4543, val_acc: 0.7550, val_precision: 0.7645, val_recall: 0.7550, val_f1: 0.7538
Epoch [26], train_loss: 0.0556, train_acc: 0.9819, val_loss: 1.5683, val_acc: 0.7568, val_precision: 0.7674, val_recall: 0.7568, val_f1: 0.7571
Epoch [27], train_loss: 0.0453, train_acc: 0.9853, val_loss: 1.6052, val_acc: 0.7504, val_precision: 0.7611, val_recall: 0.7504, val_f1: 0.7511
Epoch [28], train_loss: 0.0437, train_acc: 0.9858, val_loss: 1.6812, val_acc: 0.7512, val_precision: 0.7568, val_recall: 0.7512, val_f1: 0.7478
Epoch [29], train_loss: 0.0575, train_acc: 0.9816, val_loss: 1.6380, val_acc: 0.7564, val_precision: 0.7671, val_recall: 0.7564, val_f1: 0.7561
Epoch [30], train_loss: 0.0456, train_acc: 0.9852, val_loss: 1.6634, val_acc: 0.7526, val_precision: 0.7651, val_recall: 0.7526, val_f1: 0.7517
Epoch [31], train_loss: 0.0481, train_acc: 0.9840, val_loss: 1.6683, val_acc: 0.7483, val_precision: 0.7683, val_recall: 0.7483, val_f1: 0.7524
Epoch [32], train_loss: 0.0457, train_acc: 0.9853, val_loss: 1.7093, val_acc: 0.7525, val_precision: 0.7619, val_recall: 0.7525, val_f1: 0.7517
Epoch [33], train_loss: 0.0429, train_acc: 0.9858, val_loss: 1.6820, val_acc: 0.7520, val_precision: 0.7629, val_recall: 0.7520, val_f1: 0.7516
Epoch [34], train_loss: 0.0395, train_acc: 0.9876, val_loss: 1.6603, val_acc: 0.7524, val_precision: 0.7563, val_recall: 0.7524, val_f1: 0.7490
Epoch [35], train_loss: 0.0501, train_acc: 0.9850, val_loss: 1.6886, val_acc: 0.7480, val_precision: 0.7627, val_recall: 0.7480, val_f1: 0.7504
Epoch [36], train_loss: 0.0433, train_acc: 0.9858, val_loss: 1.8042, val_acc: 0.7492, val_precision: 0.7574, val_recall: 0.7492, val_f1: 0.7488
Epoch [37], train_loss: 0.0463, train_acc: 0.9858, val_loss: 1.6498, val_acc: 0.7532, val_precision: 0.7667, val_recall: 0.7532, val_f1: 0.7548
Epoch [38], train_loss: 0.0421, train_acc: 0.9860, val_loss: 1.7936, val_acc: 0.7444, val_precision: 0.7576, val_recall: 0.7444, val_f1: 0.7454
Epoch [39], train_loss: 0.0326, train_acc: 0.9896, val_loss: 1.8594, val_acc: 0.7517, val_precision: 0.7597, val_recall: 0.7517, val_f1: 0.7508
Epoch [40], train_loss: 0.0439, train_acc: 0.9869, val_loss: 1.6974, val_acc: 0.7541, val_precision: 0.7608, val_recall: 0.7541, val_f1: 0.7539
Epoch [41], train_loss: 0.0414, train_acc: 0.9877, val_loss: 1.7683, val_acc: 0.7390, val_precision: 0.7642, val_recall: 0.7390, val_f1: 0.7440
Epoch [42], train_loss: 0.0319, train_acc: 0.9895, val_loss: 1.7345, val_acc: 0.7582, val_precision: 0.7692, val_recall: 0.7582, val_f1: 0.7592
Epoch [43], train_loss: 0.0357, train_acc: 0.9886, val_loss: 1.9549, val_acc: 0.7449, val_precision: 0.7560, val_recall: 0.7449, val_f1: 0.7438
Epoch [44], train_loss: 0.0455, train_acc: 0.9859, val_loss: 1.8252, val_acc: 0.7464, val_precision: 0.7692, val_recall: 0.7464, val_f1: 0.7501
Epoch [45], train_loss: 0.0438, train_acc: 0.9864, val_loss: 1.8486, val_acc: 0.7475, val_precision: 0.7601, val_recall: 0.7475, val_f1: 0.7463
Epoch [46], train_loss: 0.0460, train_acc: 0.9854, val_loss: 1.8462, val_acc: 0.7435, val_precision: 0.7510, val_recall: 0.7435, val_f1: 0.7413
Epoch [47], train_loss: 0.0337, train_acc: 0.9892, val_loss: 1.9644, val_acc: 0.7277, val_precision: 0.7479, val_recall: 0.7277, val_f1: 0.7295
Epoch [48], train_loss: 0.0435, train_acc: 0.9868, val_loss: 1.6763, val_acc: 0.7520, val_precision: 0.7627, val_recall: 0.7520, val_f1: 0.7525
Epoch [49], train_loss: 0.0393, train_acc: 0.9880, val_loss: 1.8212, val_acc: 0.7486, val_precision: 0.7614, val_recall: 0.7486, val_f1: 0.7503
Epoch [50], train_loss: 0.0183, train_acc: 0.9940, val_loss: 2.0858, val_acc: 0.7575, val_precision: 0.7646, val_recall: 0.7575, val_f1: 0.7552
Epoch [51], train_loss: 0.0507, train_acc: 0.9850, val_loss: 1.6959, val_acc: 0.7410, val_precision: 0.7535, val_recall: 0.7410, val_f1: 0.7418
Epoch [52], train_loss: 0.0402, train_acc: 0.9879, val_loss: 1.8008, val_acc: 0.7490, val_precision: 0.7692, val_recall: 0.7490, val_f1: 0.7533
Epoch [53], train_loss: 0.0285, train_acc: 0.9910, val_loss: 1.9015, val_acc: 0.7537, val_precision: 0.7627, val_recall: 0.7537, val_f1: 0.7533
Epoch [54], train_loss: 0.0381, train_acc: 0.9885, val_loss: 1.8635, val_acc: 0.7530, val_precision: 0.7682, val_recall: 0.7530, val_f1: 0.7541
Epoch [55], train_loss: 0.0402, train_acc: 0.9877, val_loss: 1.8346, val_acc: 0.7581, val_precision: 0.7610, val_recall: 0.7581, val_f1: 0.7545
Epoch [56], train_loss: 0.0341, train_acc: 0.9898, val_loss: 1.8365, val_acc: 0.7591, val_precision: 0.7644, val_recall: 0.7591, val_f1: 0.7562
Epoch [57], train_loss: 0.0344, train_acc: 0.9893, val_loss: 1.8354, val_acc: 0.7522, val_precision: 0.7624, val_recall: 0.7522, val_f1: 0.7516
Epoch [58], train_loss: 0.0404, train_acc: 0.9876, val_loss: 1.8411, val_acc: 0.7445, val_precision: 0.7570, val_recall: 0.7445, val_f1: 0.7460
Epoch [59], train_loss: 0.0246, train_acc: 0.9925, val_loss: 2.0032, val_acc: 0.7475, val_precision: 0.7591, val_recall: 0.7475, val_f1: 0.7460
Epoch [60], train_loss: 0.0310, train_acc: 0.9906, val_loss: 2.1124, val_acc: 0.7444, val_precision: 0.7507, val_recall: 0.7444, val_f1: 0.7412
Epoch [61], train_loss: 0.0371, train_acc: 0.9886, val_loss: 1.9717, val_acc: 0.7417, val_precision: 0.7555, val_recall: 0.7417, val_f1: 0.7434
Epoch [62], train_loss: 0.0382, train_acc: 0.9889, val_loss: 1.8555, val_acc: 0.7427, val_precision: 0.7595, val_recall: 0.7427, val_f1: 0.7457
Epoch [63], train_loss: 0.0386, train_acc: 0.9883, val_loss: 2.0593, val_acc: 0.7350, val_precision: 0.7518, val_recall: 0.7350, val_f1: 0.7365
Epoch [64], train_loss: 0.0310, train_acc: 0.9908, val_loss: 1.9759, val_acc: 0.7604, val_precision: 0.7687, val_recall: 0.7604, val_f1: 0.7597
Epoch [65], train_loss: 0.0216, train_acc: 0.9935, val_loss: 2.2387, val_acc: 0.7408, val_precision: 0.7516, val_recall: 0.7408, val_f1: 0.7393
Epoch [66], train_loss: 0.0464, train_acc: 0.9867, val_loss: 1.9874, val_acc: 0.7384, val_precision: 0.7593, val_recall: 0.7384, val_f1: 0.7413
Epoch [67], train_loss: 0.0290, train_acc: 0.9914, val_loss: 1.9433, val_acc: 0.7489, val_precision: 0.7640, val_recall: 0.7489, val_f1: 0.7510
Epoch [68], train_loss: 0.0319, train_acc: 0.9905, val_loss: 2.1337, val_acc: 0.7504, val_precision: 0.7567, val_recall: 0.7504, val_f1: 0.7471
Epoch [69], train_loss: 0.0447, train_acc: 0.9866, val_loss: 1.8269, val_acc: 0.7535, val_precision: 0.7596, val_recall: 0.7535, val_f1: 0.7519
Epoch [70], train_loss: 0.0220, train_acc: 0.9930, val_loss: 2.0832, val_acc: 0.7575, val_precision: 0.7636, val_recall: 0.7575, val_f1: 0.7564
Epoch [71], train_loss: 0.0286, train_acc: 0.9913, val_loss: 1.9335, val_acc: 0.7463, val_precision: 0.7568, val_recall: 0.7463, val_f1: 0.7462
Epoch [72], train_loss: 0.0258, train_acc: 0.9922, val_loss: 2.0971, val_acc: 0.7495, val_precision: 0.7560, val_recall: 0.7495, val_f1: 0.7480
Epoch [73], train_loss: 0.0398, train_acc: 0.9885, val_loss: 1.9505, val_acc: 0.7504, val_precision: 0.7558, val_recall: 0.7504, val_f1: 0.7492
Epoch [74], train_loss: 0.0315, train_acc: 0.9908, val_loss: 1.9520, val_acc: 0.7550, val_precision: 0.7666, val_recall: 0.7550, val_f1: 0.7555
Epoch [75], train_loss: 0.0277, train_acc: 0.9915, val_loss: 2.0774, val_acc: 0.7526, val_precision: 0.7630, val_recall: 0.7526, val_f1: 0.7518
Epoch [76], train_loss: 0.0349, train_acc: 0.9897, val_loss: 2.2482, val_acc: 0.7355, val_precision: 0.7456, val_recall: 0.7355, val_f1: 0.7349
Epoch [77], train_loss: 0.0391, train_acc: 0.9886, val_loss: 2.0756, val_acc: 0.7498, val_precision: 0.7540, val_recall: 0.7498, val_f1: 0.7465
Epoch [78], train_loss: 0.0310, train_acc: 0.9914, val_loss: 2.0337, val_acc: 0.7498, val_precision: 0.7591, val_recall: 0.7498, val_f1: 0.7483
Epoch [79], train_loss: 0.0213, train_acc: 0.9936, val_loss: 2.2095, val_acc: 0.7584, val_precision: 0.7656, val_recall: 0.7584, val_f1: 0.7573
Epoch [80], train_loss: 0.0436, train_acc: 0.9882, val_loss: 2.0868, val_acc: 0.7437, val_precision: 0.7560, val_recall: 0.7438, val_f1: 0.7444
Epoch [81], train_loss: 0.0328, train_acc: 0.9907, val_loss: 2.0242, val_acc: 0.7474, val_precision: 0.7544, val_recall: 0.7474, val_f1: 0.7454
Epoch [82], train_loss: 0.0262, train_acc: 0.9928, val_loss: 1.9913, val_acc: 0.7541, val_precision: 0.7616, val_recall: 0.7541, val_f1: 0.7521
Epoch [83], train_loss: 0.0306, train_acc: 0.9922, val_loss: 1.9682, val_acc: 0.7446, val_precision: 0.7511, val_recall: 0.7446, val_f1: 0.7425
Epoch [84], train_loss: 0.0350, train_acc: 0.9900, val_loss: 2.0454, val_acc: 0.7419, val_precision: 0.7553, val_recall: 0.7419, val_f1: 0.7424
Epoch [85], train_loss: 0.0281, train_acc: 0.9919, val_loss: 2.1872, val_acc: 0.7397, val_precision: 0.7444, val_recall: 0.7397, val_f1: 0.7354
Epoch [86], train_loss: 0.0260, train_acc: 0.9929, val_loss: 2.0268, val_acc: 0.7568, val_precision: 0.7662, val_recall: 0.7568, val_f1: 0.7577
Epoch [87], train_loss: 0.0243, train_acc: 0.9931, val_loss: 2.3088, val_acc: 0.7545, val_precision: 0.7649, val_recall: 0.7545, val_f1: 0.7553
Epoch [88], train_loss: 0.0355, train_acc: 0.9902, val_loss: 2.1880, val_acc: 0.7368, val_precision: 0.7554, val_recall: 0.7368, val_f1: 0.7396
Epoch [89], train_loss: 0.0342, train_acc: 0.9910, val_loss: 2.2619, val_acc: 0.7478, val_precision: 0.7596, val_recall: 0.7478, val_f1: 0.7483
Epoch [90], train_loss: 0.0281, train_acc: 0.9921, val_loss: 2.1428, val_acc: 0.7501, val_precision: 0.7593, val_recall: 0.7501, val_f1: 0.7499
Epoch [91], train_loss: 0.0219, train_acc: 0.9932, val_loss: 2.1874, val_acc: 0.7521, val_precision: 0.7612, val_recall: 0.7521, val_f1: 0.7513
Epoch [92], train_loss: 0.0411, train_acc: 0.9890, val_loss: 2.1636, val_acc: 0.7444, val_precision: 0.7532, val_recall: 0.7444, val_f1: 0.7436
Epoch [93], train_loss: 0.0299, train_acc: 0.9918, val_loss: 2.1811, val_acc: 0.7504, val_precision: 0.7656, val_recall: 0.7504, val_f1: 0.7530
Epoch [94], train_loss: 0.0229, train_acc: 0.9937, val_loss: 2.0585, val_acc: 0.7602, val_precision: 0.7710, val_recall: 0.7602, val_f1: 0.7605
Epoch [95], train_loss: 0.0285, train_acc: 0.9927, val_loss: 2.1059, val_acc: 0.7469, val_precision: 0.7588, val_recall: 0.7469, val_f1: 0.7479
Epoch [96], train_loss: 0.0369, train_acc: 0.9899, val_loss: 2.0463, val_acc: 0.7534, val_precision: 0.7623, val_recall: 0.7534, val_f1: 0.7533
Epoch [97], train_loss: 0.0215, train_acc: 0.9935, val_loss: 2.2372, val_acc: 0.7486, val_precision: 0.7653, val_recall: 0.7486, val_f1: 0.7515
Epoch [98], train_loss: 0.0353, train_acc: 0.9904, val_loss: 2.0855, val_acc: 0.7489, val_precision: 0.7607, val_recall: 0.7489, val_f1: 0.7491
Epoch [99], train_loss: 0.0281, train_acc: 0.9915, val_loss: 1.9969, val_acc: 0.7424, val_precision: 0.7533, val_recall: 0.7424, val_f1: 0.7428
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.7591, val_acc: 0.7466, val_precision: 0.7622, val_recall: 0.7466, val_f1: 0.7481
Summary result of test set => last model => val_loss: 2.2379, val_acc: 0.7353, val_precision: 0.7433, val_recall: 0.7353, val_f1: 0.7345
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7480
--
confusion matrix
[[747   8  48  30  25  10   6  11  74  41]
 [ 15 811   6  10   6   5  13   2  45  87]
 [ 54   1 598  78 127  51  40  30  12   9]
 [ 10   2  52 652  69 127  31  28  15  14]
 [ 13   1  38  85 754  25  28  46   7   3]
 [  6   3  36 214  51 619  11  48  10   2]
 [  7   1  48  82  60  18 764   4  14   2]
 [  4   0  27  55  74  40   2 777   6  15]
 [ 43   4   5  12   2  10   5   6 887  26]
 [ 19  40   3  19   1   7   6   3  31 871]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.81      0.75      0.78      1000
  automobile       0.93      0.81      0.87      1000
        bird       0.69      0.60      0.64      1000
         cat       0.53      0.65      0.58      1000
        deer       0.64      0.75      0.70      1000
         dog       0.68      0.62      0.65      1000
        frog       0.84      0.76      0.80      1000
       horse       0.81      0.78      0.79      1000
        ship       0.81      0.89      0.84      1000
       truck       0.81      0.87      0.84      1000

    accuracy                           0.75     10000
   macro avg       0.76      0.75      0.75     10000
weighted avg       0.76      0.75      0.75     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7580
--
confusion matrix
[[736   5  43  35  14   5   5  12  69  49]
 [ 11 833   5   8   3   1   6   2  37 104]
 [ 48   3 635  79 119  35  30  28  22   7]
 [ 10   3  51 620  67 128  37  27  19   5]
 [ 14   1  38  74 727  24  23  51   6   5]
 [  3   1  37 230  57 629  24  33   5   5]
 [  4   4  35  72  76  25 798   1   6   4]
 [  5   4  29  59  58  41   5 819   1  14]
 [ 38  10  10   8   2   5   2   1 903  19]
 [ 12  31   5  23   2   3   5  14  24 880]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.84      0.76      0.79       973
  automobile       0.93      0.82      0.87      1010
        bird       0.72      0.63      0.67      1006
         cat       0.51      0.64      0.57       967
        deer       0.65      0.75      0.70       963
         dog       0.70      0.61      0.66      1024
        frog       0.85      0.78      0.81      1025
       horse       0.83      0.79      0.81      1035
        ship       0.83      0.90      0.86       998
       truck       0.81      0.88      0.84       999

    accuracy                           0.76     10000
   macro avg       0.77      0.76      0.76     10000
weighted avg       0.77      0.76      0.76     10000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423888: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:22:54 2024
Job was executed on host(s) <hgn46>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:23:00 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:23:00 2024
Terminated at Wed Feb 28 08:31:30 2024
Results reported at Wed Feb 28 08:31:30 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_val_20_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_val_20_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1474.00 sec.
    Max Memory :                                 3361 MB
    Average Memory :                             3242.38 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6879.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   511 sec.
    Turnaround time :                            516 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_val_20_err_423888> for stderr output of this job.

