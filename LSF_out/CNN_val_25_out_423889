loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.8695, train_acc: 0.2954, val_loss: 1.4781, val_acc: 0.4531, val_precision: 0.4686, val_recall: 0.4531, val_f1: 0.4354
Epoch [1], train_loss: 1.3743, train_acc: 0.4969, val_loss: 1.2150, val_acc: 0.5618, val_precision: 0.5836, val_recall: 0.5618, val_f1: 0.5567
Epoch [2], train_loss: 1.1098, train_acc: 0.6013, val_loss: 1.0726, val_acc: 0.6087, val_precision: 0.6405, val_recall: 0.6087, val_f1: 0.6122
Epoch [3], train_loss: 0.9284, train_acc: 0.6702, val_loss: 0.9111, val_acc: 0.6747, val_precision: 0.6987, val_recall: 0.6747, val_f1: 0.6752
Epoch [4], train_loss: 0.7969, train_acc: 0.7197, val_loss: 0.8585, val_acc: 0.6962, val_precision: 0.7047, val_recall: 0.6962, val_f1: 0.6917
Epoch [5], train_loss: 0.6832, train_acc: 0.7586, val_loss: 0.8088, val_acc: 0.7198, val_precision: 0.7375, val_recall: 0.7198, val_f1: 0.7202
Epoch [6], train_loss: 0.5715, train_acc: 0.7980, val_loss: 0.7930, val_acc: 0.7283, val_precision: 0.7444, val_recall: 0.7283, val_f1: 0.7283
Epoch [7], train_loss: 0.4746, train_acc: 0.8334, val_loss: 0.7557, val_acc: 0.7412, val_precision: 0.7522, val_recall: 0.7412, val_f1: 0.7409
Epoch [8], train_loss: 0.3754, train_acc: 0.8660, val_loss: 0.8272, val_acc: 0.7433, val_precision: 0.7539, val_recall: 0.7433, val_f1: 0.7429
Epoch [9], train_loss: 0.3000, train_acc: 0.8948, val_loss: 0.8764, val_acc: 0.7397, val_precision: 0.7592, val_recall: 0.7397, val_f1: 0.7406
Epoch [10], train_loss: 0.2291, train_acc: 0.9183, val_loss: 1.0291, val_acc: 0.7315, val_precision: 0.7513, val_recall: 0.7315, val_f1: 0.7336
Epoch [11], train_loss: 0.1864, train_acc: 0.9334, val_loss: 1.0253, val_acc: 0.7414, val_precision: 0.7540, val_recall: 0.7414, val_f1: 0.7423
Epoch [12], train_loss: 0.1484, train_acc: 0.9477, val_loss: 1.1490, val_acc: 0.7346, val_precision: 0.7498, val_recall: 0.7346, val_f1: 0.7351
Epoch [13], train_loss: 0.1136, train_acc: 0.9606, val_loss: 1.1951, val_acc: 0.7385, val_precision: 0.7494, val_recall: 0.7385, val_f1: 0.7395
Epoch [14], train_loss: 0.1123, train_acc: 0.9614, val_loss: 1.3212, val_acc: 0.7387, val_precision: 0.7471, val_recall: 0.7387, val_f1: 0.7376
Epoch [15], train_loss: 0.0895, train_acc: 0.9687, val_loss: 1.3417, val_acc: 0.7339, val_precision: 0.7464, val_recall: 0.7339, val_f1: 0.7343
Epoch [16], train_loss: 0.0865, train_acc: 0.9704, val_loss: 1.3085, val_acc: 0.7386, val_precision: 0.7484, val_recall: 0.7386, val_f1: 0.7387
Epoch [17], train_loss: 0.0915, train_acc: 0.9685, val_loss: 1.5297, val_acc: 0.7434, val_precision: 0.7501, val_recall: 0.7434, val_f1: 0.7405
Epoch [18], train_loss: 0.0653, train_acc: 0.9777, val_loss: 1.5837, val_acc: 0.7478, val_precision: 0.7536, val_recall: 0.7478, val_f1: 0.7465
Epoch [19], train_loss: 0.0774, train_acc: 0.9741, val_loss: 1.5311, val_acc: 0.7349, val_precision: 0.7455, val_recall: 0.7349, val_f1: 0.7340
Epoch [20], train_loss: 0.0753, train_acc: 0.9743, val_loss: 1.5361, val_acc: 0.7401, val_precision: 0.7517, val_recall: 0.7401, val_f1: 0.7412
Epoch [21], train_loss: 0.0590, train_acc: 0.9810, val_loss: 1.6614, val_acc: 0.7342, val_precision: 0.7428, val_recall: 0.7342, val_f1: 0.7322
Epoch [22], train_loss: 0.0724, train_acc: 0.9760, val_loss: 1.5925, val_acc: 0.7424, val_precision: 0.7530, val_recall: 0.7424, val_f1: 0.7429
Epoch [23], train_loss: 0.0726, train_acc: 0.9765, val_loss: 1.5676, val_acc: 0.7441, val_precision: 0.7538, val_recall: 0.7441, val_f1: 0.7430
Epoch [24], train_loss: 0.0440, train_acc: 0.9858, val_loss: 1.6854, val_acc: 0.7460, val_precision: 0.7518, val_recall: 0.7460, val_f1: 0.7445
Epoch [25], train_loss: 0.0639, train_acc: 0.9787, val_loss: 1.7150, val_acc: 0.7451, val_precision: 0.7545, val_recall: 0.7451, val_f1: 0.7454
Epoch [26], train_loss: 0.0576, train_acc: 0.9807, val_loss: 1.7687, val_acc: 0.7434, val_precision: 0.7550, val_recall: 0.7434, val_f1: 0.7436
Epoch [27], train_loss: 0.0576, train_acc: 0.9812, val_loss: 1.6866, val_acc: 0.7381, val_precision: 0.7545, val_recall: 0.7381, val_f1: 0.7403
Epoch [28], train_loss: 0.0502, train_acc: 0.9830, val_loss: 1.6514, val_acc: 0.7401, val_precision: 0.7520, val_recall: 0.7401, val_f1: 0.7407
Epoch [29], train_loss: 0.0507, train_acc: 0.9832, val_loss: 1.7423, val_acc: 0.7336, val_precision: 0.7442, val_recall: 0.7336, val_f1: 0.7337
Epoch [30], train_loss: 0.0555, train_acc: 0.9815, val_loss: 1.7803, val_acc: 0.7437, val_precision: 0.7554, val_recall: 0.7437, val_f1: 0.7443
Epoch [31], train_loss: 0.0520, train_acc: 0.9831, val_loss: 1.8439, val_acc: 0.7396, val_precision: 0.7542, val_recall: 0.7396, val_f1: 0.7408
Epoch [32], train_loss: 0.0411, train_acc: 0.9868, val_loss: 1.9013, val_acc: 0.7482, val_precision: 0.7557, val_recall: 0.7482, val_f1: 0.7479
Epoch [33], train_loss: 0.0537, train_acc: 0.9831, val_loss: 1.7943, val_acc: 0.7438, val_precision: 0.7515, val_recall: 0.7438, val_f1: 0.7429
Epoch [34], train_loss: 0.0438, train_acc: 0.9851, val_loss: 1.9623, val_acc: 0.7319, val_precision: 0.7493, val_recall: 0.7319, val_f1: 0.7343
Epoch [35], train_loss: 0.0520, train_acc: 0.9836, val_loss: 1.9300, val_acc: 0.7207, val_precision: 0.7344, val_recall: 0.7207, val_f1: 0.7213
Epoch [36], train_loss: 0.0438, train_acc: 0.9860, val_loss: 1.7540, val_acc: 0.7393, val_precision: 0.7516, val_recall: 0.7393, val_f1: 0.7397
Epoch [37], train_loss: 0.0420, train_acc: 0.9864, val_loss: 1.9121, val_acc: 0.7381, val_precision: 0.7458, val_recall: 0.7381, val_f1: 0.7368
Epoch [38], train_loss: 0.0529, train_acc: 0.9832, val_loss: 1.8831, val_acc: 0.7319, val_precision: 0.7445, val_recall: 0.7319, val_f1: 0.7315
Epoch [39], train_loss: 0.0425, train_acc: 0.9862, val_loss: 1.8894, val_acc: 0.7394, val_precision: 0.7478, val_recall: 0.7394, val_f1: 0.7391
Epoch [40], train_loss: 0.0450, train_acc: 0.9857, val_loss: 1.9001, val_acc: 0.7434, val_precision: 0.7519, val_recall: 0.7434, val_f1: 0.7417
Epoch [41], train_loss: 0.0358, train_acc: 0.9882, val_loss: 1.8898, val_acc: 0.7413, val_precision: 0.7485, val_recall: 0.7413, val_f1: 0.7403
Epoch [42], train_loss: 0.0437, train_acc: 0.9858, val_loss: 1.9137, val_acc: 0.7360, val_precision: 0.7463, val_recall: 0.7360, val_f1: 0.7373
Epoch [43], train_loss: 0.0493, train_acc: 0.9845, val_loss: 1.9787, val_acc: 0.7350, val_precision: 0.7511, val_recall: 0.7350, val_f1: 0.7374
Epoch [44], train_loss: 0.0398, train_acc: 0.9871, val_loss: 1.9755, val_acc: 0.7328, val_precision: 0.7497, val_recall: 0.7328, val_f1: 0.7361
Epoch [45], train_loss: 0.0489, train_acc: 0.9846, val_loss: 1.9580, val_acc: 0.7268, val_precision: 0.7446, val_recall: 0.7268, val_f1: 0.7307
Epoch [46], train_loss: 0.0433, train_acc: 0.9855, val_loss: 1.8542, val_acc: 0.7366, val_precision: 0.7464, val_recall: 0.7366, val_f1: 0.7369
Epoch [47], train_loss: 0.0455, train_acc: 0.9857, val_loss: 1.9457, val_acc: 0.7384, val_precision: 0.7523, val_recall: 0.7384, val_f1: 0.7408
Epoch [48], train_loss: 0.0311, train_acc: 0.9900, val_loss: 1.9617, val_acc: 0.7464, val_precision: 0.7547, val_recall: 0.7464, val_f1: 0.7462
Epoch [49], train_loss: 0.0360, train_acc: 0.9890, val_loss: 2.0252, val_acc: 0.7415, val_precision: 0.7496, val_recall: 0.7415, val_f1: 0.7404
Epoch [50], train_loss: 0.0263, train_acc: 0.9916, val_loss: 2.1273, val_acc: 0.7346, val_precision: 0.7510, val_recall: 0.7346, val_f1: 0.7369
Epoch [51], train_loss: 0.0587, train_acc: 0.9821, val_loss: 1.9546, val_acc: 0.7381, val_precision: 0.7487, val_recall: 0.7381, val_f1: 0.7385
Epoch [52], train_loss: 0.0426, train_acc: 0.9867, val_loss: 1.9625, val_acc: 0.7368, val_precision: 0.7463, val_recall: 0.7368, val_f1: 0.7363
Epoch [53], train_loss: 0.0391, train_acc: 0.9881, val_loss: 2.0815, val_acc: 0.7266, val_precision: 0.7417, val_recall: 0.7266, val_f1: 0.7287
Epoch [54], train_loss: 0.0506, train_acc: 0.9852, val_loss: 1.9470, val_acc: 0.7445, val_precision: 0.7499, val_recall: 0.7445, val_f1: 0.7431
Epoch [55], train_loss: 0.0330, train_acc: 0.9892, val_loss: 2.1980, val_acc: 0.7334, val_precision: 0.7430, val_recall: 0.7334, val_f1: 0.7343
Epoch [56], train_loss: 0.0368, train_acc: 0.9884, val_loss: 2.1387, val_acc: 0.7434, val_precision: 0.7529, val_recall: 0.7434, val_f1: 0.7433
Epoch [57], train_loss: 0.0522, train_acc: 0.9846, val_loss: 1.8907, val_acc: 0.7402, val_precision: 0.7479, val_recall: 0.7402, val_f1: 0.7384
Epoch [58], train_loss: 0.0342, train_acc: 0.9897, val_loss: 1.9856, val_acc: 0.7449, val_precision: 0.7534, val_recall: 0.7449, val_f1: 0.7448
Epoch [59], train_loss: 0.0378, train_acc: 0.9880, val_loss: 1.9469, val_acc: 0.7418, val_precision: 0.7525, val_recall: 0.7418, val_f1: 0.7421
Epoch [60], train_loss: 0.0325, train_acc: 0.9896, val_loss: 2.1034, val_acc: 0.7347, val_precision: 0.7480, val_recall: 0.7347, val_f1: 0.7358
Epoch [61], train_loss: 0.0447, train_acc: 0.9865, val_loss: 1.9250, val_acc: 0.7402, val_precision: 0.7467, val_recall: 0.7402, val_f1: 0.7382
Epoch [62], train_loss: 0.0309, train_acc: 0.9900, val_loss: 2.1040, val_acc: 0.7330, val_precision: 0.7472, val_recall: 0.7330, val_f1: 0.7353
Epoch [63], train_loss: 0.0492, train_acc: 0.9853, val_loss: 1.8965, val_acc: 0.7399, val_precision: 0.7453, val_recall: 0.7399, val_f1: 0.7383
Epoch [64], train_loss: 0.0288, train_acc: 0.9908, val_loss: 2.2021, val_acc: 0.7382, val_precision: 0.7501, val_recall: 0.7382, val_f1: 0.7389
Epoch [65], train_loss: 0.0373, train_acc: 0.9893, val_loss: 2.2985, val_acc: 0.7325, val_precision: 0.7436, val_recall: 0.7325, val_f1: 0.7305
Epoch [66], train_loss: 0.0342, train_acc: 0.9899, val_loss: 2.1867, val_acc: 0.7386, val_precision: 0.7449, val_recall: 0.7386, val_f1: 0.7373
Epoch [67], train_loss: 0.0417, train_acc: 0.9885, val_loss: 2.2035, val_acc: 0.7244, val_precision: 0.7363, val_recall: 0.7244, val_f1: 0.7240
Epoch [68], train_loss: 0.0475, train_acc: 0.9853, val_loss: 1.9529, val_acc: 0.7387, val_precision: 0.7458, val_recall: 0.7387, val_f1: 0.7378
Epoch [69], train_loss: 0.0295, train_acc: 0.9917, val_loss: 2.2913, val_acc: 0.7422, val_precision: 0.7525, val_recall: 0.7422, val_f1: 0.7427
Epoch [70], train_loss: 0.0323, train_acc: 0.9902, val_loss: 2.1923, val_acc: 0.7402, val_precision: 0.7553, val_recall: 0.7402, val_f1: 0.7427
Epoch [71], train_loss: 0.0332, train_acc: 0.9901, val_loss: 2.2072, val_acc: 0.7312, val_precision: 0.7468, val_recall: 0.7312, val_f1: 0.7321
Epoch [72], train_loss: 0.0374, train_acc: 0.9892, val_loss: 2.1414, val_acc: 0.7334, val_precision: 0.7419, val_recall: 0.7334, val_f1: 0.7327
Epoch [73], train_loss: 0.0368, train_acc: 0.9891, val_loss: 2.0140, val_acc: 0.7392, val_precision: 0.7473, val_recall: 0.7392, val_f1: 0.7390
Epoch [74], train_loss: 0.0290, train_acc: 0.9919, val_loss: 2.3395, val_acc: 0.7390, val_precision: 0.7465, val_recall: 0.7390, val_f1: 0.7383
Epoch [75], train_loss: 0.0521, train_acc: 0.9850, val_loss: 2.0775, val_acc: 0.7324, val_precision: 0.7433, val_recall: 0.7324, val_f1: 0.7323
Epoch [76], train_loss: 0.0304, train_acc: 0.9906, val_loss: 2.2771, val_acc: 0.7310, val_precision: 0.7412, val_recall: 0.7310, val_f1: 0.7306
Epoch [77], train_loss: 0.0262, train_acc: 0.9919, val_loss: 2.3129, val_acc: 0.7357, val_precision: 0.7524, val_recall: 0.7357, val_f1: 0.7371
Epoch [78], train_loss: 0.0408, train_acc: 0.9879, val_loss: 2.2083, val_acc: 0.7380, val_precision: 0.7469, val_recall: 0.7380, val_f1: 0.7377
Epoch [79], train_loss: 0.0338, train_acc: 0.9902, val_loss: 2.2867, val_acc: 0.7422, val_precision: 0.7549, val_recall: 0.7422, val_f1: 0.7430
Epoch [80], train_loss: 0.0453, train_acc: 0.9879, val_loss: 2.2282, val_acc: 0.7422, val_precision: 0.7516, val_recall: 0.7422, val_f1: 0.7413
Epoch [81], train_loss: 0.0250, train_acc: 0.9924, val_loss: 2.5566, val_acc: 0.7392, val_precision: 0.7485, val_recall: 0.7392, val_f1: 0.7384
Epoch [82], train_loss: 0.0446, train_acc: 0.9872, val_loss: 2.0926, val_acc: 0.7360, val_precision: 0.7463, val_recall: 0.7360, val_f1: 0.7358
Epoch [83], train_loss: 0.0308, train_acc: 0.9906, val_loss: 2.4972, val_acc: 0.7296, val_precision: 0.7376, val_recall: 0.7296, val_f1: 0.7260
Epoch [84], train_loss: 0.0425, train_acc: 0.9882, val_loss: 2.2534, val_acc: 0.7400, val_precision: 0.7494, val_recall: 0.7400, val_f1: 0.7404
Epoch [85], train_loss: 0.0360, train_acc: 0.9899, val_loss: 2.1793, val_acc: 0.7358, val_precision: 0.7535, val_recall: 0.7358, val_f1: 0.7387
Epoch [86], train_loss: 0.0356, train_acc: 0.9891, val_loss: 2.3161, val_acc: 0.7400, val_precision: 0.7515, val_recall: 0.7400, val_f1: 0.7410
Epoch [87], train_loss: 0.0337, train_acc: 0.9904, val_loss: 2.4757, val_acc: 0.7396, val_precision: 0.7480, val_recall: 0.7396, val_f1: 0.7389
Epoch [88], train_loss: 0.0451, train_acc: 0.9875, val_loss: 2.2389, val_acc: 0.7360, val_precision: 0.7449, val_recall: 0.7360, val_f1: 0.7358
Epoch [89], train_loss: 0.0346, train_acc: 0.9901, val_loss: 2.3305, val_acc: 0.7457, val_precision: 0.7561, val_recall: 0.7457, val_f1: 0.7458
Epoch [90], train_loss: 0.0275, train_acc: 0.9920, val_loss: 2.3709, val_acc: 0.7433, val_precision: 0.7551, val_recall: 0.7433, val_f1: 0.7443
Epoch [91], train_loss: 0.0328, train_acc: 0.9903, val_loss: 2.3106, val_acc: 0.7479, val_precision: 0.7526, val_recall: 0.7479, val_f1: 0.7454
Epoch [92], train_loss: 0.0265, train_acc: 0.9920, val_loss: 2.4182, val_acc: 0.7387, val_precision: 0.7461, val_recall: 0.7387, val_f1: 0.7373
Epoch [93], train_loss: 0.0278, train_acc: 0.9916, val_loss: 2.4478, val_acc: 0.7377, val_precision: 0.7484, val_recall: 0.7377, val_f1: 0.7371
Epoch [94], train_loss: 0.0448, train_acc: 0.9878, val_loss: 2.3279, val_acc: 0.7397, val_precision: 0.7442, val_recall: 0.7397, val_f1: 0.7370
Epoch [95], train_loss: 0.0303, train_acc: 0.9910, val_loss: 2.4912, val_acc: 0.7366, val_precision: 0.7490, val_recall: 0.7366, val_f1: 0.7366
Epoch [96], train_loss: 0.0310, train_acc: 0.9907, val_loss: 2.4218, val_acc: 0.7408, val_precision: 0.7537, val_recall: 0.7408, val_f1: 0.7423
Epoch [97], train_loss: 0.0445, train_acc: 0.9870, val_loss: 2.3418, val_acc: 0.7411, val_precision: 0.7516, val_recall: 0.7411, val_f1: 0.7414
Epoch [98], train_loss: 0.0274, train_acc: 0.9921, val_loss: 2.4183, val_acc: 0.7416, val_precision: 0.7503, val_recall: 0.7416, val_f1: 0.7417
Epoch [99], train_loss: 0.0397, train_acc: 0.9893, val_loss: 2.3991, val_acc: 0.7264, val_precision: 0.7410, val_recall: 0.7264, val_f1: 0.7286
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.7839, val_acc: 0.7332, val_precision: 0.7457, val_recall: 0.7332, val_f1: 0.7327
Summary result of test set => last model => val_loss: 2.6574, val_acc: 0.7112, val_precision: 0.7280, val_recall: 0.7112, val_f1: 0.7143
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7328
--
confusion matrix
[[797  11  57  18  17   2   4  17  39  38]
 [ 22 770  10  13   7   0  10   1  28 139]
 [ 71   2 639  69  73  35  57  34   5  15]
 [ 29   5  65 550  61 145  60  52  14  19]
 [ 17   3  67  53 691  32  53  74   7   3]
 [ 16   0  52 203  49 580  23  65   4   8]
 [ 12   2  41  53  41  21 819   2   6   3]
 [  7   0  34  41  59  43   3 801   4   8]
 [ 94  22  14  24  12   2   4   6 791  31]
 [ 30  26   3  12   3   9   3  14  10 890]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.73      0.80      0.76      1000
  automobile       0.92      0.77      0.84      1000
        bird       0.65      0.64      0.64      1000
         cat       0.53      0.55      0.54      1000
        deer       0.68      0.69      0.69      1000
         dog       0.67      0.58      0.62      1000
        frog       0.79      0.82      0.80      1000
       horse       0.75      0.80      0.78      1000
        ship       0.87      0.79      0.83      1000
       truck       0.77      0.89      0.83      1000

    accuracy                           0.73     10000
   macro avg       0.74      0.73      0.73     10000
weighted avg       0.74      0.73      0.73     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.7412
--
confusion matrix
[[ 954   11   69   22   28    3    6   15   44   55]
 [  23 1001   10    9    4    2    7    2   15  193]
 [  95    0  829   76  102   57   57   53   11    9]
 [  35    3   64  703   75  171   70   62   15   17]
 [  29    0   80   71  846   39   43   76    4   10]
 [   6    1   67  254   58  725   39   86    4   13]
 [  14    2   42   68   52   31 1036    8    9   10]
 [  16    0   41   44   77   54    6 1029    6   18]
 [ 131   23   14   22   15    1    6    3 1000   36]
 [  30   20    3   15    4    3   10   19   12 1142]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.72      0.79      0.75      1207
  automobile       0.94      0.79      0.86      1266
        bird       0.68      0.64      0.66      1289
         cat       0.55      0.58      0.56      1215
        deer       0.67      0.71      0.69      1198
         dog       0.67      0.58      0.62      1253
        frog       0.81      0.81      0.81      1272
       horse       0.76      0.80      0.78      1291
        ship       0.89      0.80      0.84      1251
       truck       0.76      0.91      0.83      1258

    accuracy                           0.74     12500
   macro avg       0.74      0.74      0.74     12500
weighted avg       0.75      0.74      0.74     12500

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423889: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:23:10 2024
Job was executed on host(s) <hgn44>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:23:21 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:23:21 2024
Terminated at Wed Feb 28 08:32:20 2024
Results reported at Wed Feb 28 08:32:20 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/CNN_val_25_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/CNN_val_25_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1565.00 sec.
    Max Memory :                                 3349 MB
    Average Memory :                             3209.98 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6891.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   540 sec.
    Turnaround time :                            550 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/CNN_val_25_err_423889> for stderr output of this job.

