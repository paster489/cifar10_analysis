loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.3465, train_acc: 0.5109, val_loss: 1.1866, val_acc: 0.5898, val_precision: 0.5898, val_recall: 0.5898, val_f1: 0.5898
Epoch [1], train_loss: 0.8458, train_acc: 0.7000, val_loss: 0.7640, val_acc: 0.7299, val_precision: 0.7299, val_recall: 0.7299, val_f1: 0.7299
Epoch [2], train_loss: 0.6260, train_acc: 0.7807, val_loss: 0.6518, val_acc: 0.7683, val_precision: 0.7683, val_recall: 0.7683, val_f1: 0.7683
Epoch [3], train_loss: 0.4909, train_acc: 0.8282, val_loss: 0.5616, val_acc: 0.8091, val_precision: 0.8091, val_recall: 0.8091, val_f1: 0.8091
Epoch [4], train_loss: 0.3815, train_acc: 0.8679, val_loss: 0.5430, val_acc: 0.8134, val_precision: 0.8134, val_recall: 0.8134, val_f1: 0.8134
Epoch [5], train_loss: 0.2880, train_acc: 0.8988, val_loss: 0.5623, val_acc: 0.8180, val_precision: 0.8180, val_recall: 0.8180, val_f1: 0.8180
Epoch [6], train_loss: 0.2091, train_acc: 0.9253, val_loss: 0.5762, val_acc: 0.8233, val_precision: 0.8233, val_recall: 0.8233, val_f1: 0.8233
Epoch [7], train_loss: 0.1511, train_acc: 0.9467, val_loss: 0.6050, val_acc: 0.8268, val_precision: 0.8268, val_recall: 0.8268, val_f1: 0.8268
Epoch [8], train_loss: 0.1071, train_acc: 0.9623, val_loss: 0.6665, val_acc: 0.8257, val_precision: 0.8257, val_recall: 0.8257, val_f1: 0.8257
Epoch [9], train_loss: 0.0938, train_acc: 0.9665, val_loss: 0.7116, val_acc: 0.8169, val_precision: 0.8169, val_recall: 0.8169, val_f1: 0.8169
Epoch [10], train_loss: 0.0638, train_acc: 0.9783, val_loss: 0.7212, val_acc: 0.8319, val_precision: 0.8319, val_recall: 0.8319, val_f1: 0.8319
Epoch [11], train_loss: 0.0738, train_acc: 0.9736, val_loss: 0.7458, val_acc: 0.8343, val_precision: 0.8343, val_recall: 0.8343, val_f1: 0.8343
Epoch [12], train_loss: 0.0494, train_acc: 0.9830, val_loss: 0.7855, val_acc: 0.8286, val_precision: 0.8286, val_recall: 0.8286, val_f1: 0.8286
Epoch [13], train_loss: 0.0528, train_acc: 0.9810, val_loss: 0.7688, val_acc: 0.8271, val_precision: 0.8271, val_recall: 0.8271, val_f1: 0.8271
Epoch [14], train_loss: 0.0430, train_acc: 0.9845, val_loss: 0.7491, val_acc: 0.8347, val_precision: 0.8347, val_recall: 0.8347, val_f1: 0.8347
Epoch [15], train_loss: 0.0390, train_acc: 0.9862, val_loss: 0.8514, val_acc: 0.8279, val_precision: 0.8279, val_recall: 0.8279, val_f1: 0.8279
Epoch [16], train_loss: 0.0485, train_acc: 0.9833, val_loss: 0.7559, val_acc: 0.8396, val_precision: 0.8396, val_recall: 0.8396, val_f1: 0.8396
Epoch [17], train_loss: 0.0282, train_acc: 0.9900, val_loss: 0.7338, val_acc: 0.8443, val_precision: 0.8443, val_recall: 0.8443, val_f1: 0.8443
Epoch [18], train_loss: 0.0381, train_acc: 0.9866, val_loss: 0.8265, val_acc: 0.8338, val_precision: 0.8338, val_recall: 0.8338, val_f1: 0.8338
Epoch [19], train_loss: 0.0393, train_acc: 0.9870, val_loss: 0.7504, val_acc: 0.8369, val_precision: 0.8369, val_recall: 0.8369, val_f1: 0.8369
Epoch [20], train_loss: 0.0326, train_acc: 0.9886, val_loss: 0.7908, val_acc: 0.8431, val_precision: 0.8431, val_recall: 0.8431, val_f1: 0.8431
Epoch [21], train_loss: 0.0237, train_acc: 0.9919, val_loss: 0.9540, val_acc: 0.8214, val_precision: 0.8214, val_recall: 0.8214, val_f1: 0.8214
Epoch [22], train_loss: 0.0337, train_acc: 0.9882, val_loss: 0.8168, val_acc: 0.8445, val_precision: 0.8445, val_recall: 0.8445, val_f1: 0.8445
Epoch [23], train_loss: 0.0229, train_acc: 0.9920, val_loss: 0.8375, val_acc: 0.8407, val_precision: 0.8407, val_recall: 0.8407, val_f1: 0.8407
Epoch [24], train_loss: 0.0240, train_acc: 0.9915, val_loss: 0.9390, val_acc: 0.8209, val_precision: 0.8209, val_recall: 0.8209, val_f1: 0.8209
Epoch [25], train_loss: 0.0274, train_acc: 0.9906, val_loss: 0.8021, val_acc: 0.8449, val_precision: 0.8449, val_recall: 0.8449, val_f1: 0.8449
Epoch [26], train_loss: 0.0228, train_acc: 0.9924, val_loss: 0.8341, val_acc: 0.8411, val_precision: 0.8411, val_recall: 0.8411, val_f1: 0.8411
Epoch [27], train_loss: 0.0262, train_acc: 0.9909, val_loss: 0.8101, val_acc: 0.8447, val_precision: 0.8447, val_recall: 0.8447, val_f1: 0.8447
Epoch [28], train_loss: 0.0216, train_acc: 0.9929, val_loss: 0.9148, val_acc: 0.8321, val_precision: 0.8321, val_recall: 0.8321, val_f1: 0.8321
Epoch [29], train_loss: 0.0199, train_acc: 0.9929, val_loss: 0.8409, val_acc: 0.8446, val_precision: 0.8446, val_recall: 0.8446, val_f1: 0.8446
Epoch [30], train_loss: 0.0212, train_acc: 0.9924, val_loss: 0.9567, val_acc: 0.8395, val_precision: 0.8395, val_recall: 0.8395, val_f1: 0.8395
Epoch [31], train_loss: 0.0232, train_acc: 0.9925, val_loss: 0.8669, val_acc: 0.8436, val_precision: 0.8436, val_recall: 0.8436, val_f1: 0.8436
Epoch [32], train_loss: 0.0115, train_acc: 0.9961, val_loss: 0.9842, val_acc: 0.8270, val_precision: 0.8270, val_recall: 0.8270, val_f1: 0.8270
Epoch [33], train_loss: 0.0217, train_acc: 0.9927, val_loss: 0.9763, val_acc: 0.8342, val_precision: 0.8342, val_recall: 0.8342, val_f1: 0.8342
Epoch [34], train_loss: 0.0241, train_acc: 0.9916, val_loss: 0.9143, val_acc: 0.8328, val_precision: 0.8328, val_recall: 0.8328, val_f1: 0.8328
Epoch [35], train_loss: 0.0122, train_acc: 0.9960, val_loss: 0.9189, val_acc: 0.8383, val_precision: 0.8383, val_recall: 0.8383, val_f1: 0.8383
Epoch [36], train_loss: 0.0144, train_acc: 0.9953, val_loss: 0.9141, val_acc: 0.8385, val_precision: 0.8385, val_recall: 0.8385, val_f1: 0.8385
Epoch [37], train_loss: 0.0226, train_acc: 0.9923, val_loss: 0.8431, val_acc: 0.8387, val_precision: 0.8387, val_recall: 0.8387, val_f1: 0.8387
Epoch [38], train_loss: 0.0156, train_acc: 0.9946, val_loss: 0.9593, val_acc: 0.8407, val_precision: 0.8407, val_recall: 0.8407, val_f1: 0.8407
Epoch [39], train_loss: 0.0088, train_acc: 0.9972, val_loss: 0.8833, val_acc: 0.8475, val_precision: 0.8475, val_recall: 0.8475, val_f1: 0.8475
Epoch [40], train_loss: 0.0125, train_acc: 0.9960, val_loss: 1.0246, val_acc: 0.8271, val_precision: 0.8271, val_recall: 0.8271, val_f1: 0.8271
Epoch [41], train_loss: 0.0229, train_acc: 0.9924, val_loss: 0.8738, val_acc: 0.8471, val_precision: 0.8471, val_recall: 0.8471, val_f1: 0.8471
Epoch [42], train_loss: 0.0166, train_acc: 0.9939, val_loss: 0.9761, val_acc: 0.8363, val_precision: 0.8363, val_recall: 0.8363, val_f1: 0.8363
Epoch [43], train_loss: 0.0107, train_acc: 0.9963, val_loss: 0.9375, val_acc: 0.8477, val_precision: 0.8477, val_recall: 0.8477, val_f1: 0.8477
Epoch [44], train_loss: 0.0138, train_acc: 0.9953, val_loss: 0.8829, val_acc: 0.8509, val_precision: 0.8509, val_recall: 0.8509, val_f1: 0.8509
Epoch [45], train_loss: 0.0121, train_acc: 0.9959, val_loss: 0.8807, val_acc: 0.8502, val_precision: 0.8502, val_recall: 0.8502, val_f1: 0.8502
Epoch [46], train_loss: 0.0143, train_acc: 0.9951, val_loss: 0.9429, val_acc: 0.8429, val_precision: 0.8429, val_recall: 0.8429, val_f1: 0.8429
Epoch [47], train_loss: 0.0139, train_acc: 0.9952, val_loss: 0.9180, val_acc: 0.8468, val_precision: 0.8468, val_recall: 0.8468, val_f1: 0.8468
Epoch [48], train_loss: 0.0140, train_acc: 0.9951, val_loss: 0.9280, val_acc: 0.8545, val_precision: 0.8545, val_recall: 0.8545, val_f1: 0.8545
Epoch [49], train_loss: 0.0104, train_acc: 0.9961, val_loss: 0.9210, val_acc: 0.8481, val_precision: 0.8481, val_recall: 0.8481, val_f1: 0.8481
Epoch [50], train_loss: 0.0102, train_acc: 0.9964, val_loss: 0.8937, val_acc: 0.8488, val_precision: 0.8488, val_recall: 0.8488, val_f1: 0.8488
Epoch [51], train_loss: 0.0152, train_acc: 0.9947, val_loss: 0.9888, val_acc: 0.8400, val_precision: 0.8400, val_recall: 0.8400, val_f1: 0.8400
Epoch [52], train_loss: 0.0139, train_acc: 0.9956, val_loss: 0.9998, val_acc: 0.8361, val_precision: 0.8361, val_recall: 0.8361, val_f1: 0.8361
Epoch [53], train_loss: 0.0106, train_acc: 0.9963, val_loss: 0.9042, val_acc: 0.8469, val_precision: 0.8469, val_recall: 0.8469, val_f1: 0.8469
Epoch [54], train_loss: 0.0038, train_acc: 0.9988, val_loss: 0.9002, val_acc: 0.8539, val_precision: 0.8539, val_recall: 0.8539, val_f1: 0.8539
Epoch [55], train_loss: 0.0113, train_acc: 0.9963, val_loss: 0.8659, val_acc: 0.8496, val_precision: 0.8496, val_recall: 0.8496, val_f1: 0.8496
Epoch [56], train_loss: 0.0103, train_acc: 0.9965, val_loss: 0.9558, val_acc: 0.8387, val_precision: 0.8387, val_recall: 0.8387, val_f1: 0.8387
Epoch [57], train_loss: 0.0107, train_acc: 0.9961, val_loss: 0.9349, val_acc: 0.8481, val_precision: 0.8481, val_recall: 0.8481, val_f1: 0.8481
Epoch [58], train_loss: 0.0101, train_acc: 0.9963, val_loss: 1.0463, val_acc: 0.8415, val_precision: 0.8415, val_recall: 0.8415, val_f1: 0.8415
Epoch [59], train_loss: 0.0100, train_acc: 0.9968, val_loss: 1.0257, val_acc: 0.8384, val_precision: 0.8384, val_recall: 0.8384, val_f1: 0.8384
Epoch [60], train_loss: 0.0107, train_acc: 0.9963, val_loss: 0.9537, val_acc: 0.8511, val_precision: 0.8511, val_recall: 0.8511, val_f1: 0.8511
Epoch [61], train_loss: 0.0077, train_acc: 0.9978, val_loss: 0.9603, val_acc: 0.8449, val_precision: 0.8449, val_recall: 0.8449, val_f1: 0.8449
Epoch [62], train_loss: 0.0093, train_acc: 0.9966, val_loss: 1.1302, val_acc: 0.8362, val_precision: 0.8362, val_recall: 0.8362, val_f1: 0.8362
Epoch [63], train_loss: 0.0104, train_acc: 0.9963, val_loss: 1.0667, val_acc: 0.8424, val_precision: 0.8424, val_recall: 0.8424, val_f1: 0.8424
Epoch [64], train_loss: 0.0108, train_acc: 0.9965, val_loss: 1.0256, val_acc: 0.8477, val_precision: 0.8477, val_recall: 0.8477, val_f1: 0.8477
Epoch [65], train_loss: 0.0066, train_acc: 0.9974, val_loss: 0.9750, val_acc: 0.8541, val_precision: 0.8541, val_recall: 0.8541, val_f1: 0.8541
Epoch [66], train_loss: 0.0095, train_acc: 0.9970, val_loss: 0.9961, val_acc: 0.8520, val_precision: 0.8520, val_recall: 0.8520, val_f1: 0.8520
Epoch [67], train_loss: 0.0065, train_acc: 0.9978, val_loss: 1.0335, val_acc: 0.8492, val_precision: 0.8492, val_recall: 0.8492, val_f1: 0.8492
Epoch [68], train_loss: 0.0047, train_acc: 0.9985, val_loss: 0.9887, val_acc: 0.8558, val_precision: 0.8558, val_recall: 0.8558, val_f1: 0.8558
Epoch [69], train_loss: 0.0129, train_acc: 0.9954, val_loss: 1.0769, val_acc: 0.8461, val_precision: 0.8461, val_recall: 0.8461, val_f1: 0.8461
Epoch [70], train_loss: 0.0128, train_acc: 0.9954, val_loss: 1.0267, val_acc: 0.8465, val_precision: 0.8465, val_recall: 0.8465, val_f1: 0.8465
Epoch [71], train_loss: 0.0055, train_acc: 0.9983, val_loss: 1.0162, val_acc: 0.8530, val_precision: 0.8530, val_recall: 0.8530, val_f1: 0.8530
Epoch [72], train_loss: 0.0076, train_acc: 0.9973, val_loss: 0.9739, val_acc: 0.8500, val_precision: 0.8500, val_recall: 0.8500, val_f1: 0.8500
Epoch [73], train_loss: 0.0064, train_acc: 0.9979, val_loss: 1.0338, val_acc: 0.8503, val_precision: 0.8503, val_recall: 0.8503, val_f1: 0.8503
Epoch [74], train_loss: 0.0074, train_acc: 0.9975, val_loss: 1.1208, val_acc: 0.8421, val_precision: 0.8421, val_recall: 0.8421, val_f1: 0.8421
Epoch [75], train_loss: 0.0074, train_acc: 0.9979, val_loss: 0.9656, val_acc: 0.8516, val_precision: 0.8516, val_recall: 0.8516, val_f1: 0.8516
Epoch [76], train_loss: 0.0056, train_acc: 0.9979, val_loss: 0.9892, val_acc: 0.8496, val_precision: 0.8496, val_recall: 0.8496, val_f1: 0.8496
Epoch [77], train_loss: 0.0090, train_acc: 0.9969, val_loss: 1.0149, val_acc: 0.8485, val_precision: 0.8485, val_recall: 0.8485, val_f1: 0.8485
Epoch [78], train_loss: 0.0081, train_acc: 0.9974, val_loss: 1.0240, val_acc: 0.8488, val_precision: 0.8488, val_recall: 0.8488, val_f1: 0.8488
Epoch [79], train_loss: 0.0076, train_acc: 0.9977, val_loss: 1.0445, val_acc: 0.8516, val_precision: 0.8516, val_recall: 0.8516, val_f1: 0.8516
Epoch [80], train_loss: 0.0059, train_acc: 0.9979, val_loss: 1.0299, val_acc: 0.8495, val_precision: 0.8495, val_recall: 0.8495, val_f1: 0.8495
Epoch [81], train_loss: 0.0083, train_acc: 0.9974, val_loss: 1.0262, val_acc: 0.8495, val_precision: 0.8495, val_recall: 0.8495, val_f1: 0.8495
Epoch [82], train_loss: 0.0084, train_acc: 0.9971, val_loss: 0.9833, val_acc: 0.8590, val_precision: 0.8590, val_recall: 0.8590, val_f1: 0.8590
Epoch [83], train_loss: 0.0057, train_acc: 0.9982, val_loss: 0.9939, val_acc: 0.8561, val_precision: 0.8561, val_recall: 0.8561, val_f1: 0.8561
Epoch [84], train_loss: 0.0027, train_acc: 0.9990, val_loss: 0.9953, val_acc: 0.8580, val_precision: 0.8580, val_recall: 0.8580, val_f1: 0.8580
Epoch [85], train_loss: 0.0085, train_acc: 0.9974, val_loss: 1.0759, val_acc: 0.8530, val_precision: 0.8530, val_recall: 0.8530, val_f1: 0.8530
Epoch [86], train_loss: 0.0057, train_acc: 0.9981, val_loss: 0.9999, val_acc: 0.8578, val_precision: 0.8578, val_recall: 0.8578, val_f1: 0.8578
Epoch [87], train_loss: 0.0014, train_acc: 0.9996, val_loss: 1.0864, val_acc: 0.8537, val_precision: 0.8537, val_recall: 0.8537, val_f1: 0.8537
Epoch [88], train_loss: 0.0102, train_acc: 0.9968, val_loss: 1.1517, val_acc: 0.8433, val_precision: 0.8433, val_recall: 0.8433, val_f1: 0.8433
Epoch [89], train_loss: 0.0118, train_acc: 0.9964, val_loss: 1.0025, val_acc: 0.8480, val_precision: 0.8480, val_recall: 0.8480, val_f1: 0.8480
Epoch [90], train_loss: 0.0049, train_acc: 0.9984, val_loss: 1.0424, val_acc: 0.8533, val_precision: 0.8533, val_recall: 0.8533, val_f1: 0.8533
Epoch [91], train_loss: 0.0070, train_acc: 0.9978, val_loss: 1.0171, val_acc: 0.8529, val_precision: 0.8529, val_recall: 0.8529, val_f1: 0.8529
Epoch [92], train_loss: 0.0028, train_acc: 0.9991, val_loss: 1.0348, val_acc: 0.8599, val_precision: 0.8599, val_recall: 0.8599, val_f1: 0.8599
Epoch [93], train_loss: 0.0015, train_acc: 0.9996, val_loss: 1.0271, val_acc: 0.8547, val_precision: 0.8547, val_recall: 0.8547, val_f1: 0.8547
Epoch [94], train_loss: 0.0027, train_acc: 0.9992, val_loss: 1.0503, val_acc: 0.8500, val_precision: 0.8500, val_recall: 0.8500, val_f1: 0.8500
Epoch [95], train_loss: 0.0142, train_acc: 0.9954, val_loss: 1.2287, val_acc: 0.8292, val_precision: 0.8292, val_recall: 0.8292, val_f1: 0.8292
Epoch [96], train_loss: 0.0127, train_acc: 0.9961, val_loss: 0.9709, val_acc: 0.8574, val_precision: 0.8574, val_recall: 0.8574, val_f1: 0.8574
Epoch [97], train_loss: 0.0030, train_acc: 0.9990, val_loss: 0.9817, val_acc: 0.8557, val_precision: 0.8557, val_recall: 0.8557, val_f1: 0.8557
Epoch [98], train_loss: 0.0016, train_acc: 0.9995, val_loss: 0.9599, val_acc: 0.8597, val_precision: 0.8597, val_recall: 0.8597, val_f1: 0.8597
Epoch [99], train_loss: 0.0010, train_acc: 0.9997, val_loss: 1.0052, val_acc: 0.8580, val_precision: 0.8580, val_recall: 0.8580, val_f1: 0.8580
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.5731, val_acc: 0.8094, val_precision: 0.8094, val_recall: 0.8094, val_f1: 0.8094
Summary result of test set => last model => val_loss: 1.0812, val_acc: 0.8487, val_precision: 0.8487, val_recall: 0.8487, val_f1: 0.8487
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.8093
--
confusion matrix
[[850  54   7   6  12   2  10   7  28  24]
 [  2 964   0   0   2   1   2   0   5  24]
 [ 89   6 661  21  48  43 102  16   6   8]
 [ 29  12  60 612  33 113 104  15  11  11]
 [ 14   4  52  23 791  21  56  34   3   2]
 [  7   7  46 101  27 739  45  20   4   4]
 [  4   4  19   8   7  10 939   2   6   1]
 [ 21   6  24  26  41  39  24 810   1   8]
 [ 46  65   6   3   3   0   5   1 858  13]
 [ 11 104   0   2   0   3   3   2   6 869]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.79      0.85      0.82      1000
  automobile       0.79      0.96      0.87      1000
        bird       0.76      0.66      0.71      1000
         cat       0.76      0.61      0.68      1000
        deer       0.82      0.79      0.81      1000
         dog       0.76      0.74      0.75      1000
        frog       0.73      0.94      0.82      1000
       horse       0.89      0.81      0.85      1000
        ship       0.92      0.86      0.89      1000
       truck       0.90      0.87      0.88      1000

    accuracy                           0.81     10000
   macro avg       0.81      0.81      0.81     10000
weighted avg       0.81      0.81      0.81     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.8126
--
confusion matrix
[[410  23   5   2   5   2   3   5  20  13]
 [  1 500   0   0   0   2   1   0   0   8]
 [ 39   6 379   7  25  19  37  10   4   6]
 [ 14  10  32 271  10  60  58   5   2   9]
 [  8   1  22  13 378  16  17  14   0   2]
 [  3   5  26  58  14 372  29   4   3   0]
 [  3   5  10   4   9   3 470   2   0   1]
 [  7   5  14   8  20  19   7 413   1   6]
 [ 25  28   1   3   1   0   3   0 438   5]
 [  3  59   1   1   0   1   2   0   2 432]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.80      0.84      0.82       488
  automobile       0.78      0.98      0.87       512
        bird       0.77      0.71      0.74       532
         cat       0.74      0.58      0.65       471
        deer       0.82      0.80      0.81       471
         dog       0.75      0.72      0.74       514
        frog       0.75      0.93      0.83       507
       horse       0.91      0.83      0.87       500
        ship       0.93      0.87      0.90       504
       truck       0.90      0.86      0.88       501

    accuracy                           0.81      5000
   macro avg       0.82      0.81      0.81      5000
weighted avg       0.82      0.81      0.81      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 298726: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Tue Feb 27 14:06:42 2024
Job was executed on host(s) <hgn43>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 14:07:03 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 14:07:03 2024
Terminated at Tue Feb 27 14:31:13 2024
Results reported at Tue Feb 27 14:31:13 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/ResNet_18_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/ResNet_18_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2776.00 sec.
    Max Memory :                                 3474 MB
    Average Memory :                             3411.77 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6766.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                76
    Run time :                                   1451 sec.
    Turnaround time :                            1471 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/ResNet_18_err_298726> for stderr output of this job.

