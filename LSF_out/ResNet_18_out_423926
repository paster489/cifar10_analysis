loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.3465, train_acc: 0.5109, val_loss: 1.1866, val_acc: 0.5898, val_precision: 0.6511, val_recall: 0.5898, val_f1: 0.5910
Epoch [1], train_loss: 0.8458, train_acc: 0.7000, val_loss: 0.7640, val_acc: 0.7299, val_precision: 0.7415, val_recall: 0.7299, val_f1: 0.7282
Epoch [2], train_loss: 0.6260, train_acc: 0.7807, val_loss: 0.6518, val_acc: 0.7683, val_precision: 0.7895, val_recall: 0.7683, val_f1: 0.7681
Epoch [3], train_loss: 0.4909, train_acc: 0.8282, val_loss: 0.5616, val_acc: 0.8091, val_precision: 0.8262, val_recall: 0.8091, val_f1: 0.8088
Epoch [4], train_loss: 0.3815, train_acc: 0.8679, val_loss: 0.5430, val_acc: 0.8134, val_precision: 0.8219, val_recall: 0.8134, val_f1: 0.8111
Epoch [5], train_loss: 0.2880, train_acc: 0.8988, val_loss: 0.5623, val_acc: 0.8180, val_precision: 0.8315, val_recall: 0.8180, val_f1: 0.8195
Epoch [6], train_loss: 0.2091, train_acc: 0.9253, val_loss: 0.5762, val_acc: 0.8233, val_precision: 0.8327, val_recall: 0.8233, val_f1: 0.8229
Epoch [7], train_loss: 0.1511, train_acc: 0.9467, val_loss: 0.6050, val_acc: 0.8268, val_precision: 0.8409, val_recall: 0.8268, val_f1: 0.8272
Epoch [8], train_loss: 0.1071, train_acc: 0.9623, val_loss: 0.6665, val_acc: 0.8257, val_precision: 0.8383, val_recall: 0.8257, val_f1: 0.8262
Epoch [9], train_loss: 0.0938, train_acc: 0.9665, val_loss: 0.7116, val_acc: 0.8169, val_precision: 0.8335, val_recall: 0.8169, val_f1: 0.8193
Epoch [10], train_loss: 0.0638, train_acc: 0.9783, val_loss: 0.7212, val_acc: 0.8319, val_precision: 0.8386, val_recall: 0.8319, val_f1: 0.8310
Epoch [11], train_loss: 0.0738, train_acc: 0.9736, val_loss: 0.7458, val_acc: 0.8343, val_precision: 0.8431, val_recall: 0.8343, val_f1: 0.8346
Epoch [12], train_loss: 0.0494, train_acc: 0.9830, val_loss: 0.7855, val_acc: 0.8286, val_precision: 0.8366, val_recall: 0.8286, val_f1: 0.8265
Epoch [13], train_loss: 0.0528, train_acc: 0.9810, val_loss: 0.7688, val_acc: 0.8271, val_precision: 0.8336, val_recall: 0.8271, val_f1: 0.8268
Epoch [14], train_loss: 0.0430, train_acc: 0.9845, val_loss: 0.7491, val_acc: 0.8347, val_precision: 0.8448, val_recall: 0.8347, val_f1: 0.8357
Epoch [15], train_loss: 0.0390, train_acc: 0.9862, val_loss: 0.8514, val_acc: 0.8279, val_precision: 0.8396, val_recall: 0.8279, val_f1: 0.8276
Epoch [16], train_loss: 0.0485, train_acc: 0.9833, val_loss: 0.7559, val_acc: 0.8396, val_precision: 0.8459, val_recall: 0.8396, val_f1: 0.8394
Epoch [17], train_loss: 0.0282, train_acc: 0.9900, val_loss: 0.7338, val_acc: 0.8443, val_precision: 0.8504, val_recall: 0.8443, val_f1: 0.8439
Epoch [18], train_loss: 0.0381, train_acc: 0.9866, val_loss: 0.8265, val_acc: 0.8338, val_precision: 0.8423, val_recall: 0.8338, val_f1: 0.8342
Epoch [19], train_loss: 0.0393, train_acc: 0.9870, val_loss: 0.7504, val_acc: 0.8369, val_precision: 0.8436, val_recall: 0.8369, val_f1: 0.8364
Epoch [20], train_loss: 0.0326, train_acc: 0.9886, val_loss: 0.7908, val_acc: 0.8431, val_precision: 0.8474, val_recall: 0.8431, val_f1: 0.8421
Epoch [21], train_loss: 0.0237, train_acc: 0.9919, val_loss: 0.9540, val_acc: 0.8214, val_precision: 0.8352, val_recall: 0.8214, val_f1: 0.8208
Epoch [22], train_loss: 0.0337, train_acc: 0.9882, val_loss: 0.8168, val_acc: 0.8445, val_precision: 0.8529, val_recall: 0.8445, val_f1: 0.8451
Epoch [23], train_loss: 0.0229, train_acc: 0.9920, val_loss: 0.8375, val_acc: 0.8407, val_precision: 0.8468, val_recall: 0.8407, val_f1: 0.8384
Epoch [24], train_loss: 0.0240, train_acc: 0.9915, val_loss: 0.9390, val_acc: 0.8209, val_precision: 0.8314, val_recall: 0.8209, val_f1: 0.8214
Epoch [25], train_loss: 0.0274, train_acc: 0.9906, val_loss: 0.8021, val_acc: 0.8449, val_precision: 0.8544, val_recall: 0.8449, val_f1: 0.8457
Epoch [26], train_loss: 0.0228, train_acc: 0.9924, val_loss: 0.8341, val_acc: 0.8411, val_precision: 0.8469, val_recall: 0.8411, val_f1: 0.8402
Epoch [27], train_loss: 0.0262, train_acc: 0.9909, val_loss: 0.8101, val_acc: 0.8447, val_precision: 0.8484, val_recall: 0.8447, val_f1: 0.8430
Epoch [28], train_loss: 0.0216, train_acc: 0.9929, val_loss: 0.9148, val_acc: 0.8321, val_precision: 0.8429, val_recall: 0.8321, val_f1: 0.8333
Epoch [29], train_loss: 0.0199, train_acc: 0.9929, val_loss: 0.8409, val_acc: 0.8446, val_precision: 0.8500, val_recall: 0.8446, val_f1: 0.8437
Epoch [30], train_loss: 0.0212, train_acc: 0.9924, val_loss: 0.9567, val_acc: 0.8395, val_precision: 0.8442, val_recall: 0.8395, val_f1: 0.8372
Epoch [31], train_loss: 0.0232, train_acc: 0.9925, val_loss: 0.8669, val_acc: 0.8436, val_precision: 0.8494, val_recall: 0.8436, val_f1: 0.8432
Epoch [32], train_loss: 0.0115, train_acc: 0.9961, val_loss: 0.9842, val_acc: 0.8270, val_precision: 0.8366, val_recall: 0.8270, val_f1: 0.8254
Epoch [33], train_loss: 0.0217, train_acc: 0.9927, val_loss: 0.9763, val_acc: 0.8342, val_precision: 0.8465, val_recall: 0.8342, val_f1: 0.8351
Epoch [34], train_loss: 0.0241, train_acc: 0.9916, val_loss: 0.9143, val_acc: 0.8328, val_precision: 0.8453, val_recall: 0.8328, val_f1: 0.8345
Epoch [35], train_loss: 0.0122, train_acc: 0.9960, val_loss: 0.9189, val_acc: 0.8383, val_precision: 0.8509, val_recall: 0.8383, val_f1: 0.8404
Epoch [36], train_loss: 0.0144, train_acc: 0.9953, val_loss: 0.9141, val_acc: 0.8385, val_precision: 0.8460, val_recall: 0.8385, val_f1: 0.8369
Epoch [37], train_loss: 0.0226, train_acc: 0.9923, val_loss: 0.8431, val_acc: 0.8387, val_precision: 0.8450, val_recall: 0.8387, val_f1: 0.8386
Epoch [38], train_loss: 0.0156, train_acc: 0.9946, val_loss: 0.9593, val_acc: 0.8407, val_precision: 0.8541, val_recall: 0.8407, val_f1: 0.8423
Epoch [39], train_loss: 0.0088, train_acc: 0.9972, val_loss: 0.8833, val_acc: 0.8475, val_precision: 0.8525, val_recall: 0.8475, val_f1: 0.8457
Epoch [40], train_loss: 0.0125, train_acc: 0.9960, val_loss: 1.0246, val_acc: 0.8271, val_precision: 0.8339, val_recall: 0.8271, val_f1: 0.8259
Epoch [41], train_loss: 0.0229, train_acc: 0.9924, val_loss: 0.8738, val_acc: 0.8471, val_precision: 0.8530, val_recall: 0.8471, val_f1: 0.8470
Epoch [42], train_loss: 0.0166, train_acc: 0.9939, val_loss: 0.9761, val_acc: 0.8363, val_precision: 0.8461, val_recall: 0.8363, val_f1: 0.8360
Epoch [43], train_loss: 0.0107, train_acc: 0.9963, val_loss: 0.9375, val_acc: 0.8477, val_precision: 0.8529, val_recall: 0.8477, val_f1: 0.8474
Epoch [44], train_loss: 0.0138, train_acc: 0.9953, val_loss: 0.8829, val_acc: 0.8509, val_precision: 0.8558, val_recall: 0.8509, val_f1: 0.8497
Epoch [45], train_loss: 0.0121, train_acc: 0.9959, val_loss: 0.8807, val_acc: 0.8502, val_precision: 0.8566, val_recall: 0.8502, val_f1: 0.8503
Epoch [46], train_loss: 0.0143, train_acc: 0.9951, val_loss: 0.9429, val_acc: 0.8429, val_precision: 0.8471, val_recall: 0.8429, val_f1: 0.8418
Epoch [47], train_loss: 0.0139, train_acc: 0.9952, val_loss: 0.9180, val_acc: 0.8468, val_precision: 0.8555, val_recall: 0.8468, val_f1: 0.8477
Epoch [48], train_loss: 0.0140, train_acc: 0.9951, val_loss: 0.9280, val_acc: 0.8545, val_precision: 0.8584, val_recall: 0.8545, val_f1: 0.8528
Epoch [49], train_loss: 0.0104, train_acc: 0.9961, val_loss: 0.9210, val_acc: 0.8481, val_precision: 0.8540, val_recall: 0.8481, val_f1: 0.8477
Epoch [50], train_loss: 0.0102, train_acc: 0.9964, val_loss: 0.8937, val_acc: 0.8488, val_precision: 0.8533, val_recall: 0.8488, val_f1: 0.8476
Epoch [51], train_loss: 0.0152, train_acc: 0.9947, val_loss: 0.9888, val_acc: 0.8400, val_precision: 0.8482, val_recall: 0.8400, val_f1: 0.8391
Epoch [52], train_loss: 0.0139, train_acc: 0.9956, val_loss: 0.9998, val_acc: 0.8361, val_precision: 0.8458, val_recall: 0.8361, val_f1: 0.8368
Epoch [53], train_loss: 0.0106, train_acc: 0.9963, val_loss: 0.9042, val_acc: 0.8469, val_precision: 0.8543, val_recall: 0.8469, val_f1: 0.8479
Epoch [54], train_loss: 0.0038, train_acc: 0.9988, val_loss: 0.9002, val_acc: 0.8539, val_precision: 0.8617, val_recall: 0.8539, val_f1: 0.8541
Epoch [55], train_loss: 0.0113, train_acc: 0.9963, val_loss: 0.8659, val_acc: 0.8496, val_precision: 0.8533, val_recall: 0.8496, val_f1: 0.8485
Epoch [56], train_loss: 0.0103, train_acc: 0.9965, val_loss: 0.9558, val_acc: 0.8387, val_precision: 0.8431, val_recall: 0.8387, val_f1: 0.8369
Epoch [57], train_loss: 0.0107, train_acc: 0.9961, val_loss: 0.9349, val_acc: 0.8481, val_precision: 0.8534, val_recall: 0.8481, val_f1: 0.8476
Epoch [58], train_loss: 0.0101, train_acc: 0.9963, val_loss: 1.0463, val_acc: 0.8415, val_precision: 0.8477, val_recall: 0.8415, val_f1: 0.8418
Epoch [59], train_loss: 0.0100, train_acc: 0.9968, val_loss: 1.0257, val_acc: 0.8384, val_precision: 0.8483, val_recall: 0.8384, val_f1: 0.8399
Epoch [60], train_loss: 0.0107, train_acc: 0.9963, val_loss: 0.9537, val_acc: 0.8511, val_precision: 0.8552, val_recall: 0.8511, val_f1: 0.8506
Epoch [61], train_loss: 0.0077, train_acc: 0.9978, val_loss: 0.9603, val_acc: 0.8449, val_precision: 0.8503, val_recall: 0.8449, val_f1: 0.8441
Epoch [62], train_loss: 0.0093, train_acc: 0.9966, val_loss: 1.1302, val_acc: 0.8362, val_precision: 0.8473, val_recall: 0.8362, val_f1: 0.8355
Epoch [63], train_loss: 0.0104, train_acc: 0.9963, val_loss: 1.0667, val_acc: 0.8424, val_precision: 0.8479, val_recall: 0.8424, val_f1: 0.8415
Epoch [64], train_loss: 0.0108, train_acc: 0.9965, val_loss: 1.0256, val_acc: 0.8477, val_precision: 0.8572, val_recall: 0.8477, val_f1: 0.8486
Epoch [65], train_loss: 0.0066, train_acc: 0.9974, val_loss: 0.9750, val_acc: 0.8541, val_precision: 0.8595, val_recall: 0.8541, val_f1: 0.8538
Epoch [66], train_loss: 0.0095, train_acc: 0.9970, val_loss: 0.9961, val_acc: 0.8520, val_precision: 0.8586, val_recall: 0.8520, val_f1: 0.8516
Epoch [67], train_loss: 0.0065, train_acc: 0.9978, val_loss: 1.0335, val_acc: 0.8492, val_precision: 0.8537, val_recall: 0.8492, val_f1: 0.8473
Epoch [68], train_loss: 0.0047, train_acc: 0.9985, val_loss: 0.9887, val_acc: 0.8558, val_precision: 0.8623, val_recall: 0.8558, val_f1: 0.8563
Epoch [69], train_loss: 0.0129, train_acc: 0.9954, val_loss: 1.0769, val_acc: 0.8461, val_precision: 0.8515, val_recall: 0.8461, val_f1: 0.8452
Epoch [70], train_loss: 0.0128, train_acc: 0.9954, val_loss: 1.0267, val_acc: 0.8465, val_precision: 0.8508, val_recall: 0.8465, val_f1: 0.8456
Epoch [71], train_loss: 0.0055, train_acc: 0.9983, val_loss: 1.0162, val_acc: 0.8530, val_precision: 0.8646, val_recall: 0.8530, val_f1: 0.8538
Epoch [72], train_loss: 0.0076, train_acc: 0.9973, val_loss: 0.9739, val_acc: 0.8500, val_precision: 0.8564, val_recall: 0.8500, val_f1: 0.8501
Epoch [73], train_loss: 0.0064, train_acc: 0.9979, val_loss: 1.0338, val_acc: 0.8503, val_precision: 0.8548, val_recall: 0.8503, val_f1: 0.8501
Epoch [74], train_loss: 0.0074, train_acc: 0.9975, val_loss: 1.1208, val_acc: 0.8421, val_precision: 0.8520, val_recall: 0.8421, val_f1: 0.8414
Epoch [75], train_loss: 0.0074, train_acc: 0.9979, val_loss: 0.9656, val_acc: 0.8516, val_precision: 0.8578, val_recall: 0.8516, val_f1: 0.8512
Epoch [76], train_loss: 0.0056, train_acc: 0.9979, val_loss: 0.9892, val_acc: 0.8496, val_precision: 0.8544, val_recall: 0.8496, val_f1: 0.8488
Epoch [77], train_loss: 0.0090, train_acc: 0.9969, val_loss: 1.0149, val_acc: 0.8485, val_precision: 0.8546, val_recall: 0.8485, val_f1: 0.8488
Epoch [78], train_loss: 0.0081, train_acc: 0.9974, val_loss: 1.0240, val_acc: 0.8488, val_precision: 0.8547, val_recall: 0.8488, val_f1: 0.8487
Epoch [79], train_loss: 0.0076, train_acc: 0.9977, val_loss: 1.0445, val_acc: 0.8516, val_precision: 0.8584, val_recall: 0.8516, val_f1: 0.8511
Epoch [80], train_loss: 0.0059, train_acc: 0.9979, val_loss: 1.0299, val_acc: 0.8495, val_precision: 0.8572, val_recall: 0.8495, val_f1: 0.8492
Epoch [81], train_loss: 0.0083, train_acc: 0.9974, val_loss: 1.0262, val_acc: 0.8495, val_precision: 0.8531, val_recall: 0.8495, val_f1: 0.8480
Epoch [82], train_loss: 0.0084, train_acc: 0.9971, val_loss: 0.9833, val_acc: 0.8590, val_precision: 0.8631, val_recall: 0.8590, val_f1: 0.8580
Epoch [83], train_loss: 0.0057, train_acc: 0.9982, val_loss: 0.9939, val_acc: 0.8561, val_precision: 0.8611, val_recall: 0.8561, val_f1: 0.8555
Epoch [84], train_loss: 0.0027, train_acc: 0.9990, val_loss: 0.9953, val_acc: 0.8580, val_precision: 0.8623, val_recall: 0.8580, val_f1: 0.8570
Epoch [85], train_loss: 0.0085, train_acc: 0.9974, val_loss: 1.0759, val_acc: 0.8530, val_precision: 0.8602, val_recall: 0.8530, val_f1: 0.8532
Epoch [86], train_loss: 0.0057, train_acc: 0.9981, val_loss: 0.9999, val_acc: 0.8578, val_precision: 0.8620, val_recall: 0.8578, val_f1: 0.8572
Epoch [87], train_loss: 0.0014, train_acc: 0.9996, val_loss: 1.0864, val_acc: 0.8537, val_precision: 0.8607, val_recall: 0.8537, val_f1: 0.8536
Epoch [88], train_loss: 0.0102, train_acc: 0.9968, val_loss: 1.1517, val_acc: 0.8433, val_precision: 0.8498, val_recall: 0.8433, val_f1: 0.8436
Epoch [89], train_loss: 0.0118, train_acc: 0.9964, val_loss: 1.0025, val_acc: 0.8480, val_precision: 0.8569, val_recall: 0.8480, val_f1: 0.8483
Epoch [90], train_loss: 0.0049, train_acc: 0.9984, val_loss: 1.0424, val_acc: 0.8533, val_precision: 0.8575, val_recall: 0.8533, val_f1: 0.8524
Epoch [91], train_loss: 0.0070, train_acc: 0.9978, val_loss: 1.0171, val_acc: 0.8529, val_precision: 0.8603, val_recall: 0.8529, val_f1: 0.8526
Epoch [92], train_loss: 0.0028, train_acc: 0.9991, val_loss: 1.0348, val_acc: 0.8599, val_precision: 0.8650, val_recall: 0.8599, val_f1: 0.8590
Epoch [93], train_loss: 0.0015, train_acc: 0.9996, val_loss: 1.0271, val_acc: 0.8547, val_precision: 0.8611, val_recall: 0.8547, val_f1: 0.8551
Epoch [94], train_loss: 0.0027, train_acc: 0.9992, val_loss: 1.0503, val_acc: 0.8500, val_precision: 0.8553, val_recall: 0.8500, val_f1: 0.8498
Epoch [95], train_loss: 0.0142, train_acc: 0.9954, val_loss: 1.2287, val_acc: 0.8292, val_precision: 0.8399, val_recall: 0.8292, val_f1: 0.8282
Epoch [96], train_loss: 0.0127, train_acc: 0.9961, val_loss: 0.9709, val_acc: 0.8574, val_precision: 0.8613, val_recall: 0.8574, val_f1: 0.8565
Epoch [97], train_loss: 0.0030, train_acc: 0.9990, val_loss: 0.9817, val_acc: 0.8557, val_precision: 0.8589, val_recall: 0.8557, val_f1: 0.8543
Epoch [98], train_loss: 0.0016, train_acc: 0.9995, val_loss: 0.9599, val_acc: 0.8597, val_precision: 0.8644, val_recall: 0.8597, val_f1: 0.8588
Epoch [99], train_loss: 0.0010, train_acc: 0.9997, val_loss: 1.0052, val_acc: 0.8580, val_precision: 0.8645, val_recall: 0.8580, val_f1: 0.8577
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.5731, val_acc: 0.8094, val_precision: 0.8202, val_recall: 0.8094, val_f1: 0.8080
Summary result of test set => last model => val_loss: 1.0812, val_acc: 0.8487, val_precision: 0.8539, val_recall: 0.8487, val_f1: 0.8479
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.8093
--
confusion matrix
[[850  54   7   6  12   2  10   7  28  24]
 [  2 964   0   0   2   1   2   0   5  24]
 [ 89   6 661  21  48  43 102  16   6   8]
 [ 29  12  60 612  33 113 104  15  11  11]
 [ 14   4  52  23 791  21  56  34   3   2]
 [  7   7  46 101  27 739  45  20   4   4]
 [  4   4  19   8   7  10 939   2   6   1]
 [ 21   6  24  26  41  39  24 810   1   8]
 [ 46  65   6   3   3   0   5   1 858  13]
 [ 11 104   0   2   0   3   3   2   6 869]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.79      0.85      0.82      1000
  automobile       0.79      0.96      0.87      1000
        bird       0.76      0.66      0.71      1000
         cat       0.76      0.61      0.68      1000
        deer       0.82      0.79      0.81      1000
         dog       0.76      0.74      0.75      1000
        frog       0.73      0.94      0.82      1000
       horse       0.89      0.81      0.85      1000
        ship       0.92      0.86      0.89      1000
       truck       0.90      0.87      0.88      1000

    accuracy                           0.81     10000
   macro avg       0.81      0.81      0.81     10000
weighted avg       0.81      0.81      0.81     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.8126
--
confusion matrix
[[410  23   5   2   5   2   3   5  20  13]
 [  1 500   0   0   0   2   1   0   0   8]
 [ 39   6 379   7  25  19  37  10   4   6]
 [ 14  10  32 271  10  60  58   5   2   9]
 [  8   1  22  13 378  16  17  14   0   2]
 [  3   5  26  58  14 372  29   4   3   0]
 [  3   5  10   4   9   3 470   2   0   1]
 [  7   5  14   8  20  19   7 413   1   6]
 [ 25  28   1   3   1   0   3   0 438   5]
 [  3  59   1   1   0   1   2   0   2 432]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.80      0.84      0.82       488
  automobile       0.78      0.98      0.87       512
        bird       0.77      0.71      0.74       532
         cat       0.74      0.58      0.65       471
        deer       0.82      0.80      0.81       471
         dog       0.75      0.72      0.74       514
        frog       0.75      0.93      0.83       507
       horse       0.91      0.83      0.87       500
        ship       0.93      0.87      0.90       504
       truck       0.90      0.86      0.88       501

    accuracy                           0.81      5000
   macro avg       0.82      0.81      0.81      5000
weighted avg       0.82      0.81      0.81      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423926: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:25:18 2024
Job was executed on host(s) <hgn41>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:27:44 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:27:44 2024
Terminated at Wed Feb 28 09:04:52 2024
Results reported at Wed Feb 28 09:04:52 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/ResNet_18_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/ResNet_18_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3887.00 sec.
    Max Memory :                                 3577 MB
    Average Memory :                             3434.84 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6663.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   2231 sec.
    Turnaround time :                            2374 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/ResNet_18_err_423926> for stderr output of this job.

