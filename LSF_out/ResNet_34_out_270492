loading ...
loaded conda.sh
sh shell detected
main => start
 
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Train/Validation random split => start
 
DataLoader => start
 
To_device => start
 
Train => start
 
Epoch [0], train_loss: 1.4924, train_acc: 0.4579, val_loss: 1.3182, val_acc: 0.5479
Epoch [1], train_loss: 0.9777, train_acc: 0.6527, val_loss: 0.8290, val_acc: 0.7030
Epoch [2], train_loss: 0.7165, train_acc: 0.7475, val_loss: 0.9283, val_acc: 0.7270
Epoch [3], train_loss: 0.5672, train_acc: 0.8029, val_loss: 0.6430, val_acc: 0.7860
Epoch [4], train_loss: 0.4463, train_acc: 0.8456, val_loss: 0.5565, val_acc: 0.8129
Epoch [5], train_loss: 0.3578, train_acc: 0.8760, val_loss: 0.5336, val_acc: 0.8219
Epoch [6], train_loss: 0.2804, train_acc: 0.9019, val_loss: 0.5778, val_acc: 0.8139
Epoch [7], train_loss: 0.2182, train_acc: 0.9234, val_loss: 0.5569, val_acc: 0.8356
Epoch [8], train_loss: 0.1626, train_acc: 0.9431, val_loss: 0.5871, val_acc: 0.8269
Epoch [9], train_loss: 0.1249, train_acc: 0.9565, val_loss: 0.6297, val_acc: 0.8291
Epoch [10], train_loss: 0.0928, train_acc: 0.9678, val_loss: 0.7191, val_acc: 0.8229
Epoch [11], train_loss: 0.0766, train_acc: 0.9735, val_loss: 0.7323, val_acc: 0.8268
Epoch [12], train_loss: 0.0689, train_acc: 0.9751, val_loss: 0.7369, val_acc: 0.8325
Epoch [13], train_loss: 0.0624, train_acc: 0.9777, val_loss: 0.7160, val_acc: 0.8365
Epoch [14], train_loss: 0.0482, train_acc: 0.9835, val_loss: 0.6716, val_acc: 0.8393
Epoch [15], train_loss: 0.0457, train_acc: 0.9833, val_loss: 0.7440, val_acc: 0.8437
Epoch [16], train_loss: 0.0442, train_acc: 0.9847, val_loss: 0.7602, val_acc: 0.8461
Epoch [17], train_loss: 0.0425, train_acc: 0.9851, val_loss: 0.7447, val_acc: 0.8345
Epoch [18], train_loss: 0.0300, train_acc: 0.9900, val_loss: 0.7285, val_acc: 0.8531
Epoch [19], train_loss: 0.0461, train_acc: 0.9840, val_loss: 0.7340, val_acc: 0.8471
Epoch [20], train_loss: 0.0304, train_acc: 0.9894, val_loss: 0.7496, val_acc: 0.8460
Epoch [21], train_loss: 0.0401, train_acc: 0.9865, val_loss: 0.7588, val_acc: 0.8424
Epoch [22], train_loss: 0.0318, train_acc: 0.9894, val_loss: 0.7531, val_acc: 0.8469
Epoch [23], train_loss: 0.0246, train_acc: 0.9921, val_loss: 0.7985, val_acc: 0.8447
Epoch [24], train_loss: 0.0181, train_acc: 0.9942, val_loss: 0.8956, val_acc: 0.8343
Epoch [25], train_loss: 0.0358, train_acc: 0.9873, val_loss: 0.7657, val_acc: 0.8416
Epoch [26], train_loss: 0.0190, train_acc: 0.9939, val_loss: 0.7771, val_acc: 0.8412
Epoch [27], train_loss: 0.0194, train_acc: 0.9935, val_loss: 0.8799, val_acc: 0.8417
Epoch [28], train_loss: 0.0338, train_acc: 0.9886, val_loss: 0.8207, val_acc: 0.8457
Epoch [29], train_loss: 0.0195, train_acc: 0.9930, val_loss: 0.8222, val_acc: 0.8399
Epoch [30], train_loss: 0.0215, train_acc: 0.9928, val_loss: 0.9308, val_acc: 0.8389
Epoch [31], train_loss: 0.0258, train_acc: 0.9910, val_loss: 0.7963, val_acc: 0.8454
Epoch [32], train_loss: 0.0151, train_acc: 0.9950, val_loss: 0.8364, val_acc: 0.8443
Epoch [33], train_loss: 0.0170, train_acc: 0.9941, val_loss: 0.8071, val_acc: 0.8577
Epoch [34], train_loss: 0.0202, train_acc: 0.9927, val_loss: 0.9176, val_acc: 0.8392
Epoch [35], train_loss: 0.0184, train_acc: 0.9938, val_loss: 0.8492, val_acc: 0.8510
Epoch [36], train_loss: 0.0203, train_acc: 0.9926, val_loss: 0.8003, val_acc: 0.8472
Epoch [37], train_loss: 0.0134, train_acc: 0.9956, val_loss: 0.8193, val_acc: 0.8480
Epoch [38], train_loss: 0.0131, train_acc: 0.9957, val_loss: 0.8452, val_acc: 0.8523
Epoch [39], train_loss: 0.0160, train_acc: 0.9944, val_loss: 0.9936, val_acc: 0.8400
Epoch [40], train_loss: 0.0205, train_acc: 0.9936, val_loss: 0.8086, val_acc: 0.8527
Epoch [41], train_loss: 0.0103, train_acc: 0.9967, val_loss: 0.8497, val_acc: 0.8457
Epoch [42], train_loss: 0.0210, train_acc: 0.9927, val_loss: 0.8975, val_acc: 0.8439
Epoch [43], train_loss: 0.0149, train_acc: 0.9948, val_loss: 0.8288, val_acc: 0.8566
Epoch [44], train_loss: 0.0069, train_acc: 0.9977, val_loss: 0.9193, val_acc: 0.8485
Epoch [45], train_loss: 0.0142, train_acc: 0.9953, val_loss: 0.8621, val_acc: 0.8496
Epoch [46], train_loss: 0.0135, train_acc: 0.9955, val_loss: 0.9557, val_acc: 0.8346
Epoch [47], train_loss: 0.0172, train_acc: 0.9942, val_loss: 0.8789, val_acc: 0.8503
Epoch [48], train_loss: 0.0095, train_acc: 0.9967, val_loss: 0.8594, val_acc: 0.8548
Epoch [49], train_loss: 0.0107, train_acc: 0.9964, val_loss: 0.8417, val_acc: 0.8553
Epoch [50], train_loss: 0.0080, train_acc: 0.9972, val_loss: 0.8938, val_acc: 0.8475
Epoch [51], train_loss: 0.0134, train_acc: 0.9957, val_loss: 0.9748, val_acc: 0.8408
Epoch [52], train_loss: 0.0173, train_acc: 0.9941, val_loss: 0.8207, val_acc: 0.8543
Epoch [53], train_loss: 0.0088, train_acc: 0.9972, val_loss: 0.8337, val_acc: 0.8560
Epoch [54], train_loss: 0.0081, train_acc: 0.9971, val_loss: 0.8629, val_acc: 0.8526
Epoch [55], train_loss: 0.0116, train_acc: 0.9959, val_loss: 0.9358, val_acc: 0.8459
Epoch [56], train_loss: 0.0155, train_acc: 0.9947, val_loss: 0.8742, val_acc: 0.8491
Epoch [57], train_loss: 0.0079, train_acc: 0.9976, val_loss: 0.8814, val_acc: 0.8504
Epoch [58], train_loss: 0.0034, train_acc: 0.9990, val_loss: 0.8594, val_acc: 0.8546
Epoch [59], train_loss: 0.0039, train_acc: 0.9989, val_loss: 0.9277, val_acc: 0.8570
Epoch [60], train_loss: 0.0193, train_acc: 0.9942, val_loss: 0.9901, val_acc: 0.8361
Epoch [61], train_loss: 0.0129, train_acc: 0.9956, val_loss: 0.8316, val_acc: 0.8521
Epoch [62], train_loss: 0.0051, train_acc: 0.9984, val_loss: 0.8296, val_acc: 0.8607
Epoch [63], train_loss: 0.0022, train_acc: 0.9994, val_loss: 0.8719, val_acc: 0.8562
Epoch [64], train_loss: 0.0127, train_acc: 0.9961, val_loss: 1.2378, val_acc: 0.8179
Epoch [65], train_loss: 0.0208, train_acc: 0.9933, val_loss: 0.8899, val_acc: 0.8590
Epoch [66], train_loss: 0.0057, train_acc: 0.9980, val_loss: 0.8800, val_acc: 0.8543
Epoch [67], train_loss: 0.0044, train_acc: 0.9984, val_loss: 0.8758, val_acc: 0.8579
Epoch [68], train_loss: 0.0041, train_acc: 0.9985, val_loss: 1.0626, val_acc: 0.8422
Epoch [69], train_loss: 0.0097, train_acc: 0.9969, val_loss: 0.9690, val_acc: 0.8488
Epoch [70], train_loss: 0.0093, train_acc: 0.9968, val_loss: 1.1000, val_acc: 0.8342
Epoch [71], train_loss: 0.0119, train_acc: 0.9962, val_loss: 0.9755, val_acc: 0.8459
Epoch [72], train_loss: 0.0085, train_acc: 0.9968, val_loss: 0.8893, val_acc: 0.8504
Epoch [73], train_loss: 0.0032, train_acc: 0.9988, val_loss: 0.9163, val_acc: 0.8551
Epoch [74], train_loss: 0.0052, train_acc: 0.9982, val_loss: 1.0214, val_acc: 0.8451
Epoch [75], train_loss: 0.0079, train_acc: 0.9972, val_loss: 1.0388, val_acc: 0.8432
Epoch [76], train_loss: 0.0089, train_acc: 0.9972, val_loss: 1.0006, val_acc: 0.8499
Epoch [77], train_loss: 0.0096, train_acc: 0.9971, val_loss: 0.9827, val_acc: 0.8496
Epoch [78], train_loss: 0.0069, train_acc: 0.9978, val_loss: 0.9512, val_acc: 0.8523
Epoch [79], train_loss: 0.0095, train_acc: 0.9967, val_loss: 0.9849, val_acc: 0.8510
Epoch [80], train_loss: 0.0074, train_acc: 0.9974, val_loss: 0.9613, val_acc: 0.8524
Epoch [81], train_loss: 0.0012, train_acc: 0.9995, val_loss: 0.9286, val_acc: 0.8582
Epoch [82], train_loss: 0.0015, train_acc: 0.9996, val_loss: 1.0031, val_acc: 0.8576
Epoch [83], train_loss: 0.0161, train_acc: 0.9947, val_loss: 0.9474, val_acc: 0.8496
Epoch [84], train_loss: 0.0077, train_acc: 0.9974, val_loss: 0.8710, val_acc: 0.8581
Epoch [85], train_loss: 0.0039, train_acc: 0.9988, val_loss: 0.9424, val_acc: 0.8505
Epoch [86], train_loss: 0.0029, train_acc: 0.9991, val_loss: 0.9552, val_acc: 0.8518
Epoch [87], train_loss: 0.0059, train_acc: 0.9979, val_loss: 1.0595, val_acc: 0.8474
Epoch [88], train_loss: 0.0107, train_acc: 0.9965, val_loss: 1.0124, val_acc: 0.8457
Epoch [89], train_loss: 0.0079, train_acc: 0.9973, val_loss: 0.8783, val_acc: 0.8549
Epoch [90], train_loss: 0.0052, train_acc: 0.9982, val_loss: 0.9671, val_acc: 0.8533
Epoch [91], train_loss: 0.0021, train_acc: 0.9993, val_loss: 0.9230, val_acc: 0.8599
Epoch [92], train_loss: 0.0040, train_acc: 0.9987, val_loss: 1.0381, val_acc: 0.8535
Epoch [93], train_loss: 0.0072, train_acc: 0.9977, val_loss: 1.0567, val_acc: 0.8403
Epoch [94], train_loss: 0.0101, train_acc: 0.9966, val_loss: 0.9491, val_acc: 0.8526
Epoch [95], train_loss: 0.0048, train_acc: 0.9982, val_loss: 1.0133, val_acc: 0.8520
Epoch [96], train_loss: 0.0036, train_acc: 0.9989, val_loss: 0.9885, val_acc: 0.8580
Epoch [97], train_loss: 0.0043, train_acc: 0.9985, val_loss: 0.9702, val_acc: 0.8568
Epoch [98], train_loss: 0.0017, train_acc: 0.9994, val_loss: 0.9511, val_acc: 0.8645
Epoch [99], train_loss: 0.0056, train_acc: 0.9983, val_loss: 1.1009, val_acc: 0.8408
 
Visualize trining => save images
 
Load the model => start
 
Check best/last models => start
 
Summary result of test set => best model  {'val_loss': 0.5751696825027466, 'val_acc': 0.811328113079071}
Summary result of test set => last model {'val_loss': 1.1440445184707642, 'val_acc': 0.8359375}
Test set evaluation => save results for postprocessing
 
** accuracy: 0.810
--
confusion matrix
[[827  17  23  15  23   2   5  17  39  32]
 [ 11 939   0   3   1   2   1   0  15  28]
 [ 66   3 550  46 100  87  67  69  10   2]
 [ 12   6  14 630  55 169  63  25  15  11]
 [ 11   3  17  22 831  32  24  55   4   1]
 [  3   3   6 101  35 796  21  26   4   5]
 [ 10   8   9  37  31  24 847  19  13   2]
 [  9   0   2  25  25  55   3 869   6   6]
 [ 47  11   1   3   1   3   3   2 917  12]
 [ 15  65   1   3   0   3   0   3  18 892]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.82      0.83      0.82      1000
  automobile       0.89      0.94      0.91      1000
        bird       0.88      0.55      0.68      1000
         cat       0.71      0.63      0.67      1000
        deer       0.75      0.83      0.79      1000
         dog       0.68      0.80      0.73      1000
        frog       0.82      0.85      0.83      1000
       horse       0.80      0.87      0.83      1000
        ship       0.88      0.92      0.90      1000
       truck       0.90      0.89      0.90      1000

    accuracy                           0.81     10000
   macro avg       0.81      0.81      0.81     10000
weighted avg       0.81      0.81      0.81     10000

END OF CODE
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 270492: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <access4> by user <ingap> in cluster <wexac> at Tue Feb 27 11:11:29 2024
Job was executed on host(s) <hgn55>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 11:12:45 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 11:12:45 2024
Terminated at Tue Feb 27 11:54:36 2024
Results reported at Tue Feb 27 11:54:36 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/ResNet_34_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/ResNet_34_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4269.00 sec.
    Max Memory :                                 3585 MB
    Average Memory :                             3492.51 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6655.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                76
    Run time :                                   2511 sec.
    Turnaround time :                            2587 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/ResNet_34_err_270492> for stderr output of this job.

