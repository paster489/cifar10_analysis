loading ...
loaded conda.sh
sh shell detected
main => start
 
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Train/Validation random split => start
 
DataLoader => start
 
To_device => start
 
Train => start
 
Epoch [0], train_loss: 2.0272, train_acc: 0.2530, val_loss: 1.9341, val_acc: 0.2770
Epoch [1], train_loss: 1.9007, train_acc: 0.3088, val_loss: 1.8647, val_acc: 0.3359
Epoch [2], train_loss: 1.8506, train_acc: 0.3298, val_loss: 1.8230, val_acc: 0.3401
Epoch [3], train_loss: 1.8089, train_acc: 0.3463, val_loss: 1.7772, val_acc: 0.3626
Epoch [4], train_loss: 1.7821, train_acc: 0.3621, val_loss: 1.8047, val_acc: 0.3496
Epoch [5], train_loss: 1.7738, train_acc: 0.3606, val_loss: 1.7368, val_acc: 0.3713
Epoch [6], train_loss: 1.7527, train_acc: 0.3666, val_loss: 1.8735, val_acc: 0.3080
Epoch [7], train_loss: 1.7715, train_acc: 0.3676, val_loss: 1.7305, val_acc: 0.3762
Epoch [8], train_loss: 1.7674, train_acc: 0.3685, val_loss: 1.7275, val_acc: 0.3736
Epoch [9], train_loss: 1.7644, train_acc: 0.3674, val_loss: 1.7817, val_acc: 0.3581
Epoch [10], train_loss: 1.8149, train_acc: 0.3449, val_loss: 1.7727, val_acc: 0.3663
Epoch [11], train_loss: 1.7620, train_acc: 0.3636, val_loss: 1.7017, val_acc: 0.3903
Epoch [12], train_loss: 1.7301, train_acc: 0.3786, val_loss: 1.7061, val_acc: 0.3864
Epoch [13], train_loss: 1.7349, train_acc: 0.3766, val_loss: 1.7208, val_acc: 0.3818
Epoch [14], train_loss: 1.7293, train_acc: 0.3773, val_loss: 1.7751, val_acc: 0.3606
Epoch [15], train_loss: 1.7747, train_acc: 0.3612, val_loss: 1.7383, val_acc: 0.3735
Epoch [16], train_loss: 1.7327, train_acc: 0.3757, val_loss: 1.6947, val_acc: 0.3879
Epoch [17], train_loss: 1.7081, train_acc: 0.3840, val_loss: 1.6974, val_acc: 0.3991
Epoch [18], train_loss: 1.7185, train_acc: 0.3779, val_loss: 1.7445, val_acc: 0.3716
Epoch [19], train_loss: 1.7497, train_acc: 0.3687, val_loss: 1.7883, val_acc: 0.3576
Epoch [20], train_loss: 1.7992, train_acc: 0.3510, val_loss: 1.7666, val_acc: 0.3641
Epoch [21], train_loss: 1.7331, train_acc: 0.3730, val_loss: 1.6990, val_acc: 0.3845
Epoch [22], train_loss: 1.6914, train_acc: 0.3925, val_loss: 1.6870, val_acc: 0.3891
Epoch [23], train_loss: 1.6683, train_acc: 0.3994, val_loss: 1.6599, val_acc: 0.4013
Epoch [24], train_loss: 1.6751, train_acc: 0.3945, val_loss: 1.6400, val_acc: 0.4110
Epoch [25], train_loss: 1.6475, train_acc: 0.4059, val_loss: 1.6211, val_acc: 0.4169
Epoch [26], train_loss: 1.6340, train_acc: 0.4137, val_loss: 1.6073, val_acc: 0.4185
Epoch [27], train_loss: 1.6275, train_acc: 0.4146, val_loss: 1.5925, val_acc: 0.4206
Epoch [28], train_loss: 1.6370, train_acc: 0.4096, val_loss: 1.6527, val_acc: 0.4041
Epoch [29], train_loss: 1.6191, train_acc: 0.4181, val_loss: 1.6198, val_acc: 0.4163
Epoch [30], train_loss: 1.6150, train_acc: 0.4186, val_loss: 1.6100, val_acc: 0.4159
Epoch [31], train_loss: 1.6217, train_acc: 0.4185, val_loss: 1.6766, val_acc: 0.3958
Epoch [32], train_loss: 1.6397, train_acc: 0.4138, val_loss: 1.5970, val_acc: 0.4212
Epoch [33], train_loss: 1.6043, train_acc: 0.4243, val_loss: 1.6062, val_acc: 0.4164
Epoch [34], train_loss: 1.5808, train_acc: 0.4331, val_loss: 1.5922, val_acc: 0.4220
Epoch [35], train_loss: 1.5803, train_acc: 0.4331, val_loss: 1.6071, val_acc: 0.4262
Epoch [36], train_loss: 1.5895, train_acc: 0.4278, val_loss: 1.6474, val_acc: 0.4108
Epoch [37], train_loss: 1.6065, train_acc: 0.4216, val_loss: 1.5979, val_acc: 0.4229
Epoch [38], train_loss: 1.5759, train_acc: 0.4324, val_loss: 1.5633, val_acc: 0.4375
Epoch [39], train_loss: 1.5802, train_acc: 0.4311, val_loss: 1.6320, val_acc: 0.4153
Epoch [40], train_loss: 1.5957, train_acc: 0.4243, val_loss: 1.5806, val_acc: 0.4259
Epoch [41], train_loss: 1.5697, train_acc: 0.4355, val_loss: 1.5600, val_acc: 0.4297
Epoch [42], train_loss: 1.5611, train_acc: 0.4394, val_loss: 1.5564, val_acc: 0.4455
Epoch [43], train_loss: 1.5568, train_acc: 0.4408, val_loss: 1.5687, val_acc: 0.4346
Epoch [44], train_loss: 1.5390, train_acc: 0.4485, val_loss: 1.6203, val_acc: 0.4114
Epoch [45], train_loss: 1.5875, train_acc: 0.4312, val_loss: 1.6093, val_acc: 0.4241
Epoch [46], train_loss: 1.5715, train_acc: 0.4334, val_loss: 1.5696, val_acc: 0.4340
Epoch [47], train_loss: 1.5562, train_acc: 0.4411, val_loss: 1.5854, val_acc: 0.4334
Epoch [48], train_loss: 1.5634, train_acc: 0.4372, val_loss: 1.6455, val_acc: 0.4160
Epoch [49], train_loss: 1.5654, train_acc: 0.4385, val_loss: 1.5823, val_acc: 0.4291
Epoch [50], train_loss: 1.5549, train_acc: 0.4420, val_loss: 1.5739, val_acc: 0.4311
Epoch [51], train_loss: 1.5542, train_acc: 0.4436, val_loss: 1.5826, val_acc: 0.4217
Epoch [52], train_loss: 1.5567, train_acc: 0.4421, val_loss: 1.5825, val_acc: 0.4285
Epoch [53], train_loss: 1.5311, train_acc: 0.4501, val_loss: 1.5481, val_acc: 0.4402
Epoch [54], train_loss: 1.5198, train_acc: 0.4536, val_loss: 1.5713, val_acc: 0.4470
Epoch [55], train_loss: 1.5457, train_acc: 0.4458, val_loss: 1.5813, val_acc: 0.4394
Epoch [56], train_loss: 1.5403, train_acc: 0.4477, val_loss: 1.5522, val_acc: 0.4426
Epoch [57], train_loss: 1.5417, train_acc: 0.4432, val_loss: 1.5617, val_acc: 0.4422
Epoch [58], train_loss: 1.5439, train_acc: 0.4461, val_loss: 1.6415, val_acc: 0.4114
Epoch [59], train_loss: 1.5812, train_acc: 0.4370, val_loss: 1.6379, val_acc: 0.3969
Epoch [60], train_loss: 1.5681, train_acc: 0.4370, val_loss: 1.5825, val_acc: 0.4277
Epoch [61], train_loss: 1.5389, train_acc: 0.4485, val_loss: 1.5572, val_acc: 0.4359
Epoch [62], train_loss: 1.5398, train_acc: 0.4473, val_loss: 1.5592, val_acc: 0.4370
Epoch [63], train_loss: 1.5136, train_acc: 0.4583, val_loss: 1.5449, val_acc: 0.4434
Epoch [64], train_loss: 1.5155, train_acc: 0.4556, val_loss: 1.5388, val_acc: 0.4466
Epoch [65], train_loss: 1.5083, train_acc: 0.4585, val_loss: 1.5529, val_acc: 0.4400
Epoch [66], train_loss: 1.5104, train_acc: 0.4594, val_loss: 1.5442, val_acc: 0.4525
Epoch [67], train_loss: 1.4965, train_acc: 0.4666, val_loss: 1.6388, val_acc: 0.4027
Epoch [68], train_loss: 1.4986, train_acc: 0.4619, val_loss: 1.5460, val_acc: 0.4493
Epoch [69], train_loss: 1.4864, train_acc: 0.4662, val_loss: 1.5408, val_acc: 0.4434
Epoch [70], train_loss: 1.4900, train_acc: 0.4675, val_loss: 1.5497, val_acc: 0.4471
Epoch [71], train_loss: 1.4817, train_acc: 0.4699, val_loss: 1.5417, val_acc: 0.4448
Epoch [72], train_loss: 1.5240, train_acc: 0.4543, val_loss: 1.6823, val_acc: 0.4066
Epoch [73], train_loss: 1.5666, train_acc: 0.4389, val_loss: 1.5904, val_acc: 0.4284
Epoch [74], train_loss: 1.6658, train_acc: 0.4000, val_loss: 1.6533, val_acc: 0.4026
Epoch [75], train_loss: 1.6155, train_acc: 0.4207, val_loss: 1.6452, val_acc: 0.4068
Epoch [76], train_loss: 1.5883, train_acc: 0.4309, val_loss: 1.6063, val_acc: 0.4231
Epoch [77], train_loss: 1.5624, train_acc: 0.4404, val_loss: 1.5619, val_acc: 0.4455
Epoch [78], train_loss: 1.5426, train_acc: 0.4471, val_loss: 1.5613, val_acc: 0.4304
Epoch [79], train_loss: 1.6208, train_acc: 0.4221, val_loss: 1.6010, val_acc: 0.4171
Epoch [80], train_loss: 1.6377, train_acc: 0.4109, val_loss: 1.6871, val_acc: 0.3898
Epoch [81], train_loss: 1.6368, train_acc: 0.4090, val_loss: 1.6078, val_acc: 0.4191
Epoch [82], train_loss: 1.6151, train_acc: 0.4202, val_loss: 1.6123, val_acc: 0.4169
Epoch [83], train_loss: 1.6349, train_acc: 0.4147, val_loss: 1.6204, val_acc: 0.4112
Epoch [84], train_loss: 1.6115, train_acc: 0.4205, val_loss: 1.6050, val_acc: 0.4240
Epoch [85], train_loss: 1.6070, train_acc: 0.4217, val_loss: 1.6100, val_acc: 0.4260
Epoch [86], train_loss: 1.6112, train_acc: 0.4222, val_loss: 1.6249, val_acc: 0.4160
Epoch [87], train_loss: 1.6224, train_acc: 0.4158, val_loss: 1.6105, val_acc: 0.4241
Epoch [88], train_loss: 1.6226, train_acc: 0.4156, val_loss: 1.6200, val_acc: 0.4191
Epoch [89], train_loss: 1.6183, train_acc: 0.4219, val_loss: 1.5999, val_acc: 0.4215
Epoch [90], train_loss: 1.6087, train_acc: 0.4241, val_loss: 1.6014, val_acc: 0.4222
Epoch [91], train_loss: 1.6098, train_acc: 0.4224, val_loss: 1.6320, val_acc: 0.4012
Epoch [92], train_loss: 1.6091, train_acc: 0.4218, val_loss: 1.6132, val_acc: 0.4113
Epoch [93], train_loss: 1.6039, train_acc: 0.4242, val_loss: 1.6298, val_acc: 0.4109
Epoch [94], train_loss: 1.5937, train_acc: 0.4270, val_loss: 1.6283, val_acc: 0.4077
Epoch [95], train_loss: 1.6141, train_acc: 0.4187, val_loss: 1.6093, val_acc: 0.4163
Epoch [96], train_loss: 1.6146, train_acc: 0.4200, val_loss: 1.7005, val_acc: 0.3763
Epoch [97], train_loss: 1.6267, train_acc: 0.4137, val_loss: 1.6242, val_acc: 0.4124
Epoch [98], train_loss: 1.6109, train_acc: 0.4197, val_loss: 1.5973, val_acc: 0.4272
Epoch [99], train_loss: 1.5979, train_acc: 0.4273, val_loss: 1.5975, val_acc: 0.4319
 
Visualize trining => save images
 
Load the model => start
 
Check best/last models => start
 
Summary result of test set => best model  {'val_loss': 1.5275087356567383, 'val_acc': 0.4583984315395355}
Summary result of test set => last model {'val_loss': 1.5734896659851074, 'val_acc': 0.4371093809604645}
Test set evaluation => save results for postprocessing
 
** accuracy: 0.457
--
confusion matrix
[[516  54  44  32  25  25  15  55 180  54]
 [ 38 574  14  56  11  30  12  29  91 145]
 [ 92  47 274  76 154  82  99 105  44  27]
 [ 37  46  45 349  62 189 120  75  33  44]
 [ 40  21 102  53 452  46 119 112  37  18]
 [ 24  35  70 232  73 340  77  71  53  25]
 [ 11  26  78 104 155  61 499  34  14  18]
 [ 55  28  41  87 119  54  50 485  30  51]
 [ 74  88  15  38  23  26   7  23 633  73]
 [ 59 221  10  53   8  23  25  61  88 452]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.55      0.52      0.53      1000
  automobile       0.50      0.57      0.54      1000
        bird       0.40      0.27      0.32      1000
         cat       0.32      0.35      0.34      1000
        deer       0.42      0.45      0.43      1000
         dog       0.39      0.34      0.36      1000
        frog       0.49      0.50      0.49      1000
       horse       0.46      0.48      0.47      1000
        ship       0.53      0.63      0.57      1000
       truck       0.50      0.45      0.47      1000

    accuracy                           0.46     10000
   macro avg       0.45      0.46      0.45     10000
weighted avg       0.45      0.46      0.45     10000

END OF CODE
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 270509: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <access4> by user <ingap> in cluster <wexac> at Tue Feb 27 11:11:47 2024
Job was executed on host(s) <hgn53>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 11:12:50 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 11:12:50 2024
Terminated at Tue Feb 27 11:55:15 2024
Results reported at Tue Feb 27 11:55:15 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/ViT_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/ViT_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4053.00 sec.
    Max Memory :                                 2984 MB
    Average Memory :                             2709.28 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               7256.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   2552 sec.
    Turnaround time :                            2608 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/ViT_err_270509> for stderr output of this job.

