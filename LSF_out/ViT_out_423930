loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 2.0272, train_acc: 0.2530, val_loss: 1.9341, val_acc: 0.2770, val_precision: 0.2650, val_recall: 0.2770, val_f1: 0.2320
Epoch [1], train_loss: 1.9007, train_acc: 0.3088, val_loss: 1.8647, val_acc: 0.3359, val_precision: 0.3241, val_recall: 0.3359, val_f1: 0.3146
Epoch [2], train_loss: 1.8506, train_acc: 0.3298, val_loss: 1.8230, val_acc: 0.3401, val_precision: 0.3783, val_recall: 0.3401, val_f1: 0.3330
Epoch [3], train_loss: 1.8089, train_acc: 0.3463, val_loss: 1.7772, val_acc: 0.3626, val_precision: 0.3780, val_recall: 0.3626, val_f1: 0.3557
Epoch [4], train_loss: 1.7821, train_acc: 0.3621, val_loss: 1.8047, val_acc: 0.3496, val_precision: 0.3572, val_recall: 0.3496, val_f1: 0.3303
Epoch [5], train_loss: 1.7738, train_acc: 0.3606, val_loss: 1.7368, val_acc: 0.3713, val_precision: 0.3803, val_recall: 0.3713, val_f1: 0.3678
Epoch [6], train_loss: 1.7527, train_acc: 0.3666, val_loss: 1.8735, val_acc: 0.3080, val_precision: 0.3627, val_recall: 0.3080, val_f1: 0.2934
Epoch [7], train_loss: 1.7715, train_acc: 0.3676, val_loss: 1.7305, val_acc: 0.3762, val_precision: 0.3760, val_recall: 0.3762, val_f1: 0.3574
Epoch [8], train_loss: 1.7674, train_acc: 0.3685, val_loss: 1.7275, val_acc: 0.3736, val_precision: 0.4002, val_recall: 0.3736, val_f1: 0.3693
Epoch [9], train_loss: 1.7644, train_acc: 0.3674, val_loss: 1.7817, val_acc: 0.3581, val_precision: 0.3780, val_recall: 0.3581, val_f1: 0.3468
Epoch [10], train_loss: 1.8149, train_acc: 0.3449, val_loss: 1.7727, val_acc: 0.3663, val_precision: 0.3769, val_recall: 0.3663, val_f1: 0.3386
Epoch [11], train_loss: 1.7620, train_acc: 0.3636, val_loss: 1.7017, val_acc: 0.3903, val_precision: 0.3984, val_recall: 0.3903, val_f1: 0.3774
Epoch [12], train_loss: 1.7301, train_acc: 0.3786, val_loss: 1.7061, val_acc: 0.3864, val_precision: 0.4059, val_recall: 0.3864, val_f1: 0.3712
Epoch [13], train_loss: 1.7349, train_acc: 0.3766, val_loss: 1.7208, val_acc: 0.3818, val_precision: 0.3982, val_recall: 0.3818, val_f1: 0.3661
Epoch [14], train_loss: 1.7293, train_acc: 0.3773, val_loss: 1.7751, val_acc: 0.3606, val_precision: 0.3856, val_recall: 0.3606, val_f1: 0.3560
Epoch [15], train_loss: 1.7747, train_acc: 0.3612, val_loss: 1.7383, val_acc: 0.3735, val_precision: 0.3948, val_recall: 0.3735, val_f1: 0.3591
Epoch [16], train_loss: 1.7327, train_acc: 0.3757, val_loss: 1.6947, val_acc: 0.3879, val_precision: 0.3940, val_recall: 0.3879, val_f1: 0.3716
Epoch [17], train_loss: 1.7081, train_acc: 0.3840, val_loss: 1.6974, val_acc: 0.3991, val_precision: 0.4045, val_recall: 0.3991, val_f1: 0.3906
Epoch [18], train_loss: 1.7185, train_acc: 0.3779, val_loss: 1.7445, val_acc: 0.3716, val_precision: 0.3678, val_recall: 0.3716, val_f1: 0.3531
Epoch [19], train_loss: 1.7497, train_acc: 0.3687, val_loss: 1.7883, val_acc: 0.3576, val_precision: 0.3626, val_recall: 0.3576, val_f1: 0.3352
Epoch [20], train_loss: 1.7992, train_acc: 0.3510, val_loss: 1.7666, val_acc: 0.3641, val_precision: 0.3700, val_recall: 0.3641, val_f1: 0.3490
Epoch [21], train_loss: 1.7331, train_acc: 0.3730, val_loss: 1.6990, val_acc: 0.3845, val_precision: 0.4122, val_recall: 0.3845, val_f1: 0.3703
Epoch [22], train_loss: 1.6914, train_acc: 0.3925, val_loss: 1.6870, val_acc: 0.3891, val_precision: 0.4008, val_recall: 0.3891, val_f1: 0.3691
Epoch [23], train_loss: 1.6683, train_acc: 0.3994, val_loss: 1.6599, val_acc: 0.4013, val_precision: 0.4156, val_recall: 0.4013, val_f1: 0.3945
Epoch [24], train_loss: 1.6751, train_acc: 0.3945, val_loss: 1.6400, val_acc: 0.4110, val_precision: 0.4180, val_recall: 0.4110, val_f1: 0.4051
Epoch [25], train_loss: 1.6475, train_acc: 0.4059, val_loss: 1.6211, val_acc: 0.4169, val_precision: 0.4195, val_recall: 0.4169, val_f1: 0.4090
Epoch [26], train_loss: 1.6340, train_acc: 0.4137, val_loss: 1.6073, val_acc: 0.4185, val_precision: 0.4225, val_recall: 0.4185, val_f1: 0.4025
Epoch [27], train_loss: 1.6275, train_acc: 0.4146, val_loss: 1.5925, val_acc: 0.4206, val_precision: 0.4205, val_recall: 0.4206, val_f1: 0.4087
Epoch [28], train_loss: 1.6370, train_acc: 0.4096, val_loss: 1.6527, val_acc: 0.4041, val_precision: 0.4283, val_recall: 0.4041, val_f1: 0.3925
Epoch [29], train_loss: 1.6191, train_acc: 0.4181, val_loss: 1.6198, val_acc: 0.4163, val_precision: 0.4313, val_recall: 0.4163, val_f1: 0.4063
Epoch [30], train_loss: 1.6150, train_acc: 0.4186, val_loss: 1.6100, val_acc: 0.4159, val_precision: 0.4232, val_recall: 0.4159, val_f1: 0.4087
Epoch [31], train_loss: 1.6217, train_acc: 0.4185, val_loss: 1.6766, val_acc: 0.3958, val_precision: 0.4099, val_recall: 0.3958, val_f1: 0.3801
Epoch [32], train_loss: 1.6397, train_acc: 0.4138, val_loss: 1.5970, val_acc: 0.4212, val_precision: 0.4226, val_recall: 0.4212, val_f1: 0.4103
Epoch [33], train_loss: 1.6043, train_acc: 0.4243, val_loss: 1.6062, val_acc: 0.4164, val_precision: 0.4197, val_recall: 0.4164, val_f1: 0.4076
Epoch [34], train_loss: 1.5808, train_acc: 0.4331, val_loss: 1.5922, val_acc: 0.4220, val_precision: 0.4221, val_recall: 0.4220, val_f1: 0.4022
Epoch [35], train_loss: 1.5803, train_acc: 0.4331, val_loss: 1.6071, val_acc: 0.4262, val_precision: 0.4354, val_recall: 0.4262, val_f1: 0.4189
Epoch [36], train_loss: 1.5895, train_acc: 0.4278, val_loss: 1.6474, val_acc: 0.4108, val_precision: 0.4148, val_recall: 0.4108, val_f1: 0.4034
Epoch [37], train_loss: 1.6065, train_acc: 0.4216, val_loss: 1.5979, val_acc: 0.4229, val_precision: 0.4317, val_recall: 0.4229, val_f1: 0.4185
Epoch [38], train_loss: 1.5759, train_acc: 0.4324, val_loss: 1.5633, val_acc: 0.4375, val_precision: 0.4443, val_recall: 0.4375, val_f1: 0.4347
Epoch [39], train_loss: 1.5802, train_acc: 0.4311, val_loss: 1.6320, val_acc: 0.4153, val_precision: 0.4251, val_recall: 0.4153, val_f1: 0.4065
Epoch [40], train_loss: 1.5957, train_acc: 0.4243, val_loss: 1.5806, val_acc: 0.4259, val_precision: 0.4336, val_recall: 0.4259, val_f1: 0.4198
Epoch [41], train_loss: 1.5697, train_acc: 0.4355, val_loss: 1.5600, val_acc: 0.4297, val_precision: 0.4409, val_recall: 0.4297, val_f1: 0.4221
Epoch [42], train_loss: 1.5611, train_acc: 0.4394, val_loss: 1.5564, val_acc: 0.4455, val_precision: 0.4551, val_recall: 0.4455, val_f1: 0.4407
Epoch [43], train_loss: 1.5568, train_acc: 0.4408, val_loss: 1.5687, val_acc: 0.4346, val_precision: 0.4440, val_recall: 0.4346, val_f1: 0.4309
Epoch [44], train_loss: 1.5390, train_acc: 0.4485, val_loss: 1.6203, val_acc: 0.4114, val_precision: 0.4196, val_recall: 0.4114, val_f1: 0.3998
Epoch [45], train_loss: 1.5875, train_acc: 0.4312, val_loss: 1.6093, val_acc: 0.4241, val_precision: 0.4388, val_recall: 0.4241, val_f1: 0.4173
Epoch [46], train_loss: 1.5715, train_acc: 0.4334, val_loss: 1.5696, val_acc: 0.4340, val_precision: 0.4393, val_recall: 0.4340, val_f1: 0.4256
Epoch [47], train_loss: 1.5562, train_acc: 0.4411, val_loss: 1.5854, val_acc: 0.4334, val_precision: 0.4370, val_recall: 0.4334, val_f1: 0.4226
Epoch [48], train_loss: 1.5634, train_acc: 0.4372, val_loss: 1.6455, val_acc: 0.4160, val_precision: 0.4287, val_recall: 0.4160, val_f1: 0.4048
Epoch [49], train_loss: 1.5654, train_acc: 0.4385, val_loss: 1.5823, val_acc: 0.4291, val_precision: 0.4370, val_recall: 0.4291, val_f1: 0.4226
Epoch [50], train_loss: 1.5549, train_acc: 0.4420, val_loss: 1.5739, val_acc: 0.4311, val_precision: 0.4433, val_recall: 0.4311, val_f1: 0.4267
Epoch [51], train_loss: 1.5542, train_acc: 0.4436, val_loss: 1.5826, val_acc: 0.4217, val_precision: 0.4326, val_recall: 0.4217, val_f1: 0.4193
Epoch [52], train_loss: 1.5567, train_acc: 0.4421, val_loss: 1.5825, val_acc: 0.4285, val_precision: 0.4307, val_recall: 0.4285, val_f1: 0.4130
Epoch [53], train_loss: 1.5311, train_acc: 0.4501, val_loss: 1.5481, val_acc: 0.4402, val_precision: 0.4432, val_recall: 0.4402, val_f1: 0.4302
Epoch [54], train_loss: 1.5198, train_acc: 0.4536, val_loss: 1.5713, val_acc: 0.4470, val_precision: 0.4489, val_recall: 0.4470, val_f1: 0.4402
Epoch [55], train_loss: 1.5457, train_acc: 0.4458, val_loss: 1.5813, val_acc: 0.4394, val_precision: 0.4444, val_recall: 0.4394, val_f1: 0.4316
Epoch [56], train_loss: 1.5403, train_acc: 0.4477, val_loss: 1.5522, val_acc: 0.4426, val_precision: 0.4438, val_recall: 0.4426, val_f1: 0.4329
Epoch [57], train_loss: 1.5417, train_acc: 0.4432, val_loss: 1.5617, val_acc: 0.4422, val_precision: 0.4531, val_recall: 0.4422, val_f1: 0.4388
Epoch [58], train_loss: 1.5439, train_acc: 0.4461, val_loss: 1.6415, val_acc: 0.4114, val_precision: 0.4239, val_recall: 0.4114, val_f1: 0.4005
Epoch [59], train_loss: 1.5812, train_acc: 0.4370, val_loss: 1.6379, val_acc: 0.3969, val_precision: 0.4059, val_recall: 0.3969, val_f1: 0.3824
Epoch [60], train_loss: 1.5681, train_acc: 0.4370, val_loss: 1.5825, val_acc: 0.4277, val_precision: 0.4348, val_recall: 0.4277, val_f1: 0.4154
Epoch [61], train_loss: 1.5389, train_acc: 0.4485, val_loss: 1.5572, val_acc: 0.4359, val_precision: 0.4358, val_recall: 0.4359, val_f1: 0.4267
Epoch [62], train_loss: 1.5398, train_acc: 0.4473, val_loss: 1.5592, val_acc: 0.4370, val_precision: 0.4421, val_recall: 0.4370, val_f1: 0.4307
Epoch [63], train_loss: 1.5136, train_acc: 0.4583, val_loss: 1.5449, val_acc: 0.4434, val_precision: 0.4485, val_recall: 0.4434, val_f1: 0.4360
Epoch [64], train_loss: 1.5155, train_acc: 0.4556, val_loss: 1.5388, val_acc: 0.4466, val_precision: 0.4503, val_recall: 0.4466, val_f1: 0.4417
Epoch [65], train_loss: 1.5083, train_acc: 0.4585, val_loss: 1.5529, val_acc: 0.4400, val_precision: 0.4450, val_recall: 0.4400, val_f1: 0.4312
Epoch [66], train_loss: 1.5104, train_acc: 0.4594, val_loss: 1.5442, val_acc: 0.4525, val_precision: 0.4581, val_recall: 0.4525, val_f1: 0.4453
Epoch [67], train_loss: 1.4965, train_acc: 0.4666, val_loss: 1.6388, val_acc: 0.4027, val_precision: 0.4213, val_recall: 0.4027, val_f1: 0.3838
Epoch [68], train_loss: 1.4986, train_acc: 0.4619, val_loss: 1.5460, val_acc: 0.4493, val_precision: 0.4492, val_recall: 0.4493, val_f1: 0.4424
Epoch [69], train_loss: 1.4864, train_acc: 0.4662, val_loss: 1.5408, val_acc: 0.4434, val_precision: 0.4583, val_recall: 0.4434, val_f1: 0.4399
Epoch [70], train_loss: 1.4900, train_acc: 0.4675, val_loss: 1.5497, val_acc: 0.4471, val_precision: 0.4495, val_recall: 0.4471, val_f1: 0.4397
Epoch [71], train_loss: 1.4817, train_acc: 0.4699, val_loss: 1.5417, val_acc: 0.4448, val_precision: 0.4567, val_recall: 0.4448, val_f1: 0.4363
Epoch [72], train_loss: 1.5240, train_acc: 0.4543, val_loss: 1.6823, val_acc: 0.4066, val_precision: 0.4132, val_recall: 0.4066, val_f1: 0.3951
Epoch [73], train_loss: 1.5666, train_acc: 0.4389, val_loss: 1.5904, val_acc: 0.4284, val_precision: 0.4343, val_recall: 0.4284, val_f1: 0.4219
Epoch [74], train_loss: 1.6658, train_acc: 0.4000, val_loss: 1.6533, val_acc: 0.4026, val_precision: 0.4296, val_recall: 0.4026, val_f1: 0.3950
Epoch [75], train_loss: 1.6155, train_acc: 0.4207, val_loss: 1.6452, val_acc: 0.4068, val_precision: 0.4226, val_recall: 0.4068, val_f1: 0.3997
Epoch [76], train_loss: 1.5883, train_acc: 0.4309, val_loss: 1.6063, val_acc: 0.4231, val_precision: 0.4268, val_recall: 0.4231, val_f1: 0.4147
Epoch [77], train_loss: 1.5624, train_acc: 0.4404, val_loss: 1.5619, val_acc: 0.4455, val_precision: 0.4586, val_recall: 0.4455, val_f1: 0.4331
Epoch [78], train_loss: 1.5426, train_acc: 0.4471, val_loss: 1.5613, val_acc: 0.4304, val_precision: 0.4490, val_recall: 0.4304, val_f1: 0.4205
Epoch [79], train_loss: 1.6208, train_acc: 0.4221, val_loss: 1.6010, val_acc: 0.4171, val_precision: 0.4247, val_recall: 0.4171, val_f1: 0.4094
Epoch [80], train_loss: 1.6377, train_acc: 0.4109, val_loss: 1.6871, val_acc: 0.3898, val_precision: 0.4450, val_recall: 0.3898, val_f1: 0.3857
Epoch [81], train_loss: 1.6368, train_acc: 0.4090, val_loss: 1.6078, val_acc: 0.4191, val_precision: 0.4249, val_recall: 0.4191, val_f1: 0.4073
Epoch [82], train_loss: 1.6151, train_acc: 0.4202, val_loss: 1.6123, val_acc: 0.4169, val_precision: 0.4215, val_recall: 0.4169, val_f1: 0.4092
Epoch [83], train_loss: 1.6349, train_acc: 0.4147, val_loss: 1.6204, val_acc: 0.4112, val_precision: 0.4246, val_recall: 0.4112, val_f1: 0.3995
Epoch [84], train_loss: 1.6115, train_acc: 0.4205, val_loss: 1.6050, val_acc: 0.4240, val_precision: 0.4302, val_recall: 0.4240, val_f1: 0.4193
Epoch [85], train_loss: 1.6070, train_acc: 0.4217, val_loss: 1.6100, val_acc: 0.4260, val_precision: 0.4381, val_recall: 0.4260, val_f1: 0.4182
Epoch [86], train_loss: 1.6112, train_acc: 0.4222, val_loss: 1.6249, val_acc: 0.4160, val_precision: 0.4243, val_recall: 0.4160, val_f1: 0.4110
Epoch [87], train_loss: 1.6224, train_acc: 0.4158, val_loss: 1.6105, val_acc: 0.4241, val_precision: 0.4287, val_recall: 0.4241, val_f1: 0.4145
Epoch [88], train_loss: 1.6226, train_acc: 0.4156, val_loss: 1.6200, val_acc: 0.4191, val_precision: 0.4266, val_recall: 0.4191, val_f1: 0.4083
Epoch [89], train_loss: 1.6183, train_acc: 0.4219, val_loss: 1.5999, val_acc: 0.4215, val_precision: 0.4254, val_recall: 0.4215, val_f1: 0.4155
Epoch [90], train_loss: 1.6087, train_acc: 0.4241, val_loss: 1.6014, val_acc: 0.4222, val_precision: 0.4276, val_recall: 0.4222, val_f1: 0.4158
Epoch [91], train_loss: 1.6098, train_acc: 0.4224, val_loss: 1.6320, val_acc: 0.4012, val_precision: 0.4026, val_recall: 0.4012, val_f1: 0.3868
Epoch [92], train_loss: 1.6091, train_acc: 0.4218, val_loss: 1.6132, val_acc: 0.4113, val_precision: 0.4127, val_recall: 0.4113, val_f1: 0.4009
Epoch [93], train_loss: 1.6039, train_acc: 0.4242, val_loss: 1.6298, val_acc: 0.4109, val_precision: 0.4267, val_recall: 0.4109, val_f1: 0.3973
Epoch [94], train_loss: 1.5937, train_acc: 0.4270, val_loss: 1.6283, val_acc: 0.4077, val_precision: 0.4206, val_recall: 0.4077, val_f1: 0.3953
Epoch [95], train_loss: 1.6141, train_acc: 0.4187, val_loss: 1.6093, val_acc: 0.4163, val_precision: 0.4172, val_recall: 0.4163, val_f1: 0.4038
Epoch [96], train_loss: 1.6146, train_acc: 0.4200, val_loss: 1.7005, val_acc: 0.3763, val_precision: 0.3914, val_recall: 0.3763, val_f1: 0.3582
Epoch [97], train_loss: 1.6267, train_acc: 0.4137, val_loss: 1.6242, val_acc: 0.4124, val_precision: 0.4255, val_recall: 0.4124, val_f1: 0.4098
Epoch [98], train_loss: 1.6109, train_acc: 0.4197, val_loss: 1.5973, val_acc: 0.4272, val_precision: 0.4363, val_recall: 0.4272, val_f1: 0.4212
Epoch [99], train_loss: 1.5979, train_acc: 0.4273, val_loss: 1.5975, val_acc: 0.4319, val_precision: 0.4346, val_recall: 0.4319, val_f1: 0.4238
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 1.5275, val_acc: 0.4584, val_precision: 0.4650, val_recall: 0.4584, val_f1: 0.4541
Summary result of test set => last model => val_loss: 1.5735, val_acc: 0.4371, val_precision: 0.4437, val_recall: 0.4371, val_f1: 0.4284
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.4574
--
confusion matrix
[[516  54  44  32  25  25  15  55 180  54]
 [ 38 574  14  56  11  30  12  29  91 145]
 [ 92  47 274  76 154  82  99 105  44  27]
 [ 37  46  45 349  62 189 120  75  33  44]
 [ 40  21 102  53 452  46 119 112  37  18]
 [ 24  35  70 232  73 340  77  71  53  25]
 [ 11  26  78 104 155  61 499  34  14  18]
 [ 55  28  41  87 119  54  50 485  30  51]
 [ 74  88  15  38  23  26   7  23 633  73]
 [ 59 221  10  53   8  23  25  61  88 452]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.55      0.52      0.53      1000
  automobile       0.50      0.57      0.54      1000
        bird       0.40      0.27      0.32      1000
         cat       0.32      0.35      0.34      1000
        deer       0.42      0.45      0.43      1000
         dog       0.39      0.34      0.36      1000
        frog       0.49      0.50      0.49      1000
       horse       0.46      0.48      0.47      1000
        ship       0.53      0.63      0.57      1000
       truck       0.50      0.45      0.47      1000

    accuracy                           0.46     10000
   macro avg       0.45      0.46      0.45     10000
weighted avg       0.45      0.46      0.45     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.4476
--
confusion matrix
[[251  25  19  20  16  11   9  20  80  37]
 [ 20 314  10  26   8  13   8  16  33  64]
 [ 44  26 151  38  93  32  44  63  19  22]
 [ 20  18  35 150  27 102  53  21  25  20]
 [ 15  15  59  26 197  15  66  53   7  18]
 [ 12  32  39 105  36 176  40  30  24  20]
 [  7  20  42  45  98  44 208  24   9  10]
 [ 26  22  19  38  67  36  21 237  10  24]
 [ 60  42   5  17   9   7   3   4 332  25]
 [ 28 106   4  31  10  12  11  29  48 222]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.52      0.51      0.52       488
  automobile       0.51      0.61      0.55       512
        bird       0.39      0.28      0.33       532
         cat       0.30      0.32      0.31       471
        deer       0.35      0.42      0.38       471
         dog       0.39      0.34      0.37       514
        frog       0.45      0.41      0.43       507
       horse       0.48      0.47      0.48       500
        ship       0.57      0.66      0.61       504
       truck       0.48      0.44      0.46       501

    accuracy                           0.45      5000
   macro avg       0.44      0.45      0.44      5000
weighted avg       0.44      0.45      0.44      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423930: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:25:54 2024
Job was executed on host(s) <hgn43>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:28:27 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:28:27 2024
Terminated at Wed Feb 28 09:38:09 2024
Results reported at Wed Feb 28 09:38:09 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/ViT_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/ViT_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   6559.00 sec.
    Max Memory :                                 2956 MB
    Average Memory :                             2702.47 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               7284.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                31
    Run time :                                   4184 sec.
    Turnaround time :                            4335 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/ViT_err_423930> for stderr output of this job.

