loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.9500, train_acc: 0.2836, val_loss: 1.7065, val_acc: 0.3684, val_precision: 0.3961, val_recall: 0.3684, val_f1: 0.3541
Epoch [1], train_loss: 1.6231, train_acc: 0.4126, val_loss: 1.5159, val_acc: 0.4594, val_precision: 0.4704, val_recall: 0.4594, val_f1: 0.4435
Epoch [2], train_loss: 1.5431, train_acc: 0.4430, val_loss: 1.5035, val_acc: 0.4585, val_precision: 0.4862, val_recall: 0.4585, val_f1: 0.4429
Epoch [3], train_loss: 1.5043, train_acc: 0.4590, val_loss: 1.5067, val_acc: 0.4644, val_precision: 0.4918, val_recall: 0.4644, val_f1: 0.4657
Epoch [4], train_loss: 1.4917, train_acc: 0.4600, val_loss: 1.5631, val_acc: 0.4310, val_precision: 0.4581, val_recall: 0.4310, val_f1: 0.4213
Epoch [5], train_loss: 1.5368, train_acc: 0.4438, val_loss: 1.4817, val_acc: 0.4652, val_precision: 0.4769, val_recall: 0.4652, val_f1: 0.4572
Epoch [6], train_loss: 1.4849, train_acc: 0.4631, val_loss: 1.5343, val_acc: 0.4408, val_precision: 0.4850, val_recall: 0.4408, val_f1: 0.4374
Epoch [7], train_loss: 1.4776, train_acc: 0.4621, val_loss: 1.5212, val_acc: 0.4474, val_precision: 0.4700, val_recall: 0.4474, val_f1: 0.4403
Epoch [8], train_loss: 1.5557, train_acc: 0.4338, val_loss: 1.5195, val_acc: 0.4445, val_precision: 0.4632, val_recall: 0.4445, val_f1: 0.4328
Epoch [9], train_loss: 1.4973, train_acc: 0.4589, val_loss: 1.4853, val_acc: 0.4536, val_precision: 0.4689, val_recall: 0.4536, val_f1: 0.4513
Epoch [10], train_loss: 1.4613, train_acc: 0.4701, val_loss: 1.4516, val_acc: 0.4640, val_precision: 0.4833, val_recall: 0.4640, val_f1: 0.4517
Epoch [11], train_loss: 1.4423, train_acc: 0.4790, val_loss: 1.4197, val_acc: 0.4840, val_precision: 0.4960, val_recall: 0.4840, val_f1: 0.4756
Epoch [12], train_loss: 1.4249, train_acc: 0.4843, val_loss: 1.4815, val_acc: 0.4667, val_precision: 0.4859, val_recall: 0.4667, val_f1: 0.4669
Epoch [13], train_loss: 1.4290, train_acc: 0.4835, val_loss: 1.4213, val_acc: 0.4831, val_precision: 0.4974, val_recall: 0.4831, val_f1: 0.4763
Epoch [14], train_loss: 1.4266, train_acc: 0.4843, val_loss: 1.4685, val_acc: 0.4647, val_precision: 0.4811, val_recall: 0.4647, val_f1: 0.4587
Epoch [15], train_loss: 1.4170, train_acc: 0.4892, val_loss: 1.4131, val_acc: 0.4778, val_precision: 0.4965, val_recall: 0.4778, val_f1: 0.4720
Epoch [16], train_loss: 1.4003, train_acc: 0.4950, val_loss: 1.4347, val_acc: 0.4825, val_precision: 0.4939, val_recall: 0.4825, val_f1: 0.4791
Epoch [17], train_loss: 1.3802, train_acc: 0.5042, val_loss: 1.3917, val_acc: 0.4977, val_precision: 0.5166, val_recall: 0.4977, val_f1: 0.4939
Epoch [18], train_loss: 1.3952, train_acc: 0.4993, val_loss: 1.4220, val_acc: 0.4756, val_precision: 0.4965, val_recall: 0.4756, val_f1: 0.4729
Epoch [19], train_loss: 1.3890, train_acc: 0.5005, val_loss: 1.3927, val_acc: 0.5019, val_precision: 0.5129, val_recall: 0.5019, val_f1: 0.4947
Epoch [20], train_loss: 1.3892, train_acc: 0.4981, val_loss: 1.4095, val_acc: 0.4922, val_precision: 0.5053, val_recall: 0.4922, val_f1: 0.4901
Epoch [21], train_loss: 1.3910, train_acc: 0.4963, val_loss: 1.4083, val_acc: 0.4876, val_precision: 0.5035, val_recall: 0.4876, val_f1: 0.4800
Epoch [22], train_loss: 1.5174, train_acc: 0.4534, val_loss: 1.7077, val_acc: 0.3718, val_precision: 0.3984, val_recall: 0.3718, val_f1: 0.3617
Epoch [23], train_loss: 1.5640, train_acc: 0.4341, val_loss: 1.4806, val_acc: 0.4641, val_precision: 0.5023, val_recall: 0.4641, val_f1: 0.4626
Epoch [24], train_loss: 1.4777, train_acc: 0.4648, val_loss: 1.4584, val_acc: 0.4830, val_precision: 0.4976, val_recall: 0.4830, val_f1: 0.4788
Epoch [25], train_loss: 1.4846, train_acc: 0.4614, val_loss: 1.4672, val_acc: 0.4696, val_precision: 0.4789, val_recall: 0.4696, val_f1: 0.4627
Epoch [26], train_loss: 1.4305, train_acc: 0.4821, val_loss: 1.4121, val_acc: 0.4994, val_precision: 0.5053, val_recall: 0.4994, val_f1: 0.4905
Epoch [27], train_loss: 1.4397, train_acc: 0.4784, val_loss: 1.6142, val_acc: 0.4162, val_precision: 0.4435, val_recall: 0.4162, val_f1: 0.4079
Epoch [28], train_loss: 1.4996, train_acc: 0.4568, val_loss: 1.4712, val_acc: 0.4645, val_precision: 0.4898, val_recall: 0.4645, val_f1: 0.4598
Epoch [29], train_loss: 1.4178, train_acc: 0.4865, val_loss: 1.4825, val_acc: 0.4645, val_precision: 0.4924, val_recall: 0.4645, val_f1: 0.4627
Epoch [30], train_loss: 1.4170, train_acc: 0.4857, val_loss: 1.5125, val_acc: 0.4516, val_precision: 0.4726, val_recall: 0.4516, val_f1: 0.4478
Epoch [31], train_loss: 1.4438, train_acc: 0.4760, val_loss: 1.4806, val_acc: 0.4573, val_precision: 0.4764, val_recall: 0.4573, val_f1: 0.4545
Epoch [32], train_loss: 1.4082, train_acc: 0.4918, val_loss: 1.3956, val_acc: 0.4948, val_precision: 0.5053, val_recall: 0.4948, val_f1: 0.4877
Epoch [33], train_loss: 1.3688, train_acc: 0.5037, val_loss: 1.3963, val_acc: 0.4937, val_precision: 0.5052, val_recall: 0.4937, val_f1: 0.4920
Epoch [34], train_loss: 1.3590, train_acc: 0.5092, val_loss: 1.4056, val_acc: 0.5010, val_precision: 0.5111, val_recall: 0.5010, val_f1: 0.4921
Epoch [35], train_loss: 1.3596, train_acc: 0.5083, val_loss: 1.4031, val_acc: 0.4871, val_precision: 0.5089, val_recall: 0.4871, val_f1: 0.4884
Epoch [36], train_loss: 1.3509, train_acc: 0.5102, val_loss: 1.3987, val_acc: 0.4899, val_precision: 0.5117, val_recall: 0.4899, val_f1: 0.4900
Epoch [37], train_loss: 1.3501, train_acc: 0.5141, val_loss: 1.3888, val_acc: 0.5049, val_precision: 0.5105, val_recall: 0.5049, val_f1: 0.5025
Epoch [38], train_loss: 1.3217, train_acc: 0.5233, val_loss: 1.3748, val_acc: 0.5013, val_precision: 0.5107, val_recall: 0.5013, val_f1: 0.4967
Epoch [39], train_loss: 1.3514, train_acc: 0.5129, val_loss: 1.4600, val_acc: 0.4720, val_precision: 0.5065, val_recall: 0.4720, val_f1: 0.4688
Epoch [40], train_loss: 1.3245, train_acc: 0.5198, val_loss: 1.3584, val_acc: 0.5156, val_precision: 0.5287, val_recall: 0.5156, val_f1: 0.5150
Epoch [41], train_loss: 1.2996, train_acc: 0.5323, val_loss: 1.4078, val_acc: 0.4952, val_precision: 0.5097, val_recall: 0.4952, val_f1: 0.4854
Epoch [42], train_loss: 1.3650, train_acc: 0.5089, val_loss: 1.5089, val_acc: 0.4520, val_precision: 0.4848, val_recall: 0.4520, val_f1: 0.4477
Epoch [43], train_loss: 1.3714, train_acc: 0.5053, val_loss: 1.3948, val_acc: 0.4974, val_precision: 0.5069, val_recall: 0.4974, val_f1: 0.4941
Epoch [44], train_loss: 1.4194, train_acc: 0.4853, val_loss: 1.5761, val_acc: 0.4243, val_precision: 0.4507, val_recall: 0.4243, val_f1: 0.4121
Epoch [45], train_loss: 1.4089, train_acc: 0.4887, val_loss: 1.3948, val_acc: 0.4921, val_precision: 0.5020, val_recall: 0.4921, val_f1: 0.4874
Epoch [46], train_loss: 1.3930, train_acc: 0.4951, val_loss: 1.4138, val_acc: 0.4885, val_precision: 0.5025, val_recall: 0.4885, val_f1: 0.4846
Epoch [47], train_loss: 1.4285, train_acc: 0.4826, val_loss: 1.5268, val_acc: 0.4461, val_precision: 0.4617, val_recall: 0.4461, val_f1: 0.4416
Epoch [48], train_loss: 1.4168, train_acc: 0.4909, val_loss: 1.4949, val_acc: 0.4539, val_precision: 0.4732, val_recall: 0.4539, val_f1: 0.4552
Epoch [49], train_loss: 1.4388, train_acc: 0.4814, val_loss: 1.4335, val_acc: 0.4783, val_precision: 0.4880, val_recall: 0.4783, val_f1: 0.4755
Epoch [50], train_loss: 1.3681, train_acc: 0.5048, val_loss: 1.3630, val_acc: 0.5032, val_precision: 0.5188, val_recall: 0.5032, val_f1: 0.5009
Epoch [51], train_loss: 1.3093, train_acc: 0.5258, val_loss: 1.3694, val_acc: 0.5010, val_precision: 0.5193, val_recall: 0.5010, val_f1: 0.4999
Epoch [52], train_loss: 1.4057, train_acc: 0.4919, val_loss: 1.4393, val_acc: 0.4759, val_precision: 0.4887, val_recall: 0.4759, val_f1: 0.4670
Epoch [53], train_loss: 1.4013, train_acc: 0.4934, val_loss: 1.4348, val_acc: 0.4798, val_precision: 0.5014, val_recall: 0.4798, val_f1: 0.4663
Epoch [54], train_loss: 1.3566, train_acc: 0.5074, val_loss: 1.4294, val_acc: 0.4803, val_precision: 0.4976, val_recall: 0.4803, val_f1: 0.4770
Epoch [55], train_loss: 1.3431, train_acc: 0.5153, val_loss: 1.3882, val_acc: 0.4974, val_precision: 0.5039, val_recall: 0.4974, val_f1: 0.4899
Epoch [56], train_loss: 1.3350, train_acc: 0.5164, val_loss: 1.4464, val_acc: 0.4797, val_precision: 0.4894, val_recall: 0.4797, val_f1: 0.4754
Epoch [57], train_loss: 1.3696, train_acc: 0.5052, val_loss: 1.4035, val_acc: 0.4924, val_precision: 0.5086, val_recall: 0.4924, val_f1: 0.4908
Epoch [58], train_loss: 1.3438, train_acc: 0.5152, val_loss: 1.3956, val_acc: 0.4947, val_precision: 0.5095, val_recall: 0.4947, val_f1: 0.4894
Epoch [59], train_loss: 1.3404, train_acc: 0.5150, val_loss: 1.3649, val_acc: 0.5025, val_precision: 0.5164, val_recall: 0.5025, val_f1: 0.5023
Epoch [60], train_loss: 1.3266, train_acc: 0.5191, val_loss: 1.4397, val_acc: 0.4760, val_precision: 0.4990, val_recall: 0.4760, val_f1: 0.4713
Epoch [61], train_loss: 1.3767, train_acc: 0.5000, val_loss: 1.4542, val_acc: 0.4775, val_precision: 0.4887, val_recall: 0.4775, val_f1: 0.4694
Epoch [62], train_loss: 1.5183, train_acc: 0.4476, val_loss: 1.5728, val_acc: 0.4210, val_precision: 0.4292, val_recall: 0.4210, val_f1: 0.4118
Epoch [63], train_loss: 1.5476, train_acc: 0.4394, val_loss: 1.6110, val_acc: 0.4267, val_precision: 0.4438, val_recall: 0.4267, val_f1: 0.4195
Epoch [64], train_loss: 1.5702, train_acc: 0.4326, val_loss: 1.5377, val_acc: 0.4354, val_precision: 0.4510, val_recall: 0.4354, val_f1: 0.4289
Epoch [65], train_loss: 1.4832, train_acc: 0.4632, val_loss: 1.4793, val_acc: 0.4588, val_precision: 0.4731, val_recall: 0.4588, val_f1: 0.4567
Epoch [66], train_loss: 1.4065, train_acc: 0.4906, val_loss: 1.4211, val_acc: 0.4814, val_precision: 0.4898, val_recall: 0.4814, val_f1: 0.4732
Epoch [67], train_loss: 1.4317, train_acc: 0.4836, val_loss: 1.4750, val_acc: 0.4628, val_precision: 0.4765, val_recall: 0.4628, val_f1: 0.4580
Epoch [68], train_loss: 1.4192, train_acc: 0.4883, val_loss: 1.4489, val_acc: 0.4685, val_precision: 0.4795, val_recall: 0.4685, val_f1: 0.4581
Epoch [69], train_loss: 1.3600, train_acc: 0.5093, val_loss: 1.4415, val_acc: 0.4772, val_precision: 0.4991, val_recall: 0.4772, val_f1: 0.4740
Epoch [70], train_loss: 1.3669, train_acc: 0.5074, val_loss: 1.4168, val_acc: 0.4893, val_precision: 0.5106, val_recall: 0.4893, val_f1: 0.4850
Epoch [71], train_loss: 1.3678, train_acc: 0.5056, val_loss: 1.4373, val_acc: 0.4785, val_precision: 0.4870, val_recall: 0.4785, val_f1: 0.4750
Epoch [72], train_loss: 1.3641, train_acc: 0.5055, val_loss: 1.4621, val_acc: 0.4681, val_precision: 0.4730, val_recall: 0.4681, val_f1: 0.4584
Epoch [73], train_loss: 1.3775, train_acc: 0.5045, val_loss: 1.4199, val_acc: 0.4872, val_precision: 0.4925, val_recall: 0.4872, val_f1: 0.4834
Epoch [74], train_loss: 1.3488, train_acc: 0.5125, val_loss: 1.3948, val_acc: 0.4949, val_precision: 0.5248, val_recall: 0.4949, val_f1: 0.4938
Epoch [75], train_loss: 1.3193, train_acc: 0.5252, val_loss: 1.4034, val_acc: 0.4905, val_precision: 0.5030, val_recall: 0.4905, val_f1: 0.4833
Epoch [76], train_loss: 1.3174, train_acc: 0.5250, val_loss: 1.3751, val_acc: 0.5056, val_precision: 0.5178, val_recall: 0.5056, val_f1: 0.5010
Epoch [77], train_loss: 1.4558, train_acc: 0.4766, val_loss: 1.4788, val_acc: 0.4662, val_precision: 0.4775, val_recall: 0.4662, val_f1: 0.4621
Epoch [78], train_loss: 1.4626, train_acc: 0.4720, val_loss: 1.5179, val_acc: 0.4473, val_precision: 0.4659, val_recall: 0.4473, val_f1: 0.4444
Epoch [79], train_loss: 1.4547, train_acc: 0.4749, val_loss: 1.5012, val_acc: 0.4564, val_precision: 0.4628, val_recall: 0.4564, val_f1: 0.4509
Epoch [80], train_loss: 1.4209, train_acc: 0.4886, val_loss: 1.5003, val_acc: 0.4598, val_precision: 0.4880, val_recall: 0.4598, val_f1: 0.4558
Epoch [81], train_loss: 1.4403, train_acc: 0.4802, val_loss: 1.5223, val_acc: 0.4506, val_precision: 0.4701, val_recall: 0.4506, val_f1: 0.4414
Epoch [82], train_loss: 1.4462, train_acc: 0.4803, val_loss: 1.4346, val_acc: 0.4784, val_precision: 0.4860, val_recall: 0.4784, val_f1: 0.4693
Epoch [83], train_loss: 1.4234, train_acc: 0.4883, val_loss: 1.4761, val_acc: 0.4708, val_precision: 0.4804, val_recall: 0.4708, val_f1: 0.4654
Epoch [84], train_loss: 1.4497, train_acc: 0.4756, val_loss: 1.4665, val_acc: 0.4700, val_precision: 0.4826, val_recall: 0.4700, val_f1: 0.4689
Epoch [85], train_loss: 1.4805, train_acc: 0.4636, val_loss: 1.5156, val_acc: 0.4444, val_precision: 0.4553, val_recall: 0.4444, val_f1: 0.4380
Epoch [86], train_loss: 1.4749, train_acc: 0.4658, val_loss: 1.5362, val_acc: 0.4388, val_precision: 0.4575, val_recall: 0.4388, val_f1: 0.4330
Epoch [87], train_loss: 1.4871, train_acc: 0.4582, val_loss: 1.4866, val_acc: 0.4586, val_precision: 0.4705, val_recall: 0.4586, val_f1: 0.4562
Epoch [88], train_loss: 1.4546, train_acc: 0.4708, val_loss: 1.4879, val_acc: 0.4624, val_precision: 0.4786, val_recall: 0.4624, val_f1: 0.4565
Epoch [89], train_loss: 1.4539, train_acc: 0.4730, val_loss: 1.5047, val_acc: 0.4573, val_precision: 0.4677, val_recall: 0.4573, val_f1: 0.4472
Epoch [90], train_loss: 1.4683, train_acc: 0.4697, val_loss: 1.4879, val_acc: 0.4683, val_precision: 0.4880, val_recall: 0.4683, val_f1: 0.4619
Epoch [91], train_loss: 1.4256, train_acc: 0.4848, val_loss: 1.4587, val_acc: 0.4670, val_precision: 0.4820, val_recall: 0.4670, val_f1: 0.4614
Epoch [92], train_loss: 1.4828, train_acc: 0.4655, val_loss: 1.5147, val_acc: 0.4460, val_precision: 0.4645, val_recall: 0.4460, val_f1: 0.4411
Epoch [93], train_loss: 1.4268, train_acc: 0.4847, val_loss: 1.4930, val_acc: 0.4587, val_precision: 0.4718, val_recall: 0.4587, val_f1: 0.4491
Epoch [94], train_loss: 1.4006, train_acc: 0.4923, val_loss: 1.4633, val_acc: 0.4623, val_precision: 0.4765, val_recall: 0.4623, val_f1: 0.4558
Epoch [95], train_loss: 1.3954, train_acc: 0.4934, val_loss: 1.4817, val_acc: 0.4596, val_precision: 0.4689, val_recall: 0.4596, val_f1: 0.4519
Epoch [96], train_loss: 1.4071, train_acc: 0.4903, val_loss: 1.4737, val_acc: 0.4743, val_precision: 0.4802, val_recall: 0.4743, val_f1: 0.4669
Epoch [97], train_loss: 1.3945, train_acc: 0.4959, val_loss: 1.4709, val_acc: 0.4710, val_precision: 0.4951, val_recall: 0.4710, val_f1: 0.4736
Epoch [98], train_loss: 1.3961, train_acc: 0.4943, val_loss: 1.4554, val_acc: 0.4742, val_precision: 0.4869, val_recall: 0.4742, val_f1: 0.4737
Epoch [99], train_loss: 1.3953, train_acc: 0.4934, val_loss: 1.5103, val_acc: 0.4641, val_precision: 0.4789, val_recall: 0.4641, val_f1: 0.4617
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 1.3574, val_acc: 0.5044, val_precision: 0.5205, val_recall: 0.5044, val_f1: 0.5042
Summary result of test set => last model => val_loss: 1.5061, val_acc: 0.4573, val_precision: 0.4767, val_recall: 0.4573, val_f1: 0.4565
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.5060
--
confusion matrix
[[518  49  66  27  27  27  20  26 119 121]
 [ 33 592  36  19   6  17  16  11  43 227]
 [ 71  39 371  80 168  77 105  42  14  33]
 [ 20  31  67 363  79 279  75  34  16  36]
 [ 35  15  88  74 527  52  91  78  16  24]
 [ 21  34  61 184  94 454  43  66  15  28]
 [ 13  20  63 116 148  61 520  22  17  20]
 [ 37  33  65  65  95 105  22 492  12  74]
 [ 93  82  18  25  21   9  16   6 623 107]
 [ 22 200  32  24   9  17  15  20  61 600]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.60      0.52      0.56      1000
  automobile       0.54      0.59      0.57      1000
        bird       0.43      0.37      0.40      1000
         cat       0.37      0.36      0.37      1000
        deer       0.45      0.53      0.48      1000
         dog       0.41      0.45      0.43      1000
        frog       0.56      0.52      0.54      1000
       horse       0.62      0.49      0.55      1000
        ship       0.67      0.62      0.64      1000
       truck       0.47      0.60      0.53      1000

    accuracy                           0.51     10000
   macro avg       0.51      0.51      0.51     10000
weighted avg       0.51      0.51      0.51     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.5160
--
confusion matrix
[[240  34  31  23  12  10  12   8  61  57]
 [ 19 308  17  16   3   5  11   1  31 101]
 [ 32  19 212  49  77  29  53  33  12  16]
 [ 14  11  39 168  24 134  34  16  10  21]
 [ 10   5  48  28 247  23  46  43   8  13]
 [  7  19  37  75  37 262  26  30   8  13]
 [  4  11  41  63  75  25 251  15   7  15]
 [ 12  22  39  30  49  43  12 249   6  38]
 [ 48  34   8   7  10  14   5   1 332  45]
 [ 15  86  11  16   7  17   9  12  17 311]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.60      0.49      0.54       488
  automobile       0.56      0.60      0.58       512
        bird       0.44      0.40      0.42       532
         cat       0.35      0.36      0.36       471
        deer       0.46      0.52      0.49       471
         dog       0.47      0.51      0.49       514
        frog       0.55      0.50      0.52       507
       horse       0.61      0.50      0.55       500
        ship       0.67      0.66      0.67       504
       truck       0.49      0.62      0.55       501

    accuracy                           0.52      5000
   macro avg       0.52      0.52      0.52      5000
weighted avg       0.52      0.52      0.52      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 429077: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 09:37:16 2024
Job was executed on host(s) <hgn50>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 09:37:47 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 09:37:47 2024
Terminated at Wed Feb 28 10:16:32 2024
Results reported at Wed Feb 28 10:16:32 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/ViT_simple_correct_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/ViT_simple_correct_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   3667.00 sec.
    Max Memory :                                 2787 MB
    Average Memory :                             2663.36 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               7453.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                127
    Run time :                                   2325 sec.
    Turnaround time :                            2356 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/ViT_simple_correct_err_429077> for stderr output of this job.

