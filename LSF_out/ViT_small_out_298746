loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.7502, train_acc: 0.3581, val_loss: 1.6772, val_acc: 0.3800, val_precision: 0.3800, val_recall: 0.3800, val_f1: 0.3800
Epoch [1], train_loss: 1.7129, train_acc: 0.3641, val_loss: 1.6688, val_acc: 0.3857, val_precision: 0.3857, val_recall: 0.3857, val_f1: 0.3857
Epoch [2], train_loss: 1.6537, train_acc: 0.3859, val_loss: 1.6463, val_acc: 0.3880, val_precision: 0.3880, val_recall: 0.3880, val_f1: 0.3880
Epoch [3], train_loss: 1.6651, train_acc: 0.3855, val_loss: 1.6926, val_acc: 0.3700, val_precision: 0.3700, val_recall: 0.3700, val_f1: 0.3700
Epoch [4], train_loss: 1.7005, train_acc: 0.3695, val_loss: 1.6713, val_acc: 0.3865, val_precision: 0.3865, val_recall: 0.3865, val_f1: 0.3865
Epoch [5], train_loss: 1.6821, train_acc: 0.3772, val_loss: 1.6672, val_acc: 0.3903, val_precision: 0.3903, val_recall: 0.3903, val_f1: 0.3903
Epoch [6], train_loss: 1.7901, train_acc: 0.3317, val_loss: 1.9148, val_acc: 0.2649, val_precision: 0.2649, val_recall: 0.2649, val_f1: 0.2649
Epoch [7], train_loss: 1.8487, train_acc: 0.3037, val_loss: 1.8093, val_acc: 0.3223, val_precision: 0.3223, val_recall: 0.3223, val_f1: 0.3223
Epoch [8], train_loss: 1.7825, train_acc: 0.3316, val_loss: 1.7511, val_acc: 0.3550, val_precision: 0.3550, val_recall: 0.3550, val_f1: 0.3550
Epoch [9], train_loss: 1.7653, train_acc: 0.3435, val_loss: 1.7110, val_acc: 0.3651, val_precision: 0.3651, val_recall: 0.3651, val_f1: 0.3651
Epoch [10], train_loss: 1.7825, train_acc: 0.3354, val_loss: 1.8075, val_acc: 0.3315, val_precision: 0.3315, val_recall: 0.3315, val_f1: 0.3315
Epoch [11], train_loss: 1.8022, train_acc: 0.3230, val_loss: 1.7273, val_acc: 0.3664, val_precision: 0.3664, val_recall: 0.3664, val_f1: 0.3664
Epoch [12], train_loss: 1.7621, train_acc: 0.3417, val_loss: 1.7283, val_acc: 0.3651, val_precision: 0.3651, val_recall: 0.3651, val_f1: 0.3651
Epoch [13], train_loss: 1.7529, train_acc: 0.3509, val_loss: 1.7496, val_acc: 0.3613, val_precision: 0.3613, val_recall: 0.3613, val_f1: 0.3613
Epoch [14], train_loss: 1.7401, train_acc: 0.3525, val_loss: 1.7463, val_acc: 0.3584, val_precision: 0.3584, val_recall: 0.3584, val_f1: 0.3584
Epoch [15], train_loss: 1.7624, train_acc: 0.3420, val_loss: 1.7973, val_acc: 0.3359, val_precision: 0.3359, val_recall: 0.3359, val_f1: 0.3359
Epoch [16], train_loss: 1.7541, train_acc: 0.3496, val_loss: 1.7167, val_acc: 0.3708, val_precision: 0.3708, val_recall: 0.3708, val_f1: 0.3708
Epoch [17], train_loss: 1.7228, train_acc: 0.3608, val_loss: 1.6974, val_acc: 0.3714, val_precision: 0.3714, val_recall: 0.3714, val_f1: 0.3714
Epoch [18], train_loss: 1.7194, train_acc: 0.3612, val_loss: 1.7096, val_acc: 0.3692, val_precision: 0.3692, val_recall: 0.3692, val_f1: 0.3692
Epoch [19], train_loss: 1.7661, train_acc: 0.3429, val_loss: 1.7593, val_acc: 0.3366, val_precision: 0.3366, val_recall: 0.3366, val_f1: 0.3366
Epoch [20], train_loss: 1.7640, train_acc: 0.3402, val_loss: 1.7354, val_acc: 0.3542, val_precision: 0.3542, val_recall: 0.3542, val_f1: 0.3542
Epoch [21], train_loss: 1.7294, train_acc: 0.3542, val_loss: 1.6896, val_acc: 0.3637, val_precision: 0.3637, val_recall: 0.3637, val_f1: 0.3637
Epoch [22], train_loss: 1.7008, train_acc: 0.3675, val_loss: 1.6954, val_acc: 0.3683, val_precision: 0.3683, val_recall: 0.3683, val_f1: 0.3683
Epoch [23], train_loss: 1.6800, train_acc: 0.3776, val_loss: 1.6968, val_acc: 0.3725, val_precision: 0.3725, val_recall: 0.3725, val_f1: 0.3725
Epoch [24], train_loss: 1.6768, train_acc: 0.3799, val_loss: 1.6821, val_acc: 0.3857, val_precision: 0.3857, val_recall: 0.3857, val_f1: 0.3857
Epoch [25], train_loss: 1.6828, train_acc: 0.3786, val_loss: 1.6648, val_acc: 0.3959, val_precision: 0.3959, val_recall: 0.3959, val_f1: 0.3959
Epoch [26], train_loss: 1.6646, train_acc: 0.3863, val_loss: 1.6632, val_acc: 0.3866, val_precision: 0.3866, val_recall: 0.3866, val_f1: 0.3866
Epoch [27], train_loss: 1.6457, train_acc: 0.3933, val_loss: 1.6502, val_acc: 0.3992, val_precision: 0.3992, val_recall: 0.3992, val_f1: 0.3992
Epoch [28], train_loss: 1.6398, train_acc: 0.3996, val_loss: 1.6471, val_acc: 0.3920, val_precision: 0.3920, val_recall: 0.3920, val_f1: 0.3920
Epoch [29], train_loss: 1.6097, train_acc: 0.4104, val_loss: 1.5907, val_acc: 0.4213, val_precision: 0.4213, val_recall: 0.4213, val_f1: 0.4213
Epoch [30], train_loss: 1.5924, train_acc: 0.4159, val_loss: 1.5675, val_acc: 0.4289, val_precision: 0.4289, val_recall: 0.4289, val_f1: 0.4289
Epoch [31], train_loss: 1.5744, train_acc: 0.4235, val_loss: 1.5474, val_acc: 0.4371, val_precision: 0.4371, val_recall: 0.4371, val_f1: 0.4371
Epoch [32], train_loss: 1.5511, train_acc: 0.4317, val_loss: 1.5644, val_acc: 0.4234, val_precision: 0.4234, val_recall: 0.4234, val_f1: 0.4234
Epoch [33], train_loss: 1.5406, train_acc: 0.4346, val_loss: 1.5702, val_acc: 0.4223, val_precision: 0.4223, val_recall: 0.4223, val_f1: 0.4223
Epoch [34], train_loss: 1.5238, train_acc: 0.4383, val_loss: 1.5147, val_acc: 0.4570, val_precision: 0.4570, val_recall: 0.4570, val_f1: 0.4570
Epoch [35], train_loss: 1.5022, train_acc: 0.4499, val_loss: 1.5219, val_acc: 0.4503, val_precision: 0.4503, val_recall: 0.4503, val_f1: 0.4503
Epoch [36], train_loss: 1.4954, train_acc: 0.4522, val_loss: 1.5152, val_acc: 0.4521, val_precision: 0.4521, val_recall: 0.4521, val_f1: 0.4521
Epoch [37], train_loss: 1.4936, train_acc: 0.4505, val_loss: 1.4948, val_acc: 0.4646, val_precision: 0.4646, val_recall: 0.4646, val_f1: 0.4646
Epoch [38], train_loss: 1.4761, train_acc: 0.4623, val_loss: 1.4783, val_acc: 0.4775, val_precision: 0.4775, val_recall: 0.4775, val_f1: 0.4775
Epoch [39], train_loss: 1.4664, train_acc: 0.4633, val_loss: 1.5055, val_acc: 0.4635, val_precision: 0.4635, val_recall: 0.4635, val_f1: 0.4635
Epoch [40], train_loss: 1.4502, train_acc: 0.4724, val_loss: 1.5010, val_acc: 0.4531, val_precision: 0.4531, val_recall: 0.4531, val_f1: 0.4531
Epoch [41], train_loss: 1.4674, train_acc: 0.4577, val_loss: 1.4940, val_acc: 0.4625, val_precision: 0.4625, val_recall: 0.4625, val_f1: 0.4625
Epoch [42], train_loss: 1.4776, train_acc: 0.4608, val_loss: 1.4842, val_acc: 0.4624, val_precision: 0.4624, val_recall: 0.4624, val_f1: 0.4624
Epoch [43], train_loss: 1.4614, train_acc: 0.4650, val_loss: 1.4539, val_acc: 0.4724, val_precision: 0.4724, val_recall: 0.4724, val_f1: 0.4724
Epoch [44], train_loss: 1.4320, train_acc: 0.4774, val_loss: 1.4637, val_acc: 0.4737, val_precision: 0.4737, val_recall: 0.4737, val_f1: 0.4737
Epoch [45], train_loss: 1.4334, train_acc: 0.4743, val_loss: 1.4489, val_acc: 0.4799, val_precision: 0.4799, val_recall: 0.4799, val_f1: 0.4799
Epoch [46], train_loss: 1.4140, train_acc: 0.4836, val_loss: 1.4263, val_acc: 0.4824, val_precision: 0.4824, val_recall: 0.4824, val_f1: 0.4824
Epoch [47], train_loss: 1.3916, train_acc: 0.4915, val_loss: 1.4030, val_acc: 0.4913, val_precision: 0.4913, val_recall: 0.4913, val_f1: 0.4913
Epoch [48], train_loss: 1.3806, train_acc: 0.4969, val_loss: 1.4208, val_acc: 0.4901, val_precision: 0.4901, val_recall: 0.4901, val_f1: 0.4901
Epoch [49], train_loss: 1.3822, train_acc: 0.4985, val_loss: 1.4090, val_acc: 0.4941, val_precision: 0.4941, val_recall: 0.4941, val_f1: 0.4941
Epoch [50], train_loss: 1.3706, train_acc: 0.5008, val_loss: 1.4016, val_acc: 0.4833, val_precision: 0.4833, val_recall: 0.4833, val_f1: 0.4833
Epoch [51], train_loss: 1.3621, train_acc: 0.5057, val_loss: 1.4124, val_acc: 0.4971, val_precision: 0.4971, val_recall: 0.4971, val_f1: 0.4971
Epoch [52], train_loss: 1.3520, train_acc: 0.5087, val_loss: 1.3668, val_acc: 0.5098, val_precision: 0.5098, val_recall: 0.5098, val_f1: 0.5098
Epoch [53], train_loss: 1.3389, train_acc: 0.5099, val_loss: 1.3549, val_acc: 0.5057, val_precision: 0.5057, val_recall: 0.5057, val_f1: 0.5057
Epoch [54], train_loss: 1.3311, train_acc: 0.5161, val_loss: 1.3719, val_acc: 0.5012, val_precision: 0.5012, val_recall: 0.5012, val_f1: 0.5012
Epoch [55], train_loss: 1.3197, train_acc: 0.5217, val_loss: 1.3411, val_acc: 0.5127, val_precision: 0.5127, val_recall: 0.5127, val_f1: 0.5127
Epoch [56], train_loss: 1.2937, train_acc: 0.5316, val_loss: 1.3543, val_acc: 0.5111, val_precision: 0.5111, val_recall: 0.5111, val_f1: 0.5111
Epoch [57], train_loss: 1.2896, train_acc: 0.5328, val_loss: 1.3516, val_acc: 0.5216, val_precision: 0.5216, val_recall: 0.5216, val_f1: 0.5216
Epoch [58], train_loss: 1.2736, train_acc: 0.5357, val_loss: 1.3302, val_acc: 0.5185, val_precision: 0.5185, val_recall: 0.5185, val_f1: 0.5185
Epoch [59], train_loss: 1.2683, train_acc: 0.5384, val_loss: 1.3342, val_acc: 0.5181, val_precision: 0.5181, val_recall: 0.5181, val_f1: 0.5181
Epoch [60], train_loss: 1.2511, train_acc: 0.5417, val_loss: 1.3080, val_acc: 0.5327, val_precision: 0.5327, val_recall: 0.5327, val_f1: 0.5327
Epoch [61], train_loss: 1.2350, train_acc: 0.5482, val_loss: 1.3132, val_acc: 0.5304, val_precision: 0.5304, val_recall: 0.5304, val_f1: 0.5304
Epoch [62], train_loss: 1.2237, train_acc: 0.5546, val_loss: 1.2976, val_acc: 0.5270, val_precision: 0.5270, val_recall: 0.5270, val_f1: 0.5270
Epoch [63], train_loss: 1.2127, train_acc: 0.5602, val_loss: 1.3012, val_acc: 0.5302, val_precision: 0.5302, val_recall: 0.5302, val_f1: 0.5302
Epoch [64], train_loss: 1.1878, train_acc: 0.5681, val_loss: 1.3287, val_acc: 0.5256, val_precision: 0.5256, val_recall: 0.5256, val_f1: 0.5256
Epoch [65], train_loss: 1.1779, train_acc: 0.5735, val_loss: 1.2940, val_acc: 0.5395, val_precision: 0.5395, val_recall: 0.5395, val_f1: 0.5395
Epoch [66], train_loss: 1.1628, train_acc: 0.5781, val_loss: 1.2672, val_acc: 0.5474, val_precision: 0.5474, val_recall: 0.5474, val_f1: 0.5474
Epoch [67], train_loss: 1.1490, train_acc: 0.5827, val_loss: 1.2635, val_acc: 0.5413, val_precision: 0.5413, val_recall: 0.5413, val_f1: 0.5413
Epoch [68], train_loss: 1.1201, train_acc: 0.5957, val_loss: 1.2674, val_acc: 0.5499, val_precision: 0.5499, val_recall: 0.5499, val_f1: 0.5499
Epoch [69], train_loss: 1.0973, train_acc: 0.6042, val_loss: 1.2648, val_acc: 0.5570, val_precision: 0.5570, val_recall: 0.5570, val_f1: 0.5570
Epoch [70], train_loss: 1.0817, train_acc: 0.6087, val_loss: 1.2722, val_acc: 0.5578, val_precision: 0.5578, val_recall: 0.5578, val_f1: 0.5578
Epoch [71], train_loss: 1.0575, train_acc: 0.6139, val_loss: 1.2581, val_acc: 0.5489, val_precision: 0.5489, val_recall: 0.5489, val_f1: 0.5489
Epoch [72], train_loss: 1.0314, train_acc: 0.6244, val_loss: 1.2476, val_acc: 0.5606, val_precision: 0.5606, val_recall: 0.5606, val_f1: 0.5606
Epoch [73], train_loss: 0.9942, train_acc: 0.6371, val_loss: 1.2452, val_acc: 0.5619, val_precision: 0.5619, val_recall: 0.5619, val_f1: 0.5619
Epoch [74], train_loss: 0.9632, train_acc: 0.6526, val_loss: 1.2823, val_acc: 0.5509, val_precision: 0.5509, val_recall: 0.5509, val_f1: 0.5509
Epoch [75], train_loss: 0.9323, train_acc: 0.6621, val_loss: 1.2927, val_acc: 0.5565, val_precision: 0.5565, val_recall: 0.5565, val_f1: 0.5565
Epoch [76], train_loss: 0.8904, train_acc: 0.6776, val_loss: 1.2482, val_acc: 0.5771, val_precision: 0.5771, val_recall: 0.5771, val_f1: 0.5771
Epoch [77], train_loss: 0.8290, train_acc: 0.7002, val_loss: 1.3351, val_acc: 0.5658, val_precision: 0.5658, val_recall: 0.5658, val_f1: 0.5658
Epoch [78], train_loss: 0.7742, train_acc: 0.7168, val_loss: 1.3623, val_acc: 0.5630, val_precision: 0.5630, val_recall: 0.5630, val_f1: 0.5630
Epoch [79], train_loss: 0.7194, train_acc: 0.7381, val_loss: 1.3930, val_acc: 0.5673, val_precision: 0.5673, val_recall: 0.5673, val_f1: 0.5673
Epoch [80], train_loss: 0.6539, train_acc: 0.7622, val_loss: 1.4662, val_acc: 0.5636, val_precision: 0.5636, val_recall: 0.5636, val_f1: 0.5636
Epoch [81], train_loss: 0.6044, train_acc: 0.7772, val_loss: 1.4773, val_acc: 0.5552, val_precision: 0.5552, val_recall: 0.5552, val_f1: 0.5552
Epoch [82], train_loss: 0.5486, train_acc: 0.7989, val_loss: 1.5906, val_acc: 0.5600, val_precision: 0.5600, val_recall: 0.5600, val_f1: 0.5600
Epoch [83], train_loss: 0.5054, train_acc: 0.8172, val_loss: 1.7085, val_acc: 0.5579, val_precision: 0.5579, val_recall: 0.5579, val_f1: 0.5579
Epoch [84], train_loss: 0.4641, train_acc: 0.8299, val_loss: 1.6726, val_acc: 0.5538, val_precision: 0.5538, val_recall: 0.5538, val_f1: 0.5538
Epoch [85], train_loss: 0.4151, train_acc: 0.8491, val_loss: 1.7743, val_acc: 0.5615, val_precision: 0.5615, val_recall: 0.5615, val_f1: 0.5615
Epoch [86], train_loss: 0.3782, train_acc: 0.8613, val_loss: 1.8647, val_acc: 0.5532, val_precision: 0.5532, val_recall: 0.5532, val_f1: 0.5532
Epoch [87], train_loss: 0.3462, train_acc: 0.8742, val_loss: 1.9200, val_acc: 0.5531, val_precision: 0.5531, val_recall: 0.5531, val_f1: 0.5531
Epoch [88], train_loss: 0.3067, train_acc: 0.8878, val_loss: 2.0473, val_acc: 0.5574, val_precision: 0.5574, val_recall: 0.5574, val_f1: 0.5574
Epoch [89], train_loss: 0.2851, train_acc: 0.8968, val_loss: 2.1135, val_acc: 0.5607, val_precision: 0.5607, val_recall: 0.5607, val_f1: 0.5607
Epoch [90], train_loss: 0.2641, train_acc: 0.9061, val_loss: 2.1463, val_acc: 0.5567, val_precision: 0.5567, val_recall: 0.5567, val_f1: 0.5567
Epoch [91], train_loss: 0.2392, train_acc: 0.9125, val_loss: 2.2269, val_acc: 0.5563, val_precision: 0.5563, val_recall: 0.5563, val_f1: 0.5563
Epoch [92], train_loss: 0.2153, train_acc: 0.9247, val_loss: 2.3205, val_acc: 0.5620, val_precision: 0.5620, val_recall: 0.5620, val_f1: 0.5620
Epoch [93], train_loss: 0.2038, train_acc: 0.9256, val_loss: 2.4101, val_acc: 0.5544, val_precision: 0.5544, val_recall: 0.5544, val_f1: 0.5544
Epoch [94], train_loss: 0.1965, train_acc: 0.9267, val_loss: 2.3854, val_acc: 0.5530, val_precision: 0.5530, val_recall: 0.5530, val_f1: 0.5530
Epoch [95], train_loss: 0.1816, train_acc: 0.9342, val_loss: 2.4453, val_acc: 0.5602, val_precision: 0.5602, val_recall: 0.5602, val_f1: 0.5602
Epoch [96], train_loss: 0.1684, train_acc: 0.9380, val_loss: 2.5349, val_acc: 0.5509, val_precision: 0.5509, val_recall: 0.5509, val_f1: 0.5509
Epoch [97], train_loss: 0.1573, train_acc: 0.9439, val_loss: 2.5018, val_acc: 0.5491, val_precision: 0.5491, val_recall: 0.5491, val_f1: 0.5491
Epoch [98], train_loss: 0.1573, train_acc: 0.9442, val_loss: 2.5512, val_acc: 0.5457, val_precision: 0.5457, val_recall: 0.5457, val_f1: 0.5457
Epoch [99], train_loss: 0.1446, train_acc: 0.9490, val_loss: 2.5831, val_acc: 0.5555, val_precision: 0.5555, val_recall: 0.5555, val_f1: 0.5555
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 1.2484, val_acc: 0.5665, val_precision: 0.5665, val_recall: 0.5665, val_f1: 0.5665
Summary result of test set => last model => val_loss: 2.6310, val_acc: 0.5449, val_precision: 0.5449, val_recall: 0.5449, val_f1: 0.5449
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.5666
--
confusion matrix
[[640  52  46  24  33  13  18  11 115  48]
 [ 30 706  10  16  11   9  18   4  38 158]
 [ 86  22 407  74 154  96  95  27  25  14]
 [ 17  23  73 367  69 266 122  25  20  18]
 [ 28  12  96  58 570  61  93  58  17   7]
 [ 17  10  66 168  89 537  58  36   8  11]
 [ 12  13  63  65 102  33 691   7   5   9]
 [ 27  16  39  66 165 106  31 506  16  28]
 [142  98  22  12  17   9  13   7 624  56]
 [ 56 168   8  28  18  12  20  25  47 618]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.61      0.64      0.62      1000
  automobile       0.63      0.71      0.67      1000
        bird       0.49      0.41      0.44      1000
         cat       0.42      0.37      0.39      1000
        deer       0.46      0.57      0.51      1000
         dog       0.47      0.54      0.50      1000
        frog       0.60      0.69      0.64      1000
       horse       0.72      0.51      0.59      1000
        ship       0.68      0.62      0.65      1000
       truck       0.64      0.62      0.63      1000

    accuracy                           0.57     10000
   macro avg       0.57      0.57      0.57     10000
weighted avg       0.57      0.57      0.57     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.5636
--
confusion matrix
[[300  37  30  11  18   8   7   4  52  21]
 [ 14 366   4   7   5   2  16   0  20  78]
 [ 40  18 230  33  86  38  46  20  12   9]
 [  7  12  35 167  34 121  53  16   8  18]
 [ 15   4  43  30 281  24  39  25   1   9]
 [  7  11  35  84  47 276  33  12   4   5]
 [ 10   7  37  42  57  16 323   7   5   3]
 [  9  11  16  28  99  48  16 247   5  21]
 [ 72  46   8   8  13   5   4   0 324  24]
 [ 27  81   4  17   9   5  14  14  26 304]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.60      0.61      0.61       488
  automobile       0.62      0.71      0.66       512
        bird       0.52      0.43      0.47       532
         cat       0.39      0.35      0.37       471
        deer       0.43      0.60      0.50       471
         dog       0.51      0.54      0.52       514
        frog       0.59      0.64      0.61       507
       horse       0.72      0.49      0.58       500
        ship       0.71      0.64      0.67       504
       truck       0.62      0.61      0.61       501

    accuracy                           0.56      5000
   macro avg       0.57      0.56      0.56      5000
weighted avg       0.57      0.56      0.56      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 298746: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Tue Feb 27 14:07:29 2024
Job was executed on host(s) <hgn41>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 14:08:06 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 14:08:06 2024
Terminated at Tue Feb 27 14:53:02 2024
Results reported at Tue Feb 27 14:53:02 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/ViT_small_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/ViT_small_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4314.00 sec.
    Max Memory :                                 2816 MB
    Average Memory :                             2704.11 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               7424.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   2697 sec.
    Turnaround time :                            2733 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/ViT_small_err_298746> for stderr output of this job.

