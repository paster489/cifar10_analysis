loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.7502, train_acc: 0.3581, val_loss: 1.6772, val_acc: 0.3800, val_precision: 0.4211, val_recall: 0.3800, val_f1: 0.3553
Epoch [1], train_loss: 1.7129, train_acc: 0.3641, val_loss: 1.6688, val_acc: 0.3857, val_precision: 0.3898, val_recall: 0.3857, val_f1: 0.3464
Epoch [2], train_loss: 1.6537, train_acc: 0.3859, val_loss: 1.6463, val_acc: 0.3880, val_precision: 0.4186, val_recall: 0.3880, val_f1: 0.3749
Epoch [3], train_loss: 1.6651, train_acc: 0.3855, val_loss: 1.6926, val_acc: 0.3700, val_precision: 0.4083, val_recall: 0.3700, val_f1: 0.3624
Epoch [4], train_loss: 1.7005, train_acc: 0.3695, val_loss: 1.6713, val_acc: 0.3865, val_precision: 0.4041, val_recall: 0.3865, val_f1: 0.3767
Epoch [5], train_loss: 1.6821, train_acc: 0.3772, val_loss: 1.6672, val_acc: 0.3903, val_precision: 0.4199, val_recall: 0.3903, val_f1: 0.3758
Epoch [6], train_loss: 1.7901, train_acc: 0.3317, val_loss: 1.9148, val_acc: 0.2649, val_precision: 0.3137, val_recall: 0.2649, val_f1: 0.2415
Epoch [7], train_loss: 1.8487, train_acc: 0.3037, val_loss: 1.8093, val_acc: 0.3223, val_precision: 0.3673, val_recall: 0.3223, val_f1: 0.2888
Epoch [8], train_loss: 1.7825, train_acc: 0.3316, val_loss: 1.7511, val_acc: 0.3550, val_precision: 0.3834, val_recall: 0.3550, val_f1: 0.3504
Epoch [9], train_loss: 1.7653, train_acc: 0.3435, val_loss: 1.7110, val_acc: 0.3651, val_precision: 0.3802, val_recall: 0.3651, val_f1: 0.3544
Epoch [10], train_loss: 1.7825, train_acc: 0.3354, val_loss: 1.8075, val_acc: 0.3315, val_precision: 0.3359, val_recall: 0.3315, val_f1: 0.2952
Epoch [11], train_loss: 1.8022, train_acc: 0.3230, val_loss: 1.7273, val_acc: 0.3664, val_precision: 0.3891, val_recall: 0.3664, val_f1: 0.3599
Epoch [12], train_loss: 1.7621, train_acc: 0.3417, val_loss: 1.7283, val_acc: 0.3651, val_precision: 0.3847, val_recall: 0.3651, val_f1: 0.3494
Epoch [13], train_loss: 1.7529, train_acc: 0.3509, val_loss: 1.7496, val_acc: 0.3613, val_precision: 0.3994, val_recall: 0.3613, val_f1: 0.3529
Epoch [14], train_loss: 1.7401, train_acc: 0.3525, val_loss: 1.7463, val_acc: 0.3584, val_precision: 0.3697, val_recall: 0.3584, val_f1: 0.3499
Epoch [15], train_loss: 1.7624, train_acc: 0.3420, val_loss: 1.7973, val_acc: 0.3359, val_precision: 0.3868, val_recall: 0.3359, val_f1: 0.3334
Epoch [16], train_loss: 1.7541, train_acc: 0.3496, val_loss: 1.7167, val_acc: 0.3708, val_precision: 0.3873, val_recall: 0.3708, val_f1: 0.3567
Epoch [17], train_loss: 1.7228, train_acc: 0.3608, val_loss: 1.6974, val_acc: 0.3714, val_precision: 0.3871, val_recall: 0.3714, val_f1: 0.3676
Epoch [18], train_loss: 1.7194, train_acc: 0.3612, val_loss: 1.7096, val_acc: 0.3692, val_precision: 0.3886, val_recall: 0.3692, val_f1: 0.3616
Epoch [19], train_loss: 1.7661, train_acc: 0.3429, val_loss: 1.7593, val_acc: 0.3366, val_precision: 0.3731, val_recall: 0.3366, val_f1: 0.3222
Epoch [20], train_loss: 1.7640, train_acc: 0.3402, val_loss: 1.7354, val_acc: 0.3542, val_precision: 0.3704, val_recall: 0.3542, val_f1: 0.3432
Epoch [21], train_loss: 1.7294, train_acc: 0.3542, val_loss: 1.6896, val_acc: 0.3637, val_precision: 0.3807, val_recall: 0.3637, val_f1: 0.3534
Epoch [22], train_loss: 1.7008, train_acc: 0.3675, val_loss: 1.6954, val_acc: 0.3683, val_precision: 0.3859, val_recall: 0.3683, val_f1: 0.3554
Epoch [23], train_loss: 1.6800, train_acc: 0.3776, val_loss: 1.6968, val_acc: 0.3725, val_precision: 0.4158, val_recall: 0.3725, val_f1: 0.3723
Epoch [24], train_loss: 1.6768, train_acc: 0.3799, val_loss: 1.6821, val_acc: 0.3857, val_precision: 0.4132, val_recall: 0.3857, val_f1: 0.3811
Epoch [25], train_loss: 1.6828, train_acc: 0.3786, val_loss: 1.6648, val_acc: 0.3959, val_precision: 0.4031, val_recall: 0.3959, val_f1: 0.3803
Epoch [26], train_loss: 1.6646, train_acc: 0.3863, val_loss: 1.6632, val_acc: 0.3866, val_precision: 0.4126, val_recall: 0.3866, val_f1: 0.3792
Epoch [27], train_loss: 1.6457, train_acc: 0.3933, val_loss: 1.6502, val_acc: 0.3992, val_precision: 0.4135, val_recall: 0.3992, val_f1: 0.3904
Epoch [28], train_loss: 1.6398, train_acc: 0.3996, val_loss: 1.6471, val_acc: 0.3920, val_precision: 0.4253, val_recall: 0.3920, val_f1: 0.3899
Epoch [29], train_loss: 1.6097, train_acc: 0.4104, val_loss: 1.5907, val_acc: 0.4213, val_precision: 0.4397, val_recall: 0.4213, val_f1: 0.4114
Epoch [30], train_loss: 1.5924, train_acc: 0.4159, val_loss: 1.5675, val_acc: 0.4289, val_precision: 0.4449, val_recall: 0.4289, val_f1: 0.4260
Epoch [31], train_loss: 1.5744, train_acc: 0.4235, val_loss: 1.5474, val_acc: 0.4371, val_precision: 0.4496, val_recall: 0.4371, val_f1: 0.4322
Epoch [32], train_loss: 1.5511, train_acc: 0.4317, val_loss: 1.5644, val_acc: 0.4234, val_precision: 0.4435, val_recall: 0.4234, val_f1: 0.4097
Epoch [33], train_loss: 1.5406, train_acc: 0.4346, val_loss: 1.5702, val_acc: 0.4223, val_precision: 0.4494, val_recall: 0.4223, val_f1: 0.4208
Epoch [34], train_loss: 1.5238, train_acc: 0.4383, val_loss: 1.5147, val_acc: 0.4570, val_precision: 0.4667, val_recall: 0.4570, val_f1: 0.4521
Epoch [35], train_loss: 1.5022, train_acc: 0.4499, val_loss: 1.5219, val_acc: 0.4503, val_precision: 0.4708, val_recall: 0.4503, val_f1: 0.4494
Epoch [36], train_loss: 1.4954, train_acc: 0.4522, val_loss: 1.5152, val_acc: 0.4521, val_precision: 0.4743, val_recall: 0.4521, val_f1: 0.4459
Epoch [37], train_loss: 1.4936, train_acc: 0.4505, val_loss: 1.4948, val_acc: 0.4646, val_precision: 0.4803, val_recall: 0.4646, val_f1: 0.4588
Epoch [38], train_loss: 1.4761, train_acc: 0.4623, val_loss: 1.4783, val_acc: 0.4775, val_precision: 0.4902, val_recall: 0.4775, val_f1: 0.4768
Epoch [39], train_loss: 1.4664, train_acc: 0.4633, val_loss: 1.5055, val_acc: 0.4635, val_precision: 0.4877, val_recall: 0.4635, val_f1: 0.4588
Epoch [40], train_loss: 1.4502, train_acc: 0.4724, val_loss: 1.5010, val_acc: 0.4531, val_precision: 0.4760, val_recall: 0.4531, val_f1: 0.4475
Epoch [41], train_loss: 1.4674, train_acc: 0.4577, val_loss: 1.4940, val_acc: 0.4625, val_precision: 0.4759, val_recall: 0.4625, val_f1: 0.4605
Epoch [42], train_loss: 1.4776, train_acc: 0.4608, val_loss: 1.4842, val_acc: 0.4624, val_precision: 0.4829, val_recall: 0.4624, val_f1: 0.4598
Epoch [43], train_loss: 1.4614, train_acc: 0.4650, val_loss: 1.4539, val_acc: 0.4724, val_precision: 0.4846, val_recall: 0.4724, val_f1: 0.4706
Epoch [44], train_loss: 1.4320, train_acc: 0.4774, val_loss: 1.4637, val_acc: 0.4737, val_precision: 0.4878, val_recall: 0.4737, val_f1: 0.4651
Epoch [45], train_loss: 1.4334, train_acc: 0.4743, val_loss: 1.4489, val_acc: 0.4799, val_precision: 0.5085, val_recall: 0.4799, val_f1: 0.4744
Epoch [46], train_loss: 1.4140, train_acc: 0.4836, val_loss: 1.4263, val_acc: 0.4824, val_precision: 0.4955, val_recall: 0.4824, val_f1: 0.4759
Epoch [47], train_loss: 1.3916, train_acc: 0.4915, val_loss: 1.4030, val_acc: 0.4913, val_precision: 0.5012, val_recall: 0.4913, val_f1: 0.4889
Epoch [48], train_loss: 1.3806, train_acc: 0.4969, val_loss: 1.4208, val_acc: 0.4901, val_precision: 0.5178, val_recall: 0.4901, val_f1: 0.4830
Epoch [49], train_loss: 1.3822, train_acc: 0.4985, val_loss: 1.4090, val_acc: 0.4941, val_precision: 0.5109, val_recall: 0.4941, val_f1: 0.4953
Epoch [50], train_loss: 1.3706, train_acc: 0.5008, val_loss: 1.4016, val_acc: 0.4833, val_precision: 0.5150, val_recall: 0.4833, val_f1: 0.4855
Epoch [51], train_loss: 1.3621, train_acc: 0.5057, val_loss: 1.4124, val_acc: 0.4971, val_precision: 0.5222, val_recall: 0.4971, val_f1: 0.5000
Epoch [52], train_loss: 1.3520, train_acc: 0.5087, val_loss: 1.3668, val_acc: 0.5098, val_precision: 0.5228, val_recall: 0.5098, val_f1: 0.5035
Epoch [53], train_loss: 1.3389, train_acc: 0.5099, val_loss: 1.3549, val_acc: 0.5057, val_precision: 0.5189, val_recall: 0.5057, val_f1: 0.5001
Epoch [54], train_loss: 1.3311, train_acc: 0.5161, val_loss: 1.3719, val_acc: 0.5012, val_precision: 0.5220, val_recall: 0.5012, val_f1: 0.4998
Epoch [55], train_loss: 1.3197, train_acc: 0.5217, val_loss: 1.3411, val_acc: 0.5127, val_precision: 0.5294, val_recall: 0.5127, val_f1: 0.5092
Epoch [56], train_loss: 1.2937, train_acc: 0.5316, val_loss: 1.3543, val_acc: 0.5111, val_precision: 0.5366, val_recall: 0.5111, val_f1: 0.5084
Epoch [57], train_loss: 1.2896, train_acc: 0.5328, val_loss: 1.3516, val_acc: 0.5216, val_precision: 0.5411, val_recall: 0.5216, val_f1: 0.5186
Epoch [58], train_loss: 1.2736, train_acc: 0.5357, val_loss: 1.3302, val_acc: 0.5185, val_precision: 0.5327, val_recall: 0.5185, val_f1: 0.5162
Epoch [59], train_loss: 1.2683, train_acc: 0.5384, val_loss: 1.3342, val_acc: 0.5181, val_precision: 0.5343, val_recall: 0.5181, val_f1: 0.5136
Epoch [60], train_loss: 1.2511, train_acc: 0.5417, val_loss: 1.3080, val_acc: 0.5327, val_precision: 0.5464, val_recall: 0.5327, val_f1: 0.5303
Epoch [61], train_loss: 1.2350, train_acc: 0.5482, val_loss: 1.3132, val_acc: 0.5304, val_precision: 0.5400, val_recall: 0.5304, val_f1: 0.5249
Epoch [62], train_loss: 1.2237, train_acc: 0.5546, val_loss: 1.2976, val_acc: 0.5270, val_precision: 0.5406, val_recall: 0.5270, val_f1: 0.5234
Epoch [63], train_loss: 1.2127, train_acc: 0.5602, val_loss: 1.3012, val_acc: 0.5302, val_precision: 0.5394, val_recall: 0.5302, val_f1: 0.5287
Epoch [64], train_loss: 1.1878, train_acc: 0.5681, val_loss: 1.3287, val_acc: 0.5256, val_precision: 0.5574, val_recall: 0.5256, val_f1: 0.5269
Epoch [65], train_loss: 1.1779, train_acc: 0.5735, val_loss: 1.2940, val_acc: 0.5395, val_precision: 0.5506, val_recall: 0.5395, val_f1: 0.5361
Epoch [66], train_loss: 1.1628, train_acc: 0.5781, val_loss: 1.2672, val_acc: 0.5474, val_precision: 0.5565, val_recall: 0.5474, val_f1: 0.5448
Epoch [67], train_loss: 1.1490, train_acc: 0.5827, val_loss: 1.2635, val_acc: 0.5413, val_precision: 0.5617, val_recall: 0.5413, val_f1: 0.5421
Epoch [68], train_loss: 1.1201, train_acc: 0.5957, val_loss: 1.2674, val_acc: 0.5499, val_precision: 0.5687, val_recall: 0.5499, val_f1: 0.5467
Epoch [69], train_loss: 1.0973, train_acc: 0.6042, val_loss: 1.2648, val_acc: 0.5570, val_precision: 0.5646, val_recall: 0.5570, val_f1: 0.5513
Epoch [70], train_loss: 1.0817, train_acc: 0.6087, val_loss: 1.2722, val_acc: 0.5578, val_precision: 0.5751, val_recall: 0.5578, val_f1: 0.5551
Epoch [71], train_loss: 1.0575, train_acc: 0.6139, val_loss: 1.2581, val_acc: 0.5489, val_precision: 0.5675, val_recall: 0.5489, val_f1: 0.5502
Epoch [72], train_loss: 1.0314, train_acc: 0.6244, val_loss: 1.2476, val_acc: 0.5606, val_precision: 0.5719, val_recall: 0.5606, val_f1: 0.5576
Epoch [73], train_loss: 0.9942, train_acc: 0.6371, val_loss: 1.2452, val_acc: 0.5619, val_precision: 0.5788, val_recall: 0.5619, val_f1: 0.5612
Epoch [74], train_loss: 0.9632, train_acc: 0.6526, val_loss: 1.2823, val_acc: 0.5509, val_precision: 0.5752, val_recall: 0.5509, val_f1: 0.5515
Epoch [75], train_loss: 0.9323, train_acc: 0.6621, val_loss: 1.2927, val_acc: 0.5565, val_precision: 0.5768, val_recall: 0.5565, val_f1: 0.5528
Epoch [76], train_loss: 0.8904, train_acc: 0.6776, val_loss: 1.2482, val_acc: 0.5771, val_precision: 0.5862, val_recall: 0.5771, val_f1: 0.5758
Epoch [77], train_loss: 0.8290, train_acc: 0.7002, val_loss: 1.3351, val_acc: 0.5658, val_precision: 0.5771, val_recall: 0.5658, val_f1: 0.5637
Epoch [78], train_loss: 0.7742, train_acc: 0.7168, val_loss: 1.3623, val_acc: 0.5630, val_precision: 0.5770, val_recall: 0.5630, val_f1: 0.5616
Epoch [79], train_loss: 0.7194, train_acc: 0.7381, val_loss: 1.3930, val_acc: 0.5673, val_precision: 0.5786, val_recall: 0.5673, val_f1: 0.5643
Epoch [80], train_loss: 0.6539, train_acc: 0.7622, val_loss: 1.4662, val_acc: 0.5636, val_precision: 0.5892, val_recall: 0.5636, val_f1: 0.5648
Epoch [81], train_loss: 0.6044, train_acc: 0.7772, val_loss: 1.4773, val_acc: 0.5552, val_precision: 0.5635, val_recall: 0.5552, val_f1: 0.5510
Epoch [82], train_loss: 0.5486, train_acc: 0.7989, val_loss: 1.5906, val_acc: 0.5600, val_precision: 0.5684, val_recall: 0.5600, val_f1: 0.5572
Epoch [83], train_loss: 0.5054, train_acc: 0.8172, val_loss: 1.7085, val_acc: 0.5579, val_precision: 0.5721, val_recall: 0.5579, val_f1: 0.5567
Epoch [84], train_loss: 0.4641, train_acc: 0.8299, val_loss: 1.6726, val_acc: 0.5538, val_precision: 0.5652, val_recall: 0.5538, val_f1: 0.5525
Epoch [85], train_loss: 0.4151, train_acc: 0.8491, val_loss: 1.7743, val_acc: 0.5615, val_precision: 0.5711, val_recall: 0.5615, val_f1: 0.5608
Epoch [86], train_loss: 0.3782, train_acc: 0.8613, val_loss: 1.8647, val_acc: 0.5532, val_precision: 0.5682, val_recall: 0.5532, val_f1: 0.5546
Epoch [87], train_loss: 0.3462, train_acc: 0.8742, val_loss: 1.9200, val_acc: 0.5531, val_precision: 0.5671, val_recall: 0.5531, val_f1: 0.5512
Epoch [88], train_loss: 0.3067, train_acc: 0.8878, val_loss: 2.0473, val_acc: 0.5574, val_precision: 0.5675, val_recall: 0.5574, val_f1: 0.5544
Epoch [89], train_loss: 0.2851, train_acc: 0.8968, val_loss: 2.1135, val_acc: 0.5607, val_precision: 0.5756, val_recall: 0.5607, val_f1: 0.5602
Epoch [90], train_loss: 0.2641, train_acc: 0.9061, val_loss: 2.1463, val_acc: 0.5567, val_precision: 0.5749, val_recall: 0.5567, val_f1: 0.5576
Epoch [91], train_loss: 0.2392, train_acc: 0.9125, val_loss: 2.2269, val_acc: 0.5563, val_precision: 0.5670, val_recall: 0.5563, val_f1: 0.5552
Epoch [92], train_loss: 0.2153, train_acc: 0.9247, val_loss: 2.3205, val_acc: 0.5620, val_precision: 0.5690, val_recall: 0.5620, val_f1: 0.5595
Epoch [93], train_loss: 0.2038, train_acc: 0.9256, val_loss: 2.4101, val_acc: 0.5544, val_precision: 0.5726, val_recall: 0.5544, val_f1: 0.5566
Epoch [94], train_loss: 0.1965, train_acc: 0.9267, val_loss: 2.3854, val_acc: 0.5530, val_precision: 0.5694, val_recall: 0.5530, val_f1: 0.5544
Epoch [95], train_loss: 0.1816, train_acc: 0.9342, val_loss: 2.4453, val_acc: 0.5602, val_precision: 0.5723, val_recall: 0.5602, val_f1: 0.5623
Epoch [96], train_loss: 0.1684, train_acc: 0.9380, val_loss: 2.5349, val_acc: 0.5509, val_precision: 0.5619, val_recall: 0.5509, val_f1: 0.5485
Epoch [97], train_loss: 0.1573, train_acc: 0.9439, val_loss: 2.5018, val_acc: 0.5491, val_precision: 0.5615, val_recall: 0.5491, val_f1: 0.5488
Epoch [98], train_loss: 0.1573, train_acc: 0.9442, val_loss: 2.5512, val_acc: 0.5457, val_precision: 0.5618, val_recall: 0.5457, val_f1: 0.5465
Epoch [99], train_loss: 0.1446, train_acc: 0.9490, val_loss: 2.5831, val_acc: 0.5555, val_precision: 0.5736, val_recall: 0.5555, val_f1: 0.5569
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 1.2484, val_acc: 0.5665, val_precision: 0.5814, val_recall: 0.5665, val_f1: 0.5645
Summary result of test set => last model => val_loss: 2.6310, val_acc: 0.5449, val_precision: 0.5587, val_recall: 0.5449, val_f1: 0.5443
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.5666
--
confusion matrix
[[640  52  46  24  33  13  18  11 115  48]
 [ 30 706  10  16  11   9  18   4  38 158]
 [ 86  22 407  74 154  96  95  27  25  14]
 [ 17  23  73 367  69 266 122  25  20  18]
 [ 28  12  96  58 570  61  93  58  17   7]
 [ 17  10  66 168  89 537  58  36   8  11]
 [ 12  13  63  65 102  33 691   7   5   9]
 [ 27  16  39  66 165 106  31 506  16  28]
 [142  98  22  12  17   9  13   7 624  56]
 [ 56 168   8  28  18  12  20  25  47 618]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.61      0.64      0.62      1000
  automobile       0.63      0.71      0.67      1000
        bird       0.49      0.41      0.44      1000
         cat       0.42      0.37      0.39      1000
        deer       0.46      0.57      0.51      1000
         dog       0.47      0.54      0.50      1000
        frog       0.60      0.69      0.64      1000
       horse       0.72      0.51      0.59      1000
        ship       0.68      0.62      0.65      1000
       truck       0.64      0.62      0.63      1000

    accuracy                           0.57     10000
   macro avg       0.57      0.57      0.57     10000
weighted avg       0.57      0.57      0.57     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.5636
--
confusion matrix
[[300  37  30  11  18   8   7   4  52  21]
 [ 14 366   4   7   5   2  16   0  20  78]
 [ 40  18 230  33  86  38  46  20  12   9]
 [  7  12  35 167  34 121  53  16   8  18]
 [ 15   4  43  30 281  24  39  25   1   9]
 [  7  11  35  84  47 276  33  12   4   5]
 [ 10   7  37  42  57  16 323   7   5   3]
 [  9  11  16  28  99  48  16 247   5  21]
 [ 72  46   8   8  13   5   4   0 324  24]
 [ 27  81   4  17   9   5  14  14  26 304]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.60      0.61      0.61       488
  automobile       0.62      0.71      0.66       512
        bird       0.52      0.43      0.47       532
         cat       0.39      0.35      0.37       471
        deer       0.43      0.60      0.50       471
         dog       0.51      0.54      0.52       514
        frog       0.59      0.64      0.61       507
       horse       0.72      0.49      0.58       500
        ship       0.71      0.64      0.67       504
       truck       0.62      0.61      0.61       501

    accuracy                           0.56      5000
   macro avg       0.57      0.56      0.56      5000
weighted avg       0.57      0.56      0.56      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 423935: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 08:26:13 2024
Job was executed on host(s) <hgn55>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 08:28:42 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 08:28:42 2024
Terminated at Wed Feb 28 09:57:12 2024
Results reported at Wed Feb 28 09:57:12 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/ViT_small_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/ViT_small_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model

(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   8035.00 sec.
    Max Memory :                                 2818 MB
    Average Memory :                             2712.36 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               7422.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   5312 sec.
    Turnaround time :                            5459 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/ViT_small_err_423935> for stderr output of this job.

