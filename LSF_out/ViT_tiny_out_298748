loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.7249, train_acc: 0.3670, val_loss: 1.5963, val_acc: 0.3959, val_precision: 0.3959, val_recall: 0.3959, val_f1: 0.3959
Epoch [1], train_loss: 1.6492, train_acc: 0.3933, val_loss: 1.6572, val_acc: 0.3921, val_precision: 0.3921, val_recall: 0.3921, val_f1: 0.3921
Epoch [2], train_loss: 1.6699, train_acc: 0.3886, val_loss: 1.6763, val_acc: 0.3786, val_precision: 0.3786, val_recall: 0.3786, val_f1: 0.3786
Epoch [3], train_loss: 1.6731, train_acc: 0.3868, val_loss: 1.6039, val_acc: 0.3953, val_precision: 0.3953, val_recall: 0.3953, val_f1: 0.3953
Epoch [4], train_loss: 1.6248, train_acc: 0.4011, val_loss: 1.6123, val_acc: 0.3993, val_precision: 0.3993, val_recall: 0.3993, val_f1: 0.3993
Epoch [5], train_loss: 1.6063, train_acc: 0.4084, val_loss: 1.5681, val_acc: 0.4297, val_precision: 0.4297, val_recall: 0.4297, val_f1: 0.4297
Epoch [6], train_loss: 1.5596, train_acc: 0.4283, val_loss: 1.5761, val_acc: 0.4198, val_precision: 0.4198, val_recall: 0.4198, val_f1: 0.4198
Epoch [7], train_loss: 1.5430, train_acc: 0.4352, val_loss: 1.5116, val_acc: 0.4500, val_precision: 0.4500, val_recall: 0.4500, val_f1: 0.4500
Epoch [8], train_loss: 1.5028, train_acc: 0.4480, val_loss: 1.4919, val_acc: 0.4473, val_precision: 0.4473, val_recall: 0.4473, val_f1: 0.4473
Epoch [9], train_loss: 1.4581, train_acc: 0.4689, val_loss: 1.4402, val_acc: 0.4795, val_precision: 0.4795, val_recall: 0.4795, val_f1: 0.4795
Epoch [10], train_loss: 1.4093, train_acc: 0.4835, val_loss: 1.4257, val_acc: 0.4777, val_precision: 0.4777, val_recall: 0.4777, val_f1: 0.4777
Epoch [11], train_loss: 1.3722, train_acc: 0.5004, val_loss: 1.3495, val_acc: 0.5072, val_precision: 0.5072, val_recall: 0.5072, val_f1: 0.5072
Epoch [12], train_loss: 1.3243, train_acc: 0.5179, val_loss: 1.3398, val_acc: 0.5113, val_precision: 0.5113, val_recall: 0.5113, val_f1: 0.5113
Epoch [13], train_loss: 1.2846, train_acc: 0.5347, val_loss: 1.2727, val_acc: 0.5385, val_precision: 0.5385, val_recall: 0.5385, val_f1: 0.5385
Epoch [14], train_loss: 1.2372, train_acc: 0.5518, val_loss: 1.2343, val_acc: 0.5568, val_precision: 0.5568, val_recall: 0.5568, val_f1: 0.5568
Epoch [15], train_loss: 1.1769, train_acc: 0.5760, val_loss: 1.2053, val_acc: 0.5630, val_precision: 0.5630, val_recall: 0.5630, val_f1: 0.5630
Epoch [16], train_loss: 1.1249, train_acc: 0.5941, val_loss: 1.2135, val_acc: 0.5726, val_precision: 0.5726, val_recall: 0.5726, val_f1: 0.5726
Epoch [17], train_loss: 1.0793, train_acc: 0.6106, val_loss: 1.1764, val_acc: 0.5827, val_precision: 0.5827, val_recall: 0.5827, val_f1: 0.5827
Epoch [18], train_loss: 1.0305, train_acc: 0.6298, val_loss: 1.1903, val_acc: 0.5784, val_precision: 0.5784, val_recall: 0.5784, val_f1: 0.5784
Epoch [19], train_loss: 0.9873, train_acc: 0.6437, val_loss: 1.1729, val_acc: 0.5883, val_precision: 0.5883, val_recall: 0.5883, val_f1: 0.5883
Epoch [20], train_loss: 0.9356, train_acc: 0.6621, val_loss: 1.1591, val_acc: 0.5953, val_precision: 0.5953, val_recall: 0.5953, val_f1: 0.5953
Epoch [21], train_loss: 0.8921, train_acc: 0.6786, val_loss: 1.1750, val_acc: 0.5914, val_precision: 0.5914, val_recall: 0.5914, val_f1: 0.5914
Epoch [22], train_loss: 0.8485, train_acc: 0.6928, val_loss: 1.1578, val_acc: 0.6042, val_precision: 0.6042, val_recall: 0.6042, val_f1: 0.6042
Epoch [23], train_loss: 0.8034, train_acc: 0.7086, val_loss: 1.1822, val_acc: 0.5991, val_precision: 0.5991, val_recall: 0.5991, val_f1: 0.5991
Epoch [24], train_loss: 0.7504, train_acc: 0.7273, val_loss: 1.1798, val_acc: 0.5936, val_precision: 0.5936, val_recall: 0.5936, val_f1: 0.5936
Epoch [25], train_loss: 0.7161, train_acc: 0.7400, val_loss: 1.2328, val_acc: 0.5951, val_precision: 0.5951, val_recall: 0.5951, val_f1: 0.5951
Epoch [26], train_loss: 0.6668, train_acc: 0.7565, val_loss: 1.2732, val_acc: 0.5989, val_precision: 0.5989, val_recall: 0.5989, val_f1: 0.5989
Epoch [27], train_loss: 0.6305, train_acc: 0.7702, val_loss: 1.2659, val_acc: 0.6081, val_precision: 0.6081, val_recall: 0.6081, val_f1: 0.6081
Epoch [28], train_loss: 0.5774, train_acc: 0.7899, val_loss: 1.3361, val_acc: 0.5985, val_precision: 0.5985, val_recall: 0.5985, val_f1: 0.5985
Epoch [29], train_loss: 0.5442, train_acc: 0.8018, val_loss: 1.3089, val_acc: 0.6048, val_precision: 0.6048, val_recall: 0.6048, val_f1: 0.6048
Epoch [30], train_loss: 0.4990, train_acc: 0.8173, val_loss: 1.3610, val_acc: 0.6032, val_precision: 0.6032, val_recall: 0.6032, val_f1: 0.6032
Epoch [31], train_loss: 0.4647, train_acc: 0.8308, val_loss: 1.4443, val_acc: 0.5946, val_precision: 0.5946, val_recall: 0.5946, val_f1: 0.5946
Epoch [32], train_loss: 0.4251, train_acc: 0.8431, val_loss: 1.4605, val_acc: 0.5968, val_precision: 0.5968, val_recall: 0.5968, val_f1: 0.5968
Epoch [33], train_loss: 0.3912, train_acc: 0.8548, val_loss: 1.4978, val_acc: 0.6032, val_precision: 0.6032, val_recall: 0.6032, val_f1: 0.6032
Epoch [34], train_loss: 0.3592, train_acc: 0.8693, val_loss: 1.7020, val_acc: 0.5993, val_precision: 0.5993, val_recall: 0.5993, val_f1: 0.5993
Epoch [35], train_loss: 0.3378, train_acc: 0.8753, val_loss: 1.5654, val_acc: 0.6074, val_precision: 0.6074, val_recall: 0.6074, val_f1: 0.6074
Epoch [36], train_loss: 0.3194, train_acc: 0.8848, val_loss: 1.6522, val_acc: 0.6072, val_precision: 0.6072, val_recall: 0.6072, val_f1: 0.6072
Epoch [37], train_loss: 0.2912, train_acc: 0.8965, val_loss: 1.7262, val_acc: 0.5946, val_precision: 0.5946, val_recall: 0.5946, val_f1: 0.5946
Epoch [38], train_loss: 0.2748, train_acc: 0.8999, val_loss: 1.7372, val_acc: 0.6014, val_precision: 0.6014, val_recall: 0.6014, val_f1: 0.6014
Epoch [39], train_loss: 0.2584, train_acc: 0.9062, val_loss: 1.7839, val_acc: 0.5836, val_precision: 0.5836, val_recall: 0.5836, val_f1: 0.5836
Epoch [40], train_loss: 0.2540, train_acc: 0.9094, val_loss: 1.8151, val_acc: 0.6036, val_precision: 0.6036, val_recall: 0.6036, val_f1: 0.6036
Epoch [41], train_loss: 0.2272, train_acc: 0.9190, val_loss: 1.8510, val_acc: 0.6138, val_precision: 0.6138, val_recall: 0.6138, val_f1: 0.6138
Epoch [42], train_loss: 0.2186, train_acc: 0.9208, val_loss: 1.9314, val_acc: 0.5934, val_precision: 0.5934, val_recall: 0.5934, val_f1: 0.5934
Epoch [43], train_loss: 0.2142, train_acc: 0.9237, val_loss: 1.8861, val_acc: 0.6121, val_precision: 0.6121, val_recall: 0.6121, val_f1: 0.6121
Epoch [44], train_loss: 0.1982, train_acc: 0.9283, val_loss: 1.9347, val_acc: 0.6107, val_precision: 0.6107, val_recall: 0.6107, val_f1: 0.6107
Epoch [45], train_loss: 0.1842, train_acc: 0.9345, val_loss: 1.9500, val_acc: 0.6051, val_precision: 0.6051, val_recall: 0.6051, val_f1: 0.6051
Epoch [46], train_loss: 0.1809, train_acc: 0.9344, val_loss: 2.0550, val_acc: 0.5987, val_precision: 0.5987, val_recall: 0.5987, val_f1: 0.5987
Epoch [47], train_loss: 0.1796, train_acc: 0.9358, val_loss: 2.0102, val_acc: 0.5919, val_precision: 0.5919, val_recall: 0.5919, val_f1: 0.5919
Epoch [48], train_loss: 0.1687, train_acc: 0.9407, val_loss: 2.1269, val_acc: 0.5971, val_precision: 0.5971, val_recall: 0.5971, val_f1: 0.5971
Epoch [49], train_loss: 0.1565, train_acc: 0.9436, val_loss: 2.1457, val_acc: 0.5972, val_precision: 0.5972, val_recall: 0.5972, val_f1: 0.5972
Epoch [50], train_loss: 0.1635, train_acc: 0.9430, val_loss: 2.1583, val_acc: 0.5950, val_precision: 0.5950, val_recall: 0.5950, val_f1: 0.5950
Epoch [51], train_loss: 0.1521, train_acc: 0.9438, val_loss: 2.1400, val_acc: 0.6009, val_precision: 0.6009, val_recall: 0.6009, val_f1: 0.6009
Epoch [52], train_loss: 0.1547, train_acc: 0.9449, val_loss: 2.0565, val_acc: 0.6097, val_precision: 0.6097, val_recall: 0.6097, val_f1: 0.6097
Epoch [53], train_loss: 0.1380, train_acc: 0.9502, val_loss: 2.1907, val_acc: 0.6090, val_precision: 0.6090, val_recall: 0.6090, val_f1: 0.6090
Epoch [54], train_loss: 0.1436, train_acc: 0.9488, val_loss: 2.1585, val_acc: 0.6058, val_precision: 0.6058, val_recall: 0.6058, val_f1: 0.6058
Epoch [55], train_loss: 0.1460, train_acc: 0.9497, val_loss: 2.1818, val_acc: 0.6029, val_precision: 0.6029, val_recall: 0.6029, val_f1: 0.6029
Epoch [56], train_loss: 0.1389, train_acc: 0.9505, val_loss: 2.2327, val_acc: 0.6033, val_precision: 0.6033, val_recall: 0.6033, val_f1: 0.6033
Epoch [57], train_loss: 0.1316, train_acc: 0.9539, val_loss: 2.2652, val_acc: 0.5933, val_precision: 0.5933, val_recall: 0.5933, val_f1: 0.5933
Epoch [58], train_loss: 0.1248, train_acc: 0.9555, val_loss: 2.2528, val_acc: 0.6029, val_precision: 0.6029, val_recall: 0.6029, val_f1: 0.6029
Epoch [59], train_loss: 0.1210, train_acc: 0.9568, val_loss: 2.2594, val_acc: 0.6025, val_precision: 0.6025, val_recall: 0.6025, val_f1: 0.6025
Epoch [60], train_loss: 0.1222, train_acc: 0.9572, val_loss: 2.2808, val_acc: 0.6072, val_precision: 0.6072, val_recall: 0.6072, val_f1: 0.6072
Epoch [61], train_loss: 0.1124, train_acc: 0.9599, val_loss: 2.2736, val_acc: 0.6126, val_precision: 0.6126, val_recall: 0.6126, val_f1: 0.6126
Epoch [62], train_loss: 0.1157, train_acc: 0.9593, val_loss: 2.2889, val_acc: 0.6120, val_precision: 0.6120, val_recall: 0.6120, val_f1: 0.6120
Epoch [63], train_loss: 0.1150, train_acc: 0.9594, val_loss: 2.3376, val_acc: 0.6002, val_precision: 0.6002, val_recall: 0.6002, val_f1: 0.6002
Epoch [64], train_loss: 0.1096, train_acc: 0.9618, val_loss: 2.3272, val_acc: 0.6013, val_precision: 0.6013, val_recall: 0.6013, val_f1: 0.6013
Epoch [65], train_loss: 0.1194, train_acc: 0.9586, val_loss: 2.1465, val_acc: 0.6076, val_precision: 0.6076, val_recall: 0.6076, val_f1: 0.6076
Epoch [66], train_loss: 0.1030, train_acc: 0.9622, val_loss: 2.3616, val_acc: 0.6023, val_precision: 0.6023, val_recall: 0.6023, val_f1: 0.6023
Epoch [67], train_loss: 0.1054, train_acc: 0.9620, val_loss: 2.3387, val_acc: 0.6092, val_precision: 0.6092, val_recall: 0.6092, val_f1: 0.6092
Epoch [68], train_loss: 0.1022, train_acc: 0.9626, val_loss: 2.3976, val_acc: 0.5884, val_precision: 0.5884, val_recall: 0.5884, val_f1: 0.5884
Epoch [69], train_loss: 0.1016, train_acc: 0.9634, val_loss: 2.4235, val_acc: 0.6021, val_precision: 0.6021, val_recall: 0.6021, val_f1: 0.6021
Epoch [70], train_loss: 0.1030, train_acc: 0.9634, val_loss: 2.2796, val_acc: 0.6091, val_precision: 0.6091, val_recall: 0.6091, val_f1: 0.6091
Epoch [71], train_loss: 0.0923, train_acc: 0.9686, val_loss: 2.3327, val_acc: 0.6051, val_precision: 0.6051, val_recall: 0.6051, val_f1: 0.6051
Epoch [72], train_loss: 0.0867, train_acc: 0.9687, val_loss: 2.4413, val_acc: 0.5998, val_precision: 0.5998, val_recall: 0.5998, val_f1: 0.5998
Epoch [73], train_loss: 0.0920, train_acc: 0.9683, val_loss: 2.5320, val_acc: 0.5937, val_precision: 0.5937, val_recall: 0.5937, val_f1: 0.5937
Epoch [74], train_loss: 0.1067, train_acc: 0.9628, val_loss: 2.3010, val_acc: 0.6122, val_precision: 0.6122, val_recall: 0.6122, val_f1: 0.6122
Epoch [75], train_loss: 0.0887, train_acc: 0.9688, val_loss: 2.4063, val_acc: 0.5990, val_precision: 0.5990, val_recall: 0.5990, val_f1: 0.5990
Epoch [76], train_loss: 0.0863, train_acc: 0.9685, val_loss: 2.4944, val_acc: 0.5973, val_precision: 0.5973, val_recall: 0.5973, val_f1: 0.5973
Epoch [77], train_loss: 0.0880, train_acc: 0.9693, val_loss: 2.4117, val_acc: 0.6056, val_precision: 0.6056, val_recall: 0.6056, val_f1: 0.6056
Epoch [78], train_loss: 0.0892, train_acc: 0.9679, val_loss: 2.3576, val_acc: 0.6096, val_precision: 0.6096, val_recall: 0.6096, val_f1: 0.6096
Epoch [79], train_loss: 0.0810, train_acc: 0.9714, val_loss: 2.5281, val_acc: 0.5973, val_precision: 0.5973, val_recall: 0.5973, val_f1: 0.5973
Epoch [80], train_loss: 0.0772, train_acc: 0.9725, val_loss: 2.4570, val_acc: 0.6110, val_precision: 0.6110, val_recall: 0.6110, val_f1: 0.6110
Epoch [81], train_loss: 0.0927, train_acc: 0.9691, val_loss: 2.3942, val_acc: 0.6143, val_precision: 0.6143, val_recall: 0.6143, val_f1: 0.6143
Epoch [82], train_loss: 0.0893, train_acc: 0.9680, val_loss: 2.3771, val_acc: 0.6075, val_precision: 0.6075, val_recall: 0.6075, val_f1: 0.6075
Epoch [83], train_loss: 0.0715, train_acc: 0.9755, val_loss: 2.5506, val_acc: 0.6143, val_precision: 0.6143, val_recall: 0.6143, val_f1: 0.6143
Epoch [84], train_loss: 0.0824, train_acc: 0.9698, val_loss: 2.5266, val_acc: 0.5992, val_precision: 0.5992, val_recall: 0.5992, val_f1: 0.5992
Epoch [85], train_loss: 0.0757, train_acc: 0.9735, val_loss: 2.4663, val_acc: 0.6076, val_precision: 0.6076, val_recall: 0.6076, val_f1: 0.6076
Epoch [86], train_loss: 0.0720, train_acc: 0.9756, val_loss: 2.5610, val_acc: 0.6060, val_precision: 0.6060, val_recall: 0.6060, val_f1: 0.6060
Epoch [87], train_loss: 0.0795, train_acc: 0.9702, val_loss: 2.5698, val_acc: 0.6060, val_precision: 0.6060, val_recall: 0.6060, val_f1: 0.6060
Epoch [88], train_loss: 0.0808, train_acc: 0.9712, val_loss: 2.3530, val_acc: 0.6099, val_precision: 0.6099, val_recall: 0.6099, val_f1: 0.6099
Epoch [89], train_loss: 0.0718, train_acc: 0.9748, val_loss: 2.5382, val_acc: 0.6073, val_precision: 0.6073, val_recall: 0.6073, val_f1: 0.6073
Epoch [90], train_loss: 0.0753, train_acc: 0.9744, val_loss: 2.6404, val_acc: 0.6054, val_precision: 0.6054, val_recall: 0.6054, val_f1: 0.6054
Epoch [91], train_loss: 0.0699, train_acc: 0.9752, val_loss: 2.6142, val_acc: 0.6052, val_precision: 0.6052, val_recall: 0.6052, val_f1: 0.6052
Epoch [92], train_loss: 0.0675, train_acc: 0.9754, val_loss: 2.6176, val_acc: 0.6013, val_precision: 0.6013, val_recall: 0.6013, val_f1: 0.6013
Epoch [93], train_loss: 0.0676, train_acc: 0.9758, val_loss: 2.6504, val_acc: 0.5953, val_precision: 0.5953, val_recall: 0.5953, val_f1: 0.5953
Epoch [94], train_loss: 0.0777, train_acc: 0.9722, val_loss: 2.4999, val_acc: 0.5933, val_precision: 0.5933, val_recall: 0.5933, val_f1: 0.5933
Epoch [95], train_loss: 0.0653, train_acc: 0.9766, val_loss: 2.5697, val_acc: 0.6060, val_precision: 0.6060, val_recall: 0.6060, val_f1: 0.6060
Epoch [96], train_loss: 0.0567, train_acc: 0.9793, val_loss: 2.5880, val_acc: 0.6034, val_precision: 0.6034, val_recall: 0.6034, val_f1: 0.6034
Epoch [97], train_loss: 0.0651, train_acc: 0.9767, val_loss: 2.5011, val_acc: 0.6038, val_precision: 0.6038, val_recall: 0.6038, val_f1: 0.6038
Epoch [98], train_loss: 0.0665, train_acc: 0.9766, val_loss: 2.6104, val_acc: 0.6070, val_precision: 0.6070, val_recall: 0.6070, val_f1: 0.6070
Epoch [99], train_loss: 0.0688, train_acc: 0.9752, val_loss: 2.6029, val_acc: 0.6022, val_precision: 0.6022, val_recall: 0.6022, val_f1: 0.6022
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 1.1650, val_acc: 0.5967, val_precision: 0.5967, val_recall: 0.5967, val_f1: 0.5967
Summary result of test set => last model => val_loss: 2.5592, val_acc: 0.6081, val_precision: 0.6081, val_recall: 0.6081, val_f1: 0.6081
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.5975
--
confusion matrix
[[657  20  84  20  25   6  10  16  98  64]
 [ 49 672  12  27   4   7   6   7  50 166]
 [ 74  11 503  97 108  80  52  47  14  14]
 [ 30  17  98 462  48 196  67  44  14  24]
 [ 20  10 119  80 534  58  62  88  16  13]
 [ 13  11  75 241  47 504  30  59   8  12]
 [ 13  14  95  99  80  34 628   8   6  23]
 [ 29   5  49  66  85  88   6 633   9  30]
 [118  47  26  21  16  14   7   3 688  60]
 [ 34 141  10  29   8   5   9  23  47 694]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.63      0.66      0.65      1000
  automobile       0.71      0.67      0.69      1000
        bird       0.47      0.50      0.49      1000
         cat       0.40      0.46      0.43      1000
        deer       0.56      0.53      0.55      1000
         dog       0.51      0.50      0.51      1000
        frog       0.72      0.63      0.67      1000
       horse       0.68      0.63      0.66      1000
        ship       0.72      0.69      0.71      1000
       truck       0.63      0.69      0.66      1000

    accuracy                           0.60     10000
   macro avg       0.60      0.60      0.60     10000
weighted avg       0.60      0.60      0.60     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.6044
--
confusion matrix
[[306  10  58  11  12   5   2   7  50  27]
 [ 28 355  11   6   2   7   1   4  27  71]
 [ 29   3 306  42  48  28  32  26  10   8]
 [  9   5  49 228  23 102  22  15   9   9]
 [ 12   1  68  28 260  32  13  39   5  13]
 [  3   5  48 120  27 255  18  29   4   5]
 [  5   5  51  61  34  17 319   2   3  10]
 [ 10   7  22  32  51  47   5 301   2  23]
 [ 55  23  14   8   8   4   1   1 364  26]
 [ 15  86   7  20   8   2   2  10  23 328]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.65      0.63      0.64       488
  automobile       0.71      0.69      0.70       512
        bird       0.48      0.58      0.52       532
         cat       0.41      0.48      0.44       471
        deer       0.55      0.55      0.55       471
         dog       0.51      0.50      0.50       514
        frog       0.77      0.63      0.69       507
       horse       0.69      0.60      0.64       500
        ship       0.73      0.72      0.73       504
       truck       0.63      0.65      0.64       501

    accuracy                           0.60      5000
   macro avg       0.61      0.60      0.61      5000
weighted avg       0.61      0.60      0.61      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 298748: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Tue Feb 27 14:07:44 2024
Job was executed on host(s) <hgn41>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Tue Feb 27 14:08:24 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Tue Feb 27 14:08:24 2024
Terminated at Tue Feb 27 14:35:16 2024
Results reported at Tue Feb 27 14:35:16 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/ViT_tiny_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/ViT_tiny_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"

# Optimizer
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "SGD" --experiment_name "CNN_SGD" --model "CNN"

# Model
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_18" --model "ResNet_18"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ResNet_34" --model "ResNet_34"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT" --model "ViT"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_small" --model "ViT_small"
python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "ViT_tiny" --model "ViT_tiny"

# Batch size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 32 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_32" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 64 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_64" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 256 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_256" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 512 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_512" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 1024 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_1024" --model "CNN"


(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   2879.00 sec.
    Max Memory :                                 2715 MB
    Average Memory :                             2625.48 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               7525.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   1616 sec.
    Turnaround time :                            1652 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/ViT_tiny_err_298748> for stderr output of this job.

