loading ...
loaded conda.sh
sh shell detected
 
-------------------------------------------------------------------------------
main => start
 
-------------------------------------------------------------------------------
Datasets Load => start
 
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
 
-------------------------------------------------------------------------------
Train/Validation random split => start
 
-------------------------------------------------------------------------------
DataLoader => start
 
-------------------------------------------------------------------------------
To_device => start
 
-------------------------------------------------------------------------------
Train => start
 
Epoch [0], train_loss: 1.6318, train_acc: 0.3959, val_loss: 1.4414, val_acc: 0.4709, val_precision: 0.5307, val_recall: 0.4709, val_f1: 0.4728
Epoch [1], train_loss: 1.1633, train_acc: 0.5755, val_loss: 1.1979, val_acc: 0.5704, val_precision: 0.6077, val_recall: 0.5704, val_f1: 0.5520
Epoch [2], train_loss: 0.9288, train_acc: 0.6704, val_loss: 0.9054, val_acc: 0.6843, val_precision: 0.7155, val_recall: 0.6843, val_f1: 0.6879
Epoch [3], train_loss: 0.7805, train_acc: 0.7239, val_loss: 0.9721, val_acc: 0.6767, val_precision: 0.7211, val_recall: 0.6767, val_f1: 0.6690
Epoch [4], train_loss: 0.6592, train_acc: 0.7718, val_loss: 0.6767, val_acc: 0.7658, val_precision: 0.7833, val_recall: 0.7658, val_f1: 0.7654
Epoch [5], train_loss: 0.5810, train_acc: 0.7991, val_loss: 0.6906, val_acc: 0.7572, val_precision: 0.7950, val_recall: 0.7572, val_f1: 0.7603
Epoch [6], train_loss: 0.5141, train_acc: 0.8222, val_loss: 0.7164, val_acc: 0.7588, val_precision: 0.7878, val_recall: 0.7588, val_f1: 0.7497
Epoch [7], train_loss: 0.4770, train_acc: 0.8355, val_loss: 0.5190, val_acc: 0.8194, val_precision: 0.8319, val_recall: 0.8194, val_f1: 0.8192
Epoch [8], train_loss: 0.4321, train_acc: 0.8509, val_loss: 0.6609, val_acc: 0.7870, val_precision: 0.8090, val_recall: 0.7870, val_f1: 0.7866
Epoch [9], train_loss: 0.3910, train_acc: 0.8649, val_loss: 0.4583, val_acc: 0.8424, val_precision: 0.8538, val_recall: 0.8424, val_f1: 0.8424
Epoch [10], train_loss: 0.3593, train_acc: 0.8760, val_loss: 0.5083, val_acc: 0.8273, val_precision: 0.8461, val_recall: 0.8273, val_f1: 0.8283
Epoch [11], train_loss: 0.3339, train_acc: 0.8834, val_loss: 0.4694, val_acc: 0.8371, val_precision: 0.8525, val_recall: 0.8371, val_f1: 0.8366
Epoch [12], train_loss: 0.3134, train_acc: 0.8912, val_loss: 0.4280, val_acc: 0.8616, val_precision: 0.8664, val_recall: 0.8616, val_f1: 0.8604
Epoch [13], train_loss: 0.2967, train_acc: 0.8970, val_loss: 0.3896, val_acc: 0.8659, val_precision: 0.8748, val_recall: 0.8659, val_f1: 0.8677
Epoch [14], train_loss: 0.2769, train_acc: 0.9045, val_loss: 0.4086, val_acc: 0.8568, val_precision: 0.8658, val_recall: 0.8568, val_f1: 0.8583
Epoch [15], train_loss: 0.2589, train_acc: 0.9106, val_loss: 0.3698, val_acc: 0.8793, val_precision: 0.8853, val_recall: 0.8793, val_f1: 0.8796
Epoch [16], train_loss: 0.2410, train_acc: 0.9169, val_loss: 0.3873, val_acc: 0.8720, val_precision: 0.8785, val_recall: 0.8720, val_f1: 0.8707
Epoch [17], train_loss: 0.2251, train_acc: 0.9211, val_loss: 0.3917, val_acc: 0.8700, val_precision: 0.8761, val_recall: 0.8700, val_f1: 0.8698
Epoch [18], train_loss: 0.2183, train_acc: 0.9230, val_loss: 0.3503, val_acc: 0.8812, val_precision: 0.8856, val_recall: 0.8812, val_f1: 0.8817
Epoch [19], train_loss: 0.1966, train_acc: 0.9305, val_loss: 0.3788, val_acc: 0.8776, val_precision: 0.8838, val_recall: 0.8776, val_f1: 0.8764
Epoch [20], train_loss: 0.1882, train_acc: 0.9340, val_loss: 0.4098, val_acc: 0.8731, val_precision: 0.8798, val_recall: 0.8731, val_f1: 0.8734
Epoch [21], train_loss: 0.1781, train_acc: 0.9371, val_loss: 0.3368, val_acc: 0.8884, val_precision: 0.8954, val_recall: 0.8884, val_f1: 0.8892
Epoch [22], train_loss: 0.1666, train_acc: 0.9419, val_loss: 0.4178, val_acc: 0.8773, val_precision: 0.8857, val_recall: 0.8773, val_f1: 0.8775
Epoch [23], train_loss: 0.1599, train_acc: 0.9432, val_loss: 0.3533, val_acc: 0.8909, val_precision: 0.8981, val_recall: 0.8909, val_f1: 0.8916
Epoch [24], train_loss: 0.1529, train_acc: 0.9464, val_loss: 0.4160, val_acc: 0.8717, val_precision: 0.8841, val_recall: 0.8717, val_f1: 0.8742
Epoch [25], train_loss: 0.1416, train_acc: 0.9510, val_loss: 0.3232, val_acc: 0.9008, val_precision: 0.9051, val_recall: 0.9008, val_f1: 0.9009
Epoch [26], train_loss: 0.1329, train_acc: 0.9529, val_loss: 0.3516, val_acc: 0.8917, val_precision: 0.8940, val_recall: 0.8917, val_f1: 0.8911
Epoch [27], train_loss: 0.1252, train_acc: 0.9562, val_loss: 0.3558, val_acc: 0.8927, val_precision: 0.8962, val_recall: 0.8927, val_f1: 0.8921
Epoch [28], train_loss: 0.1197, train_acc: 0.9575, val_loss: 0.3327, val_acc: 0.8991, val_precision: 0.9041, val_recall: 0.8991, val_f1: 0.9002
Epoch [29], train_loss: 0.1132, train_acc: 0.9601, val_loss: 0.3453, val_acc: 0.8966, val_precision: 0.8998, val_recall: 0.8966, val_f1: 0.8969
Epoch [30], train_loss: 0.1059, train_acc: 0.9624, val_loss: 0.3591, val_acc: 0.8977, val_precision: 0.9039, val_recall: 0.8977, val_f1: 0.8972
Epoch [31], train_loss: 0.1027, train_acc: 0.9640, val_loss: 0.3662, val_acc: 0.8934, val_precision: 0.8960, val_recall: 0.8934, val_f1: 0.8929
Epoch [32], train_loss: 0.0933, train_acc: 0.9674, val_loss: 0.3435, val_acc: 0.9004, val_precision: 0.9057, val_recall: 0.9004, val_f1: 0.9015
Epoch [33], train_loss: 0.0945, train_acc: 0.9671, val_loss: 0.3340, val_acc: 0.9025, val_precision: 0.9048, val_recall: 0.9025, val_f1: 0.9024
Epoch [34], train_loss: 0.0925, train_acc: 0.9669, val_loss: 0.3351, val_acc: 0.9050, val_precision: 0.9084, val_recall: 0.9050, val_f1: 0.9047
Epoch [35], train_loss: 0.0829, train_acc: 0.9705, val_loss: 0.3305, val_acc: 0.9072, val_precision: 0.9096, val_recall: 0.9072, val_f1: 0.9073
Epoch [36], train_loss: 0.0760, train_acc: 0.9737, val_loss: 0.3878, val_acc: 0.8967, val_precision: 0.9038, val_recall: 0.8967, val_f1: 0.8973
Epoch [37], train_loss: 0.0769, train_acc: 0.9729, val_loss: 0.3550, val_acc: 0.9034, val_precision: 0.9069, val_recall: 0.9034, val_f1: 0.9032
Epoch [38], train_loss: 0.0745, train_acc: 0.9733, val_loss: 0.3225, val_acc: 0.9107, val_precision: 0.9141, val_recall: 0.9107, val_f1: 0.9113
Epoch [39], train_loss: 0.0706, train_acc: 0.9749, val_loss: 0.3300, val_acc: 0.9115, val_precision: 0.9133, val_recall: 0.9115, val_f1: 0.9115
Epoch [40], train_loss: 0.0661, train_acc: 0.9774, val_loss: 0.3712, val_acc: 0.9045, val_precision: 0.9081, val_recall: 0.9045, val_f1: 0.9044
Epoch [41], train_loss: 0.0662, train_acc: 0.9766, val_loss: 0.3455, val_acc: 0.9051, val_precision: 0.9079, val_recall: 0.9051, val_f1: 0.9052
Epoch [42], train_loss: 0.0657, train_acc: 0.9770, val_loss: 0.3406, val_acc: 0.9119, val_precision: 0.9142, val_recall: 0.9119, val_f1: 0.9119
Epoch [43], train_loss: 0.0565, train_acc: 0.9796, val_loss: 0.3592, val_acc: 0.9112, val_precision: 0.9131, val_recall: 0.9112, val_f1: 0.9109
Epoch [44], train_loss: 0.0557, train_acc: 0.9805, val_loss: 0.3495, val_acc: 0.9111, val_precision: 0.9145, val_recall: 0.9111, val_f1: 0.9115
Epoch [45], train_loss: 0.0567, train_acc: 0.9804, val_loss: 0.3498, val_acc: 0.9116, val_precision: 0.9134, val_recall: 0.9116, val_f1: 0.9115
Epoch [46], train_loss: 0.0519, train_acc: 0.9820, val_loss: 0.3374, val_acc: 0.9154, val_precision: 0.9183, val_recall: 0.9154, val_f1: 0.9151
Epoch [47], train_loss: 0.0525, train_acc: 0.9815, val_loss: 0.3640, val_acc: 0.9080, val_precision: 0.9105, val_recall: 0.9080, val_f1: 0.9079
Epoch [48], train_loss: 0.0505, train_acc: 0.9826, val_loss: 0.3314, val_acc: 0.9122, val_precision: 0.9141, val_recall: 0.9122, val_f1: 0.9119
Epoch [49], train_loss: 0.0469, train_acc: 0.9834, val_loss: 0.3787, val_acc: 0.9009, val_precision: 0.9059, val_recall: 0.9009, val_f1: 0.9017
Epoch [50], train_loss: 0.0490, train_acc: 0.9833, val_loss: 0.3673, val_acc: 0.9099, val_precision: 0.9133, val_recall: 0.9099, val_f1: 0.9104
Epoch [51], train_loss: 0.0478, train_acc: 0.9829, val_loss: 0.3839, val_acc: 0.9087, val_precision: 0.9142, val_recall: 0.9087, val_f1: 0.9092
Epoch [52], train_loss: 0.0415, train_acc: 0.9858, val_loss: 0.3530, val_acc: 0.9138, val_precision: 0.9161, val_recall: 0.9138, val_f1: 0.9138
Epoch [53], train_loss: 0.0483, train_acc: 0.9833, val_loss: 0.3202, val_acc: 0.9178, val_precision: 0.9203, val_recall: 0.9178, val_f1: 0.9179
Epoch [54], train_loss: 0.0385, train_acc: 0.9866, val_loss: 0.3443, val_acc: 0.9199, val_precision: 0.9228, val_recall: 0.9199, val_f1: 0.9201
Epoch [55], train_loss: 0.0422, train_acc: 0.9850, val_loss: 0.3861, val_acc: 0.9105, val_precision: 0.9127, val_recall: 0.9105, val_f1: 0.9105
Epoch [56], train_loss: 0.0422, train_acc: 0.9852, val_loss: 0.3669, val_acc: 0.9111, val_precision: 0.9130, val_recall: 0.9111, val_f1: 0.9110
Epoch [57], train_loss: 0.0435, train_acc: 0.9854, val_loss: 0.3567, val_acc: 0.9162, val_precision: 0.9195, val_recall: 0.9162, val_f1: 0.9165
Epoch [58], train_loss: 0.0333, train_acc: 0.9883, val_loss: 0.3617, val_acc: 0.9166, val_precision: 0.9180, val_recall: 0.9166, val_f1: 0.9164
Epoch [59], train_loss: 0.0395, train_acc: 0.9860, val_loss: 0.3607, val_acc: 0.9190, val_precision: 0.9212, val_recall: 0.9190, val_f1: 0.9189
Epoch [60], train_loss: 0.0375, train_acc: 0.9871, val_loss: 0.3977, val_acc: 0.9089, val_precision: 0.9116, val_recall: 0.9089, val_f1: 0.9087
Epoch [61], train_loss: 0.0354, train_acc: 0.9875, val_loss: 0.3926, val_acc: 0.9060, val_precision: 0.9107, val_recall: 0.9060, val_f1: 0.9066
Epoch [62], train_loss: 0.0335, train_acc: 0.9885, val_loss: 0.3647, val_acc: 0.9172, val_precision: 0.9195, val_recall: 0.9172, val_f1: 0.9171
Epoch [63], train_loss: 0.0318, train_acc: 0.9892, val_loss: 0.3601, val_acc: 0.9155, val_precision: 0.9166, val_recall: 0.9155, val_f1: 0.9152
Epoch [64], train_loss: 0.0350, train_acc: 0.9882, val_loss: 0.3607, val_acc: 0.9144, val_precision: 0.9171, val_recall: 0.9144, val_f1: 0.9145
Epoch [65], train_loss: 0.0294, train_acc: 0.9899, val_loss: 0.3548, val_acc: 0.9163, val_precision: 0.9181, val_recall: 0.9163, val_f1: 0.9165
Epoch [66], train_loss: 0.0307, train_acc: 0.9895, val_loss: 0.3696, val_acc: 0.9156, val_precision: 0.9175, val_recall: 0.9156, val_f1: 0.9155
Epoch [67], train_loss: 0.0290, train_acc: 0.9902, val_loss: 0.3488, val_acc: 0.9235, val_precision: 0.9262, val_recall: 0.9235, val_f1: 0.9239
Epoch [68], train_loss: 0.0313, train_acc: 0.9891, val_loss: 0.3792, val_acc: 0.9161, val_precision: 0.9193, val_recall: 0.9161, val_f1: 0.9165
Epoch [69], train_loss: 0.0281, train_acc: 0.9907, val_loss: 0.3566, val_acc: 0.9223, val_precision: 0.9242, val_recall: 0.9223, val_f1: 0.9221
Epoch [70], train_loss: 0.0315, train_acc: 0.9891, val_loss: 0.3610, val_acc: 0.9190, val_precision: 0.9214, val_recall: 0.9190, val_f1: 0.9192
Epoch [71], train_loss: 0.0288, train_acc: 0.9902, val_loss: 0.3399, val_acc: 0.9207, val_precision: 0.9230, val_recall: 0.9207, val_f1: 0.9209
Epoch [72], train_loss: 0.0242, train_acc: 0.9915, val_loss: 0.4085, val_acc: 0.9127, val_precision: 0.9158, val_recall: 0.9127, val_f1: 0.9127
Epoch [73], train_loss: 0.0301, train_acc: 0.9896, val_loss: 0.3617, val_acc: 0.9179, val_precision: 0.9207, val_recall: 0.9179, val_f1: 0.9179
Epoch [74], train_loss: 0.0286, train_acc: 0.9901, val_loss: 0.3584, val_acc: 0.9178, val_precision: 0.9201, val_recall: 0.9178, val_f1: 0.9179
Epoch [75], train_loss: 0.0245, train_acc: 0.9915, val_loss: 0.3636, val_acc: 0.9228, val_precision: 0.9248, val_recall: 0.9228, val_f1: 0.9230
Epoch [76], train_loss: 0.0252, train_acc: 0.9911, val_loss: 0.3903, val_acc: 0.9171, val_precision: 0.9189, val_recall: 0.9171, val_f1: 0.9171
Epoch [77], train_loss: 0.0273, train_acc: 0.9906, val_loss: 0.4113, val_acc: 0.9137, val_precision: 0.9166, val_recall: 0.9137, val_f1: 0.9137
Epoch [78], train_loss: 0.0257, train_acc: 0.9910, val_loss: 0.3680, val_acc: 0.9185, val_precision: 0.9205, val_recall: 0.9185, val_f1: 0.9186
Epoch [79], train_loss: 0.0238, train_acc: 0.9919, val_loss: 0.3822, val_acc: 0.9150, val_precision: 0.9174, val_recall: 0.9150, val_f1: 0.9152
Epoch [80], train_loss: 0.0269, train_acc: 0.9907, val_loss: 0.3776, val_acc: 0.9207, val_precision: 0.9226, val_recall: 0.9207, val_f1: 0.9205
Epoch [81], train_loss: 0.0254, train_acc: 0.9911, val_loss: 0.4252, val_acc: 0.9129, val_precision: 0.9166, val_recall: 0.9129, val_f1: 0.9136
Epoch [82], train_loss: 0.0245, train_acc: 0.9917, val_loss: 0.4276, val_acc: 0.9098, val_precision: 0.9159, val_recall: 0.9098, val_f1: 0.9104
Epoch [83], train_loss: 0.0256, train_acc: 0.9917, val_loss: 0.3660, val_acc: 0.9198, val_precision: 0.9214, val_recall: 0.9198, val_f1: 0.9198
Epoch [84], train_loss: 0.0193, train_acc: 0.9934, val_loss: 0.3740, val_acc: 0.9181, val_precision: 0.9205, val_recall: 0.9181, val_f1: 0.9182
Epoch [85], train_loss: 0.0210, train_acc: 0.9928, val_loss: 0.3886, val_acc: 0.9175, val_precision: 0.9207, val_recall: 0.9175, val_f1: 0.9176
Epoch [86], train_loss: 0.0200, train_acc: 0.9931, val_loss: 0.3958, val_acc: 0.9215, val_precision: 0.9232, val_recall: 0.9215, val_f1: 0.9214
Epoch [87], train_loss: 0.0196, train_acc: 0.9931, val_loss: 0.4374, val_acc: 0.9161, val_precision: 0.9198, val_recall: 0.9161, val_f1: 0.9162
Epoch [88], train_loss: 0.0269, train_acc: 0.9912, val_loss: 0.3929, val_acc: 0.9131, val_precision: 0.9148, val_recall: 0.9131, val_f1: 0.9130
Epoch [89], train_loss: 0.0260, train_acc: 0.9911, val_loss: 0.4106, val_acc: 0.9201, val_precision: 0.9220, val_recall: 0.9201, val_f1: 0.9200
Epoch [90], train_loss: 0.0195, train_acc: 0.9930, val_loss: 0.4148, val_acc: 0.9181, val_precision: 0.9229, val_recall: 0.9181, val_f1: 0.9186
Epoch [91], train_loss: 0.0208, train_acc: 0.9927, val_loss: 0.3932, val_acc: 0.9217, val_precision: 0.9242, val_recall: 0.9217, val_f1: 0.9220
Epoch [92], train_loss: 0.0209, train_acc: 0.9931, val_loss: 0.4066, val_acc: 0.9176, val_precision: 0.9201, val_recall: 0.9176, val_f1: 0.9179
Epoch [93], train_loss: 0.0233, train_acc: 0.9915, val_loss: 0.4084, val_acc: 0.9171, val_precision: 0.9196, val_recall: 0.9171, val_f1: 0.9174
Epoch [94], train_loss: 0.0228, train_acc: 0.9921, val_loss: 0.4236, val_acc: 0.9155, val_precision: 0.9187, val_recall: 0.9155, val_f1: 0.9155
Epoch [95], train_loss: 0.0179, train_acc: 0.9935, val_loss: 0.4020, val_acc: 0.9221, val_precision: 0.9251, val_recall: 0.9221, val_f1: 0.9220
Epoch [96], train_loss: 0.0170, train_acc: 0.9940, val_loss: 0.4159, val_acc: 0.9198, val_precision: 0.9229, val_recall: 0.9198, val_f1: 0.9202
Epoch [97], train_loss: 0.0203, train_acc: 0.9924, val_loss: 0.3784, val_acc: 0.9182, val_precision: 0.9203, val_recall: 0.9182, val_f1: 0.9183
Epoch [98], train_loss: 0.0181, train_acc: 0.9938, val_loss: 0.4009, val_acc: 0.9185, val_precision: 0.9229, val_recall: 0.9185, val_f1: 0.9191
Epoch [99], train_loss: 0.0175, train_acc: 0.9942, val_loss: 0.4189, val_acc: 0.9140, val_precision: 0.9165, val_recall: 0.9140, val_f1: 0.9139
 
-------------------------------------------------------------------------------
Visualize trining => save images
 
-------------------------------------------------------------------------------
Load the model => start
 
-------------------------------------------------------------------------------
Check best/last models => start
 
Summary result of test set => best model => val_loss: 0.3846, val_acc: 0.9168, val_precision: 0.9183, val_recall: 0.9168, val_f1: 0.9166
Summary result of test set => last model => val_loss: 0.4838, val_acc: 0.9179, val_precision: 0.9214, val_recall: 0.9179, val_f1: 0.9175
 
-------------------------------------------------------------------------------
Test set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.9170
--
confusion matrix
[[916   4  18  11   7   0   7   4  28   5]
 [  1 970   0   1   1   0   2   0   7  18]
 [ 18   0 886  19  25  14  29   4   4   1]
 [  7   1  35 826  16  67  31   9   4   4]
 [  2   0  15  15 918  15  20  14   1   0]
 [  3   0  14  79  10 869   8  14   1   2]
 [  5   1  21  15   2   3 951   1   0   1]
 [  5   0  10  13  11  13   2 943   1   2]
 [ 18   8   3   5   2   0   2   0 950  12]
 [  8  34   0   5   0   1   1   1   9 941]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.93      0.92      0.92      1000
  automobile       0.95      0.97      0.96      1000
        bird       0.88      0.89      0.89      1000
         cat       0.84      0.83      0.83      1000
        deer       0.93      0.92      0.92      1000
         dog       0.88      0.87      0.88      1000
        frog       0.90      0.95      0.93      1000
       horse       0.95      0.94      0.95      1000
        ship       0.95      0.95      0.95      1000
       truck       0.95      0.94      0.95      1000

    accuracy                           0.92     10000
   macro avg       0.92      0.92      0.92     10000
weighted avg       0.92      0.92      0.92     10000

-------------------------------------------------------------------------------
Valid set evaluation (best model) => save results for postprocessing
 
** accuracy: 0.9214
--
confusion matrix
[[448   2  13   1   2   1   2   0  14   5]
 [  1 497   0   1   0   0   0   0   2  11]
 [  9   0 476  10   7   7  11   7   4   1]
 [  1   0  11 408   8  31   9   1   2   0]
 [  0   0   5  12 427   6  13   7   1   0]
 [  0   0  10  48   9 435   6   6   0   0]
 [  0   2   7   8   0   5 484   0   1   0]
 [  1   3   5   5  10   7   3 462   0   4]
 [ 11   3   2   0   0   0   1   0 483   4]
 [  3   5   1   2   0   0   1   2   0 487]]
--
classification report
              precision    recall  f1-score   support

    aircraft       0.95      0.92      0.93       488
  automobile       0.97      0.97      0.97       512
        bird       0.90      0.89      0.90       532
         cat       0.82      0.87      0.84       471
        deer       0.92      0.91      0.91       471
         dog       0.88      0.85      0.86       514
        frog       0.91      0.95      0.93       507
       horse       0.95      0.92      0.94       500
        ship       0.95      0.96      0.96       504
       truck       0.95      0.97      0.96       501

    accuracy                           0.92      5000
   macro avg       0.92      0.92      0.92      5000
weighted avg       0.92      0.92      0.92      5000

-------------------------------------------------------------------------------
END OF CODE
-------------------------------------------------------------------------------
 

------------------------------------------------------------
Sender: LSF System <DoNotReply>
Subject: Job 443340: <cifar10> in cluster <wexac> Done

Job <cifar10> was submitted from host <agn01> by user <ingap> in cluster <wexac> at Wed Feb 28 12:23:05 2024
Job was executed on host(s) <hgn50>, in queue <waic-short>, as user <ingap> in cluster <wexac> at Wed Feb 28 12:23:29 2024
</home/projects/bagon/ingap> was used as the home directory.
</home/projects/bagon/ingap> was used as the working directory.
Started at Wed Feb 28 12:23:29 2024
Terminated at Wed Feb 28 13:02:18 2024
Results reported at Wed Feb 28 13:02:18 2024

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J cifar10                             
#BSUB -o /home/projects/bagon/ingap/LSF_out/optimal_model_out_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_out_%J
#BSUB -e /home/projects/bagon/ingap/LSF_err/optimal_model_err_%J        #/home/projects/bagon/ingap/torch_lightening/GPU_err_%J
#BSUB -q waic-short  
#BSUB -m "waic_2023_gpu"                       
#BSUB -gpu num=1:j_exclusive=yes:gmem=30G    # Number of GPUs per node
#BSUB -R rusage[mem=10G]                     # Resource allocation per task
#BSUB -R affinity[thread*4]                  # Resource allocation per task

if [ -f ~/.bash_profile ]; then
  . ~/.bash_profile
elif [ -f ~/.profile ]; then
  . ~/.profile
fi
module purge;module load miniconda/23.3.1-0_environmentally;module load NCCL/2.12.12-GCCcore-11.3.0-CUDA-11.8.0;module load CUDA/11.8.0
. activate;conda deactivate;conda activate /home/projects/bagon/ingap/.conda/envs/torch_gpu_env # PL for MNIST
cd /home/projects/bagon/ingap/cifar10_analysis/cifar10_analysis/


# Reprodusability
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_3" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_4" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 2 --optimization "Adam" --experiment_name "CNN_No_run_5" --model "CNN"

# Normalization check
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_No" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N1_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N1_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "N2_aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_N2_aug" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "aug" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_aug" --model "CNN"

# Valid size
#python cifar_10_train_rev_2.py --normalization "No" --val_size 15 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_15" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 20 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_20" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 25 --batch_size 128 --num_workers 4 --lr 0.001 --epochs 100 --optimization "Adam" --experiment_name "CNN_val_25" --model "CNN"

# LR
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.007 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_007" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.009 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_009" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0005 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0005" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.0001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0001" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.00001 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_00001" --model "CNN"
#python cifar_10_train_rev_2.py --normalization "No" --val_size 10 --batch_size 128 --num_workers 4 --lr 0.002 --epochs 100 --optimization "Adam" --experiment_name "CNN_LR_0.002" --model "CNN"



(... more ...)
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   4649.00 sec.
    Max Memory :                                 3707 MB
    Average Memory :                             3453.91 MB
    Total Requested Memory :                     10240.00 MB
    Delta Memory :                               6533.00 MB
    Max Swap :                                   -
    Max Processes :                              8
    Max Threads :                                30
    Run time :                                   2330 sec.
    Turnaround time :                            2353 sec.

The output (if any) is above this job summary.



PS:

Read file </home/projects/bagon/ingap/LSF_err/optimal_model_err_443340> for stderr output of this job.

